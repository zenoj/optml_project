Files already downloaded and verified
Files already downloaded and verified
Gradient norm: 1.8302433490753174
Rank 2, Epoch 1, Batch 0, Loss: 2.498436
Gradient norm: 0.13241560757160187
Rank 2, Epoch 1, Batch 1, Loss: 2.300745
Gradient norm: 0.12001791596412659
Rank 2, Epoch 1, Batch 2, Loss: 2.302082
Gradient norm: 0.0958486795425415
Rank 2, Epoch 1, Batch 3, Loss: 2.302751
Gradient norm: 0.10471981763839722
Rank 2, Epoch 1, Batch 4, Loss: 2.303081
Gradient norm: 0.11161444336175919
Rank 2, Epoch 1, Batch 5, Loss: 2.303704
Gradient norm: 0.1210005134344101
Rank 2, Epoch 1, Batch 6, Loss: 2.302006
Gradient norm: 0.1362202912569046
Rank 2, Epoch 1, Batch 7, Loss: 2.301783
Gradient norm: 0.1602577567100525
Rank 2, Epoch 1, Batch 8, Loss: 2.303276
Gradient norm: 0.13790351152420044
Rank 2, Epoch 1, Batch 9, Loss: 2.302056
Gradient norm: 0.14439648389816284
Rank 2, Epoch 1, Batch 10, Loss: 2.303289
Gradient norm: 0.14965274930000305
Rank 2, Epoch 1, Batch 11, Loss: 2.303870
Gradient norm: 0.15811622142791748
Rank 2, Epoch 1, Batch 12, Loss: 2.304660
Gradient norm: 0.17072510719299316
Rank 2, Epoch 1, Batch 13, Loss: 2.301626
Gradient norm: 0.1567433923482895
Rank 2, Epoch 1, Batch 14, Loss: 2.301360
Gradient norm: 0.18468348681926727
Rank 2, Epoch 1, Batch 15, Loss: 2.300709
Gradient norm: 0.20955398678779602
Rank 2, Epoch 1, Batch 16, Loss: 2.301957
Gradient norm: 0.23728421330451965
Rank 2, Epoch 1, Batch 17, Loss: 2.301263
Gradient norm: 0.19438503682613373
Rank 2, Epoch 1, Batch 18, Loss: 2.302872
Gradient norm: 0.16417762637138367
Rank 2, Epoch 1, Batch 19, Loss: 2.303247
Gradient norm: 0.23681914806365967
Rank 2, Epoch 1, Batch 20, Loss: 2.301246
Gradient norm: 0.33684173226356506
Rank 2, Epoch 1, Batch 21, Loss: 2.298043
Gradient norm: 0.2919890284538269
Rank 2, Epoch 1, Batch 22, Loss: 2.303129
Gradient norm: 0.35697004199028015
Rank 2, Epoch 1, Batch 23, Loss: 2.299570
Gradient norm: 0.2604753375053406
Rank 2, Epoch 1, Batch 24, Loss: 2.298880
Gradient norm: 0.275467187166214
Rank 2, Epoch 1, Batch 25, Loss: 2.301692
Gradient norm: 0.5586841106414795
Rank 2, Epoch 1, Batch 26, Loss: 2.301878
Gradient norm: 0.40481194853782654
Rank 2, Epoch 1, Batch 27, Loss: 2.298455
Gradient norm: 0.4273557960987091
Rank 2, Epoch 1, Batch 28, Loss: 2.301362
Gradient norm: 0.4908866584300995
Rank 2, Epoch 1, Batch 29, Loss: 2.295390
Gradient norm: 0.5676191449165344
Rank 2, Epoch 1, Batch 30, Loss: 2.309848
Gradient norm: 0.5903043746948242
Rank 2, Epoch 1, Batch 31, Loss: 2.307805
Gradient norm: 0.6557615995407104
Rank 2, Epoch 1, Batch 32, Loss: 2.302819
Gradient norm: 0.5219165086746216
Rank 2, Epoch 1, Batch 33, Loss: 2.300375
Gradient norm: 0.4978609085083008
Rank 2, Epoch 1, Batch 34, Loss: 2.289767
Gradient norm: 0.892830491065979
Rank 2, Epoch 1, Batch 35, Loss: 2.301255
Gradient norm: 0.7539387345314026
Rank 2, Epoch 1, Batch 36, Loss: 2.291933
Gradient norm: 0.856204092502594
Rank 2, Epoch 1, Batch 37, Loss: 2.296066
Gradient norm: 0.7654908895492554
Rank 2, Epoch 1, Batch 38, Loss: 2.289607
Gradient norm: 1.129426121711731
Rank 2, Epoch 1, Batch 39, Loss: 2.302418
Gradient norm: 0.9724000692367554
Rank 2, Epoch 1, Batch 40, Loss: 2.286631
Gradient norm: 0.7176249623298645
Rank 2, Epoch 1, Batch 41, Loss: 2.291583
Gradient norm: 1.3952510356903076
Rank 2, Epoch 1, Batch 42, Loss: 2.305692
Gradient norm: 0.5961177945137024
Rank 2, Epoch 1, Batch 43, Loss: 2.286847
Gradient norm: 0.9106770157814026
Rank 2, Epoch 1, Batch 44, Loss: 2.292247
Gradient norm: 1.13348388671875
Rank 2, Epoch 1, Batch 45, Loss: 2.281694
Gradient norm: 0.6398221850395203
Rank 2, Epoch 1, Batch 46, Loss: 2.311389
Gradient norm: 1.5338168144226074
Rank 2, Epoch 1, Batch 47, Loss: 2.269772
Gradient norm: 1.7651547193527222
Rank 2, Epoch 1, Batch 48, Loss: 2.307061
Gradient norm: 1.965380072593689
Rank 2, Epoch 1, Batch 49, Loss: 2.311171
Gradient norm: 1.38495671749115
Rank 2, Epoch 1, Batch 50, Loss: 2.296734
Gradient norm: 1.256521224975586
Rank 2, Epoch 1, Batch 51, Loss: 2.288312
Gradient norm: 1.7946659326553345
Rank 2, Epoch 1, Batch 52, Loss: 2.306704
Gradient norm: 1.5035758018493652
Rank 2, Epoch 1, Batch 53, Loss: 2.268732
Gradient norm: 2.3621139526367188
Rank 2, Epoch 1, Batch 54, Loss: 2.294930
Gradient norm: 1.9741390943527222
Rank 2, Epoch 1, Batch 55, Loss: 2.304674
Gradient norm: 3.1492533683776855
Rank 2, Epoch 1, Batch 56, Loss: 2.289080
Gradient norm: 1.5148918628692627
Rank 2, Epoch 1, Batch 57, Loss: 2.260306
Gradient norm: 2.341480016708374
Rank 2, Epoch 1, Batch 58, Loss: 2.220293
Gradient norm: 1.5102916955947876
Rank 2, Epoch 1, Batch 59, Loss: 2.286307
Gradient norm: 1.9889205694198608
Rank 2, Epoch 1, Batch 60, Loss: 2.302653
Gradient norm: 2.172981023788452
Rank 2, Epoch 1, Batch 61, Loss: 2.309626
Gradient norm: 2.472364664077759
Rank 2, Epoch 1, Batch 62, Loss: 2.292784
Gradient norm: 2.5744776725769043
Rank 2, Epoch 1, Batch 63, Loss: 2.258373
Gradient norm: 1.5290837287902832
Rank 2, Epoch 1, Batch 64, Loss: 2.258856
Gradient norm: 2.553771495819092
Rank 2, Epoch 1, Batch 65, Loss: 2.297807
Gradient norm: 2.523576021194458
Rank 2, Epoch 1, Batch 66, Loss: 2.293650
Gradient norm: 2.48305606842041
Rank 2, Epoch 1, Batch 67, Loss: 2.308408
Gradient norm: 3.23575496673584
Rank 2, Epoch 1, Batch 68, Loss: 2.259719
Gradient norm: 4.378779888153076
Rank 2, Epoch 1, Batch 69, Loss: 2.320673
Gradient norm: 4.5091872215271
Rank 2, Epoch 1, Batch 70, Loss: 2.340310
Gradient norm: 5.075138568878174
Rank 2, Epoch 1, Batch 71, Loss: 2.356787
Gradient norm: 2.300015449523926
Rank 2, Epoch 1, Batch 72, Loss: 2.273981
Gradient norm: 2.9470248222351074
Rank 2, Epoch 1, Batch 73, Loss: 2.290460
Gradient norm: 2.6128501892089844
Rank 2, Epoch 1, Batch 74, Loss: 2.263594
Gradient norm: 3.0889463424682617
Rank 2, Epoch 1, Batch 75, Loss: 2.277931
Gradient norm: 4.455533504486084
Rank 2, Epoch 1, Batch 76, Loss: 2.300101
Gradient norm: 4.64625883102417
Rank 2, Epoch 1, Batch 77, Loss: 2.350741
Gradient norm: 2.2582995891571045
Rank 2, Epoch 1, Batch 78, Loss: 2.279891
Gradient norm: 3.154304265975952
Rank 2, Epoch 1, Batch 79, Loss: 2.320314
Gradient norm: 2.2792952060699463
Rank 2, Epoch 1, Batch 80, Loss: 2.283478
Gradient norm: 2.846595525741577
Rank 2, Epoch 1, Batch 81, Loss: 2.268535
Gradient norm: 2.617114543914795
Rank 2, Epoch 1, Batch 82, Loss: 2.296530
Gradient norm: 4.292917251586914
Rank 2, Epoch 1, Batch 83, Loss: 2.299342
Gradient norm: 3.7678654193878174
Rank 2, Epoch 1, Batch 84, Loss: 2.294655
Gradient norm: 5.07857084274292
Rank 2, Epoch 1, Batch 85, Loss: 2.337925
Gradient norm: 4.050559997558594
Rank 2, Epoch 1, Batch 86, Loss: 2.260287
Gradient norm: 6.250019550323486
Rank 2, Epoch 1, Batch 87, Loss: 2.321674
Gradient norm: 2.5666959285736084
Rank 2, Epoch 1, Batch 88, Loss: 2.269280
Gradient norm: 6.094142436981201
Rank 2, Epoch 1, Batch 89, Loss: 2.335970
Gradient norm: 4.518429279327393
Rank 2, Epoch 1, Batch 90, Loss: 2.282801
Gradient norm: 4.003029823303223
Rank 2, Epoch 1, Batch 91, Loss: 2.283161
Gradient norm: 6.137782573699951
Rank 2, Epoch 1, Batch 92, Loss: 2.341548
Gradient norm: 6.708337783813477
Rank 2, Epoch 1, Batch 93, Loss: 2.337316
Gradient norm: 4.144464492797852
Rank 2, Epoch 1, Batch 94, Loss: 2.295377
Gradient norm: 5.920236110687256
Rank 2, Epoch 1, Batch 95, Loss: 2.295873
Gradient norm: 5.140852451324463
Rank 2, Epoch 1, Batch 96, Loss: 2.310719
Gradient norm: 5.649327278137207
Rank 2, Epoch 1, Batch 97, Loss: 2.310152
Gradient norm: 7.1690826416015625
Rank 2, Epoch 1, Batch 98, Loss: 2.277083
Gradient norm: 6.805390357971191
Rank 2, Epoch 1, Batch 99, Loss: 2.319804
Gradient norm: 7.00302267074585
Rank 2, Epoch 1, Batch 100, Loss: 2.350470
Gradient norm: 8.798556327819824
Rank 2, Epoch 1, Batch 101, Loss: 2.330247
Gradient norm: 6.299893379211426
Rank 2, Epoch 1, Batch 102, Loss: 2.279972
Gradient norm: 6.090559482574463
Rank 2, Epoch 1, Batch 103, Loss: 2.248581
Gradient norm: 8.173101425170898
Rank 2, Epoch 1, Batch 104, Loss: 2.327845
Gradient norm: 6.729799747467041
Rank 2, Epoch 1, Batch 105, Loss: 2.311484
Gradient norm: 6.814017295837402
Rank 2, Epoch 1, Batch 106, Loss: 2.336325
Gradient norm: 6.979425430297852
Rank 2, Epoch 1, Batch 107, Loss: 2.290460
Gradient norm: 1.6589158773422241
Rank 0, Epoch 1, Batch 0, Loss: 2.492291
Gradient norm: 0.10277305543422699
Rank 0, Epoch 1, Batch 1, Loss: 2.302983
Gradient norm: 0.11587407439947128
Rank 0, Epoch 1, Batch 2, Loss: 2.302065
Gradient norm: 0.08429814875125885
Rank 0, Epoch 1, Batch 3, Loss: 2.303887
Gradient norm: 0.09349949657917023
Rank 0, Epoch 1, Batch 4, Loss: 2.304074
Gradient norm: 0.10179967433214188
Rank 0, Epoch 1, Batch 5, Loss: 2.302322
Gradient norm: 0.14907170832157135
Rank 0, Epoch 1, Batch 6, Loss: 2.306261
Gradient norm: 0.08783543854951859
Rank 0, Epoch 1, Batch 7, Loss: 2.300842
Gradient norm: 0.12168482691049576
Rank 0, Epoch 1, Batch 8, Loss: 2.301213
Gradient norm: 0.12852665781974792
Rank 0, Epoch 1, Batch 9, Loss: 2.304128
Gradient norm: 0.10612685233354568
Rank 0, Epoch 1, Batch 10, Loss: 2.301319
Gradient norm: 0.18160909414291382
Rank 0, Epoch 1, Batch 11, Loss: 2.301773
Gradient norm: 0.21723942458629608
Rank 0, Epoch 1, Batch 12, Loss: 2.305999
Gradient norm: 0.26387929916381836
Rank 0, Epoch 1, Batch 13, Loss: 2.299980
Gradient norm: 0.2035485804080963
Rank 0, Epoch 1, Batch 14, Loss: 2.304048
Gradient norm: 0.1674027144908905
Rank 0, Epoch 1, Batch 15, Loss: 2.302404
Gradient norm: 0.19085024297237396
Rank 0, Epoch 1, Batch 16, Loss: 2.302444
Gradient norm: 0.18790777027606964
Rank 0, Epoch 1, Batch 17, Loss: 2.303386
Gradient norm: 0.23392139375209808
Rank 0, Epoch 1, Batch 18, Loss: 2.299356
Gradient norm: 0.320719838142395
Rank 0, Epoch 1, Batch 19, Loss: 2.301934
Gradient norm: 0.2929633855819702
Rank 0, Epoch 1, Batch 20, Loss: 2.307325
Gradient norm: 0.39425525069236755
Rank 0, Epoch 1, Batch 21, Loss: 2.299992
Gradient norm: 0.4519636034965515
Rank 0, Epoch 1, Batch 22, Loss: 2.303730
Gradient norm: 0.41789764165878296
Rank 0, Epoch 1, Batch 23, Loss: 2.306351
Gradient norm: 0.4029897153377533
Rank 0, Epoch 1, Batch 24, Loss: 2.298188
Gradient norm: 0.3367263078689575
Rank 0, Epoch 1, Batch 25, Loss: 2.296047
Gradient norm: 0.34877124428749084
Rank 0, Epoch 1, Batch 26, Loss: 2.303213
Gradient norm: 0.5830790996551514
Rank 0, Epoch 1, Batch 27, Loss: 2.301706
Gradient norm: 0.48433440923690796
Rank 0, Epoch 1, Batch 28, Loss: 2.308877
Gradient norm: 0.6231486797332764
Rank 0, Epoch 1, Batch 29, Loss: 2.297829
Gradient norm: 0.47558504343032837
Rank 0, Epoch 1, Batch 30, Loss: 2.298369
Gradient norm: 0.7211709022521973
Rank 0, Epoch 1, Batch 31, Loss: 2.293473
Gradient norm: 0.48868200182914734
Rank 0, Epoch 1, Batch 32, Loss: 2.303383
Gradient norm: 0.601085901260376
Rank 0, Epoch 1, Batch 33, Loss: 2.301510
Gradient norm: 0.9859974980354309
Rank 0, Epoch 1, Batch 34, Loss: 2.303375
Gradient norm: 0.6232786178588867
Rank 0, Epoch 1, Batch 35, Loss: 2.281783
Gradient norm: 0.9088068008422852
Rank 0, Epoch 1, Batch 36, Loss: 2.301760
Gradient norm: 1.0949071645736694
Rank 0, Epoch 1, Batch 37, Loss: 2.270067
Gradient norm: 0.9251611232757568
Rank 0, Epoch 1, Batch 38, Loss: 2.306354
Gradient norm: 0.6218345761299133
Rank 0, Epoch 1, Batch 39, Loss: 2.305295
Gradient norm: 1.1016697883605957
Rank 0, Epoch 1, Batch 40, Loss: 2.313596
Gradient norm: 1.3864176273345947
Rank 0, Epoch 1, Batch 41, Loss: 2.311139
Gradient norm: 1.2416068315505981
Rank 0, Epoch 1, Batch 42, Loss: 2.309000
Gradient norm: 1.6880074739456177
Rank 0, Epoch 1, Batch 43, Loss: 2.316841
Gradient norm: 1.2751775979995728
Rank 0, Epoch 1, Batch 44, Loss: 2.310166
Gradient norm: 1.5894895792007446
Rank 0, Epoch 1, Batch 45, Loss: 2.332273
Gradient norm: 1.767152190208435
Rank 0, Epoch 1, Batch 46, Loss: 2.308675
Gradient norm: 1.2158440351486206
Rank 0, Epoch 1, Batch 47, Loss: 2.299823
Gradient norm: 1.1905858516693115
Rank 0, Epoch 1, Batch 48, Loss: 2.289826
Gradient norm: 1.126903772354126
Rank 0, Epoch 1, Batch 49, Loss: 2.284136
Gradient norm: 1.3083542585372925
Rank 0, Epoch 1, Batch 50, Loss: 2.300798
Gradient norm: 1.1463603973388672
Rank 0, Epoch 1, Batch 51, Loss: 2.299455
Gradient norm: 1.4002114534378052
Rank 0, Epoch 1, Batch 52, Loss: 2.293601
Gradient norm: 1.3084150552749634
Rank 0, Epoch 1, Batch 53, Loss: 2.331889
Gradient norm: 1.4798471927642822
Rank 0, Epoch 1, Batch 54, Loss: 2.281358
Gradient norm: 2.6656575202941895
Rank 0, Epoch 1, Batch 55, Loss: 2.306265
Gradient norm: 1.2446192502975464
Rank 0, Epoch 1, Batch 56, Loss: 2.311057
Gradient norm: 1.393492341041565
Rank 0, Epoch 1, Batch 57, Loss: 2.302231
Gradient norm: 2.6923229694366455
Rank 0, Epoch 1, Batch 58, Loss: 2.301117
Gradient norm: 2.0159943103790283
Rank 0, Epoch 1, Batch 59, Loss: 2.279038
Gradient norm: 2.153859853744507
Rank 0, Epoch 1, Batch 60, Loss: 2.278389
Gradient norm: 2.455974578857422
Rank 0, Epoch 1, Batch 61, Loss: 2.320724
Gradient norm: 1.6227415800094604
Rank 0, Epoch 1, Batch 62, Loss: 2.271793
Gradient norm: 2.198725461959839
Rank 0, Epoch 1, Batch 63, Loss: 2.322583
Gradient norm: 2.859863042831421
Rank 0, Epoch 1, Batch 64, Loss: 2.356987
Gradient norm: 2.318509340286255
Rank 0, Epoch 1, Batch 65, Loss: 2.305747
Gradient norm: 2.2861897945404053
Rank 0, Epoch 1, Batch 66, Loss: 2.312993
Gradient norm: 1.8430029153823853
Rank 0, Epoch 1, Batch 67, Loss: 2.291841
Gradient norm: 2.926292657852173
Rank 0, Epoch 1, Batch 68, Loss: 2.333561
Gradient norm: 2.994412422180176
Rank 0, Epoch 1, Batch 69, Loss: 2.337614
Gradient norm: 2.104811906814575
Rank 0, Epoch 1, Batch 70, Loss: 2.314605
Gradient norm: 1.7751344442367554
Rank 0, Epoch 1, Batch 71, Loss: 2.272354
Gradient norm: 3.0771589279174805
Rank 0, Epoch 1, Batch 72, Loss: 2.280146
Gradient norm: 3.936575174331665
Rank 0, Epoch 1, Batch 73, Loss: 2.347117
Gradient norm: 2.8822596073150635
Rank 0, Epoch 1, Batch 74, Loss: 2.321882
Gradient norm: 3.6431586742401123
Rank 0, Epoch 1, Batch 75, Loss: 2.267023
Gradient norm: 4.585381984710693
Rank 0, Epoch 1, Batch 76, Loss: 2.327018
Gradient norm: 2.239901065826416
Rank 0, Epoch 1, Batch 77, Loss: 2.261347
Gradient norm: 3.2731058597564697
Rank 0, Epoch 1, Batch 78, Loss: 2.347507
Gradient norm: 2.82535457611084
Rank 0, Epoch 1, Batch 79, Loss: 2.315425
Gradient norm: 2.2680842876434326
Rank 0, Epoch 1, Batch 80, Loss: 2.304121
Gradient norm: 1.7070791721343994
Rank 0, Epoch 1, Batch 81, Loss: 2.270881
Gradient norm: 3.2322070598602295
Rank 0, Epoch 1, Batch 82, Loss: 2.300214
Gradient norm: 2.5074191093444824
Rank 0, Epoch 1, Batch 83, Loss: 2.264798
Gradient norm: 5.163521766662598
Rank 0, Epoch 1, Batch 84, Loss: 2.273569
Gradient norm: 2.881521224975586
Rank 0, Epoch 1, Batch 85, Loss: 2.294776
Gradient norm: 4.360340595245361
Rank 0, Epoch 1, Batch 86, Loss: 2.316003
Gradient norm: 4.763780117034912
Rank 0, Epoch 1, Batch 87, Loss: 2.325182
Gradient norm: 5.276333332061768
Rank 0, Epoch 1, Batch 88, Loss: 2.287563
Gradient norm: 5.682411193847656
Rank 0, Epoch 1, Batch 89, Loss: 2.340132
Gradient norm: 4.380630970001221
Rank 0, Epoch 1, Batch 90, Loss: 2.309346
Gradient norm: 7.205572128295898
Rank 0, Epoch 1, Batch 91, Loss: 2.276202
Gradient norm: 4.889017581939697
Rank 0, Epoch 1, Batch 92, Loss: 2.293878
Gradient norm: 4.529067516326904
Rank 0, Epoch 1, Batch 93, Loss: 2.294921
Gradient norm: 7.148725509643555
Rank 0, Epoch 1, Batch 94, Loss: 2.339251
Gradient norm: 5.1263957023620605
Rank 0, Epoch 1, Batch 95, Loss: 2.275128
Gradient norm: 4.790692329406738
Rank 0, Epoch 1, Batch 96, Loss: 2.288651
Gradient norm: 3.8790407180786133
Rank 0, Epoch 1, Batch 97, Loss: 2.321845
Gradient norm: 4.50449275970459
Rank 0, Epoch 1, Batch 98, Loss: 2.272717
Gradient norm: 6.483760356903076
Rank 0, Epoch 1, Batch 99, Loss: 2.296256
Gradient norm: 6.843641757965088
Rank 0, Epoch 1, Batch 100, Loss: 2.260942
Gradient norm: 4.736910343170166
Rank 0, Epoch 1, Batch 101, Loss: 2.296175
Gradient norm: 7.94019079208374
Rank 0, Epoch 1, Batch 102, Loss: 2.331020
Gradient norm: 5.666369915008545
Rank 0, Epoch 1, Batch 103, Loss: 2.301860
Gradient norm: 7.602566719055176
Rank 0, Epoch 1, Batch 104, Loss: 2.256291
Gradient norm: 7.2578606605529785
Rank 0, Epoch 1, Batch 105, Loss: 2.294660
Gradient norm: 5.7343034744262695
Rank 0, Epoch 1, Batch 106, Loss: 2.260200
Gradient norm: 10.31222915649414
Rank 0, Epoch 1, Batch 107, Loss: 2.369383
Gradient norm: 1.3646295070648193
Rank 1, Epoch 1, Batch 0, Loss: 2.355725
Gradient norm: 0.09046383947134018
Rank 1, Epoch 1, Batch 1, Loss: 2.303267
Gradient norm: 0.07826226204633713
Rank 1, Epoch 1, Batch 2, Loss: 2.302437
Gradient norm: 0.10323822498321533
Rank 1, Epoch 1, Batch 3, Loss: 2.300928
Gradient norm: 0.18180003762245178
Rank 1, Epoch 1, Batch 4, Loss: 2.302683
Gradient norm: 0.173068568110466
Rank 1, Epoch 1, Batch 5, Loss: 2.304221
Gradient norm: 0.09614607691764832
Rank 1, Epoch 1, Batch 6, Loss: 2.302371
Gradient norm: 0.1382278949022293
Rank 1, Epoch 1, Batch 7, Loss: 2.302794
Gradient norm: 0.25506484508514404
Rank 1, Epoch 1, Batch 8, Loss: 2.300682
Gradient norm: 0.1365361362695694
Rank 1, Epoch 1, Batch 9, Loss: 2.304211
Gradient norm: 0.20018188655376434
Rank 1, Epoch 1, Batch 10, Loss: 2.305558
Gradient norm: 0.2498152256011963
Rank 1, Epoch 1, Batch 11, Loss: 2.303684
Gradient norm: 0.17330734431743622
Rank 1, Epoch 1, Batch 12, Loss: 2.302960
Gradient norm: 0.20490853488445282
Rank 1, Epoch 1, Batch 13, Loss: 2.304754
Gradient norm: 0.3404991924762726
Rank 1, Epoch 1, Batch 14, Loss: 2.304199
Gradient norm: 0.397320955991745
Rank 1, Epoch 1, Batch 15, Loss: 2.301227
Gradient norm: 0.3737547695636749
Rank 1, Epoch 1, Batch 16, Loss: 2.304778
Gradient norm: 0.422542542219162
Rank 1, Epoch 1, Batch 17, Loss: 2.299247
Gradient norm: 0.3459475636482239
Rank 1, Epoch 1, Batch 18, Loss: 2.302099
Gradient norm: 0.5550330877304077
Rank 1, Epoch 1, Batch 19, Loss: 2.300824
Gradient norm: 0.47952044010162354
Rank 1, Epoch 1, Batch 20, Loss: 2.306194
Gradient norm: 0.761347234249115
Rank 1, Epoch 1, Batch 21, Loss: 2.306940
Gradient norm: 0.8601862192153931
Rank 1, Epoch 1, Batch 22, Loss: 2.306849
Gradient norm: 0.674667239189148
Rank 1, Epoch 1, Batch 23, Loss: 2.309438
Gradient norm: 0.8139823079109192
Rank 1, Epoch 1, Batch 24, Loss: 2.311046
Gradient norm: 0.8496175408363342
Rank 1, Epoch 1, Batch 25, Loss: 2.302438
Gradient norm: 1.2249724864959717
Rank 1, Epoch 1, Batch 26, Loss: 2.313819
Gradient norm: 1.288750410079956
Rank 1, Epoch 1, Batch 27, Loss: 2.310427
Gradient norm: 0.8765047788619995
Rank 1, Epoch 1, Batch 28, Loss: 2.309664
Gradient norm: 0.7717186212539673
Rank 1, Epoch 1, Batch 29, Loss: 2.304385
Gradient norm: 1.3529304265975952
Rank 1, Epoch 1, Batch 30, Loss: 2.298950
Gradient norm: 1.6909358501434326
Rank 1, Epoch 1, Batch 31, Loss: 2.306149
Gradient norm: 1.2204257249832153
Rank 1, Epoch 1, Batch 32, Loss: 2.308466
Gradient norm: 1.2350397109985352
Rank 1, Epoch 1, Batch 33, Loss: 2.312108
Gradient norm: 1.8375643491744995
Rank 1, Epoch 1, Batch 34, Loss: 2.301067
Gradient norm: 1.4151066541671753
Rank 1, Epoch 1, Batch 35, Loss: 2.313711
Gradient norm: 2.142702341079712
Rank 1, Epoch 1, Batch 36, Loss: 2.317760
Gradient norm: 1.7004663944244385
Rank 1, Epoch 1, Batch 37, Loss: 2.316394
Gradient norm: 1.7246943712234497
Rank 1, Epoch 1, Batch 38, Loss: 2.287476
Gradient norm: 1.9689193964004517
Rank 1, Epoch 1, Batch 39, Loss: 2.317259
Gradient norm: 1.7590168714523315
Rank 1, Epoch 1, Batch 40, Loss: 2.311044
Gradient norm: 1.7756397724151611
Rank 1, Epoch 1, Batch 41, Loss: 2.290478
Gradient norm: 1.7895143032073975
Rank 1, Epoch 1, Batch 42, Loss: 2.292399
Gradient norm: 2.370313882827759
Rank 1, Epoch 1, Batch 43, Loss: 2.286807
Gradient norm: 2.3744959831237793
Rank 1, Epoch 1, Batch 44, Loss: 2.300243
Gradient norm: 1.5878355503082275
Rank 1, Epoch 1, Batch 45, Loss: 2.277715
Gradient norm: 2.5878520011901855
Rank 1, Epoch 1, Batch 46, Loss: 2.286581
Gradient norm: 3.6868228912353516
Rank 1, Epoch 1, Batch 47, Loss: 2.294505
Gradient norm: 2.2531204223632812
Rank 1, Epoch 1, Batch 48, Loss: 2.275714
Gradient norm: 3.203307867050171
Rank 1, Epoch 1, Batch 49, Loss: 2.263431
Gradient norm: 2.9418773651123047
Rank 1, Epoch 1, Batch 50, Loss: 2.283242
Gradient norm: 3.7694289684295654
Rank 1, Epoch 1, Batch 51, Loss: 2.314748
Gradient norm: 3.441638231277466
Rank 1, Epoch 1, Batch 52, Loss: 2.288527
Gradient norm: 3.4434292316436768
Rank 1, Epoch 1, Batch 53, Loss: 2.339069
Gradient norm: 3.3500843048095703
Rank 1, Epoch 1, Batch 54, Loss: 2.284428
Gradient norm: 5.376218795776367
Rank 1, Epoch 1, Batch 55, Loss: 2.230486
Gradient norm: 3.09207820892334
Rank 1, Epoch 1, Batch 56, Loss: 2.308594
Gradient norm: 6.149220943450928
Rank 1, Epoch 1, Batch 57, Loss: 2.297716
Gradient norm: 3.288710594177246
Rank 1, Epoch 1, Batch 58, Loss: 2.270386
Gradient norm: 3.454875946044922
Rank 1, Epoch 1, Batch 59, Loss: 2.291084
Gradient norm: 4.697058200836182
Rank 1, Epoch 1, Batch 60, Loss: 2.254665
Gradient norm: 2.653748035430908
Rank 1, Epoch 1, Batch 61, Loss: 2.285117
Gradient norm: 5.074087619781494
Rank 1, Epoch 1, Batch 62, Loss: 2.317354
Gradient norm: 3.321880578994751
Rank 1, Epoch 1, Batch 63, Loss: 2.311791
Gradient norm: 3.4877331256866455
Rank 1, Epoch 1, Batch 64, Loss: 2.283690
Gradient norm: 6.3786702156066895
Rank 1, Epoch 1, Batch 65, Loss: 2.334899
Gradient norm: 8.557734489440918
Rank 1, Epoch 1, Batch 66, Loss: 2.379439
Gradient norm: 5.683524131774902
Rank 1, Epoch 1, Batch 67, Loss: 2.322778
Gradient norm: 5.333870887756348
Rank 1, Epoch 1, Batch 68, Loss: 2.286754
Gradient norm: 5.351977348327637
Rank 1, Epoch 1, Batch 69, Loss: 2.289321
Gradient norm: 7.681301116943359
Rank 1, Epoch 1, Batch 70, Loss: 2.362762
Gradient norm: 4.979537487030029
Rank 1, Epoch 1, Batch 71, Loss: 2.296496
Gradient norm: 8.320908546447754
Rank 1, Epoch 1, Batch 72, Loss: 2.269581
Gradient norm: 5.789790153503418
Rank 1, Epoch 1, Batch 73, Loss: 2.345222
Gradient norm: 4.254180908203125
Rank 1, Epoch 1, Batch 74, Loss: 2.283491
Gradient norm: 8.48588752746582
Rank 1, Epoch 1, Batch 75, Loss: 2.271663
Gradient norm: 5.134157180786133
Rank 1, Epoch 1, Batch 76, Loss: 2.274806
Gradient norm: 4.766875267028809
Rank 1, Epoch 1, Batch 77, Loss: 2.308449
Gradient norm: 9.385171890258789
Rank 1, Epoch 1, Batch 78, Loss: 2.342672
Gradient norm: 8.810495376586914
Rank 1, Epoch 1, Batch 79, Loss: 2.288558
Gradient norm: 10.100448608398438
Rank 1, Epoch 1, Batch 80, Loss: 2.374957
Gradient norm: 10.454390525817871
Rank 1, Epoch 1, Batch 81, Loss: 2.306973
Gradient norm: 14.302374839782715
Rank 1, Epoch 1, Batch 82, Loss: 2.244298
Gradient norm: 13.93915843963623
Rank 1, Epoch 1, Batch 83, Loss: 2.355382
Gradient norm: 10.918132781982422
Rank 1, Epoch 1, Batch 84, Loss: 2.309927
Gradient norm: 8.258567810058594
Rank 1, Epoch 1, Batch 85, Loss: 2.293083
Gradient norm: 13.082082748413086
Rank 1, Epoch 1, Batch 86, Loss: 2.337680
Gradient norm: 9.156743049621582
Rank 1, Epoch 1, Batch 87, Loss: 2.297319
Gradient norm: 10.879137992858887
Rank 1, Epoch 1, Batch 88, Loss: 2.255909
Gradient norm: 8.661820411682129
Rank 1, Epoch 1, Batch 89, Loss: 2.288300
Gradient norm: 11.462294578552246
Rank 1, Epoch 1, Batch 90, Loss: 2.373226
Gradient norm: 7.731534481048584
Rank 1, Epoch 1, Batch 91, Loss: 2.307633
Gradient norm: 12.571125030517578
Rank 1, Epoch 1, Batch 92, Loss: 2.341832
Gradient norm: 12.59250259399414
Rank 1, Epoch 1, Batch 93, Loss: 2.381969
Gradient norm: 10.317095756530762
Rank 1, Epoch 1, Batch 94, Loss: 2.325959
Gradient norm: 9.719903945922852
Rank 1, Epoch 1, Batch 95, Loss: 2.298202
Gradient norm: 13.703795433044434
Rank 1, Epoch 1, Batch 96, Loss: 2.386871
Gradient norm: 12.940272331237793
Rank 1, Epoch 1, Batch 97, Loss: 2.339047
Gradient norm: 16.603012084960938
Rank 1, Epoch 1, Batch 98, Loss: 2.368856
Gradient norm: 16.894107818603516
Rank 1, Epoch 1, Batch 99, Loss: 2.318455
Gradient norm: 18.818124771118164
Rank 1, Epoch 1, Batch 100, Loss: 2.425570
Gradient norm: 16.701622009277344
Rank 1, Epoch 1, Batch 101, Loss: 2.315454
Gradient norm: 15.358813285827637
Rank 1, Epoch 1, Batch 102, Loss: 2.383933
Gradient norm: 13.207758903503418
Rank 1, Epoch 1, Batch 103, Loss: 2.352591
Gradient norm: 24.875459671020508
Rank 1, Epoch 1, Batch 104, Loss: 2.377419
Gradient norm: 27.8885555267334
Rank 1, Epoch 1, Batch 105, Loss: 2.506682
Gradient norm: 19.016172409057617
Rank 1, Epoch 1, Batch 106, Loss: 2.392808
Gradient norm: 15.589146614074707
Rank 1, Epoch 1, Batch 107, Loss: 2.345800
Gradient norm: 23.08682632446289
Rank 1, Epoch 1, Batch 108, Loss: 2.378556
Gradient norm: 30.327829360961914
Rank 1, Epoch 1, Batch 109, Loss: 2.360414
Gradient norm: 32.592464447021484
Rank 1, Epoch 1, Batch 110, Loss: 2.543320
Gradient norm: 29.03302001953125
Rank 1, Epoch 1, Batch 111, Loss: 2.550560
Gradient norm: 22.39501190185547
Rank 1, Epoch 1, Batch 112, Loss: 2.341008
Gradient norm: 41.49700164794922
Rank 1, Epoch 1, Batch 113, Loss: 2.674273
Gradient norm: 50.64907455444336
Rank 1, Epoch 1, Batch 114, Loss: 2.946954
Gradient norm: 37.03297805786133
Rank 1, Epoch 1, Batch 115, Loss: 2.880469
Gradient norm: 26.860292434692383
Rank 1, Epoch 1, Batch 116, Loss: 2.521819
Gradient norm: 24.091148376464844
Rank 1, Epoch 1, Batch 117, Loss: 2.398459
Gradient norm: 43.14581298828125
Rank 1, Epoch 1, Batch 118, Loss: 2.653947
Gradient norm: 37.19921875
Rank 1, Epoch 1, Batch 119, Loss: 2.640220
Gradient norm: 25.207698822021484
Rank 1, Epoch 1, Batch 120, Loss: 2.524592
Gradient norm: 33.87279510498047
Rank 1, Epoch 1, Batch 121, Loss: 2.645256
Gradient norm: 33.01292037963867
Rank 1, Epoch 1, Batch 122, Loss: 2.457216
Gradient norm: 44.241783142089844
Rank 1, Epoch 1, Batch 123, Loss: 2.679231
Gradient norm: 29.89348602294922
Rank 1, Epoch 1, Batch 124, Loss: 2.611969
Gradient norm: 54.25687026977539
Rank 1, Epoch 1, Batch 125, Loss: 3.003508
Gradient norm: 43.88520050048828
Rank 1, Epoch 1, Batch 126, Loss: 2.996446
Gradient norm: 53.16185760498047
Rank 1, Epoch 1, Batch 127, Loss: 2.909361
Gradient norm: 55.71611404418945
Rank 1, Epoch 1, Batch 128, Loss: 2.900105
Gradient norm: 44.50674057006836
Rank 1, Epoch 1, Batch 129, Loss: 2.621945
Gradient norm: 103.76016998291016
Rank 1, Epoch 1, Batch 130, Loss: 4.267306
Gradient norm: 61.61369705200195
Rank 1, Epoch 1, Batch 131, Loss: 5.307859
Gradient norm: 75.48674774169922
Rank 1, Epoch 1, Batch 132, Loss: 3.449909
Gradient norm: 43.166648864746094
Rank 1, Epoch 1, Batch 133, Loss: 3.589005
Gradient norm: 52.4781379699707
Rank 1, Epoch 1, Batch 134, Loss: 3.452704
Gradient norm: 61.71387481689453
Rank 1, Epoch 1, Batch 135, Loss: 3.785975
Gradient norm: 40.436744689941406
Rank 1, Epoch 1, Batch 136, Loss: 2.778738
Gradient norm: 93.88016510009766
Rank 1, Epoch 1, Batch 137, Loss: 3.404749
Gradient norm: 54.41714096069336
Rank 1, Epoch 1, Batch 138, Loss: 4.093890
Gradient norm: 68.98419952392578
Rank 1, Epoch 1, Batch 139, Loss: 3.637882
Gradient norm: 104.05628204345703
Rank 1, Epoch 1, Batch 140, Loss: 4.874781
Gradient norm: 79.13320922851562
Rank 1, Epoch 1, Batch 141, Loss: 4.655057
Gradient norm: 87.97637939453125
Rank 1, Epoch 1, Batch 142, Loss: 5.381894
Gradient norm: 68.63616180419922
Rank 1, Epoch 1, Batch 143, Loss: 5.147868
Gradient norm: 119.70635986328125
Rank 1, Epoch 1, Batch 144, Loss: 5.568265
Gradient norm: 72.0385971069336
Rank 1, Epoch 1, Batch 145, Loss: 5.775684
Gradient norm: 84.3338623046875
Rank 1, Epoch 1, Batch 146, Loss: 5.235499
Gradient norm: 86.80657958984375
Rank 1, Epoch 1, Batch 147, Loss: 4.279521
Gradient norm: 122.1461410522461
Rank 1, Epoch 1, Batch 148, Loss: 5.132903
Gradient norm: 87.67799377441406
Rank 1, Epoch 1, Batch 149, Loss: 6.127681
Gradient norm: 144.0961151123047
Rank 1, Epoch 1, Batch 150, Loss: 8.915888
Gradient norm: 153.3195343017578
Rank 1, Epoch 1, Batch 151, Loss: 8.144625
Gradient norm: 130.98707580566406
Rank 1, Epoch 1, Batch 152, Loss: 7.650413
Gradient norm: 64.86613464355469
Rank 1, Epoch 1, Batch 153, Loss: 7.370990
Gradient norm: 109.20011138916016
Rank 1, Epoch 1, Batch 154, Loss: 8.368047
Gradient norm: 112.73897552490234
Rank 1, Epoch 1, Batch 155, Loss: 7.389958
Gradient norm: 118.97566223144531
Rank 1, Epoch 1, Batch 156, Loss: 7.836030
Gradient norm: 137.7697296142578
Rank 1, Epoch 1, Batch 157, Loss: 7.162434
Gradient norm: 109.0084228515625
Rank 1, Epoch 1, Batch 158, Loss: 9.340341
Gradient norm: 118.03700256347656
Rank 1, Epoch 1, Batch 159, Loss: 8.838037
Gradient norm: 106.12300109863281
Rank 1, Epoch 1, Batch 160, Loss: 8.330521
Gradient norm: 125.8335952758789
Rank 1, Epoch 1, Batch 161, Loss: 6.941552
Gradient norm: 127.35370635986328
Rank 1, Epoch 1, Batch 162, Loss: 7.148248
Gradient norm: 168.8673553466797
Rank 1, Epoch 1, Batch 163, Loss: 11.200430
Gradient norm: 184.77645874023438
Rank 1, Epoch 1, Batch 164, Loss: 11.169430
Gradient norm: 131.2017822265625
Rank 1, Epoch 1, Batch 165, Loss: 10.795362
Gradient norm: 176.46644592285156
Rank 1, Epoch 1, Batch 166, Loss: 13.343741
Gradient norm: 150.3952178955078
Rank 1, Epoch 1, Batch 167, Loss: 13.591352
Gradient norm: 183.1473846435547
Rank 1, Epoch 1, Batch 168, Loss: 13.861330
Gradient norm: 160.36135864257812
Rank 1, Epoch 1, Batch 169, Loss: 13.178393
Gradient norm: 202.75604248046875
Rank 1, Epoch 1, Batch 170, Loss: 20.698202
Gradient norm: 125.39799499511719
Rank 1, Epoch 1, Batch 171, Loss: 18.425695
Gradient norm: 222.98587036132812
Rank 1, Epoch 1, Batch 172, Loss: 14.047190
Gradient norm: 209.999267578125
Rank 1, Epoch 1, Batch 173, Loss: 18.117332
Gradient norm: 215.72650146484375
Rank 1, Epoch 1, Batch 174, Loss: 17.235140
Gradient norm: 94.62313842773438
Rank 1, Epoch 1, Batch 175, Loss: 14.561563
Gradient norm: 187.2375030517578
Rank 1, Epoch 1, Batch 176, Loss: 18.841942
Gradient norm: 238.6402130126953
Rank 1, Epoch 1, Batch 177, Loss: 18.385876
Gradient norm: 156.95782470703125
Rank 1, Epoch 1, Batch 178, Loss: 19.583038
Gradient norm: 216.8404083251953
Rank 1, Epoch 1, Batch 179, Loss: 23.031824
Gradient norm: 216.67759704589844
Rank 1, Epoch 1, Batch 180, Loss: 23.477249
Gradient norm: 207.918701171875
Rank 1, Epoch 1, Batch 181, Loss: 24.740870
Gradient norm: 214.0060272216797
Rank 1, Epoch 1, Batch 182, Loss: 20.322676
Gradient norm: 202.75961303710938
Rank 1, Epoch 1, Batch 183, Loss: 18.555395
Gradient norm: 238.49758911132812
Rank 1, Epoch 1, Batch 184, Loss: 21.449863
Gradient norm: 139.94956970214844
Rank 1, Epoch 1, Batch 185, Loss: 17.269215
Gradient norm: 244.24501037597656
Rank 1, Epoch 1, Batch 186, Loss: 23.484859
Gradient norm: 230.12747192382812
Rank 1, Epoch 1, Batch 187, Loss: 20.936844
Gradient norm: 272.5149841308594
Rank 1, Epoch 1, Batch 188, Loss: 39.170761
Gradient norm: 173.97483825683594
Rank 1, Epoch 1, Batch 189, Loss: 22.703800
Gradient norm: 242.52178955078125
Rank 1, Epoch 1, Batch 190, Loss: 23.384485
Gradient norm: 196.3096466064453
Rank 1, Epoch 1, Batch 191, Loss: 21.074963
Gradient norm: 284.9588317871094
Rank 1, Epoch 1, Batch 192, Loss: 27.047348
Gradient norm: 256.69134521484375
Rank 1, Epoch 1, Batch 193, Loss: 34.177853
Gradient norm: 260.7146301269531
Rank 1, Epoch 1, Batch 194, Loss: 29.730946
Gradient norm: 213.3930206298828
Rank 1, Epoch 1, Batch 195, Loss: 34.787766
Gradient norm: 253.92611694335938
Rank 1, Epoch 1, Batch 196, Loss: 32.938244
Gradient norm: 160.14208984375
Rank 1, Epoch 1, Batch 197, Loss: 23.176506
Gradient norm: 175.767822265625
Rank 1, Epoch 1, Batch 198, Loss: 20.706179
Gradient norm: 201.1731719970703
Rank 1, Epoch 1, Batch 199, Loss: 13.780751
Gradient norm: 282.6761779785156
Rank 1, Epoch 1, Batch 200, Loss: 22.155207
Gradient norm: 261.84100341796875
Rank 1, Epoch 1, Batch 201, Loss: 32.925259
Gradient norm: 200.87448120117188
Rank 1, Epoch 1, Batch 202, Loss: 29.091930
Gradient norm: 274.7871398925781
Rank 1, Epoch 1, Batch 203, Loss: 25.942102
Gradient norm: 306.383544921875
Rank 1, Epoch 1, Batch 204, Loss: 34.245571
Gradient norm: 214.53662109375
Rank 1, Epoch 1, Batch 205, Loss: 26.201618
Gradient norm: 304.1727294921875
Rank 1, Epoch 1, Batch 206, Loss: 45.394958
Gradient norm: 327.3042297363281
Rank 1, Epoch 1, Batch 207, Loss: 42.166382
Gradient norm: 244.54539489746094
Rank 1, Epoch 1, Batch 208, Loss: 39.440201
Gradient norm: 300.1447448730469
Rank 1, Epoch 1, Batch 209, Loss: 45.814987
Gradient norm: 273.048095703125
Rank 1, Epoch 1, Batch 210, Loss: 37.252274
Gradient norm: 337.7052001953125
Rank 1, Epoch 1, Batch 211, Loss: 37.338982
Gradient norm: 329.9974365234375
Rank 1, Epoch 1, Batch 212, Loss: 43.190331
Gradient norm: 259.97882080078125
Rank 1, Epoch 1, Batch 213, Loss: 52.196854
Gradient norm: 327.88702392578125
Gradient norm: 8.891940116882324
Rank 2, Epoch 1, Batch 108, Loss: 2.288382
Gradient norm: 5.420231342315674
Rank 2, Epoch 1, Batch 109, Loss: 2.273672
Gradient norm: 8.4193115234375
Rank 2, Epoch 1, Batch 110, Loss: 2.298285
Gradient norm: 4.404332637786865
Rank 2, Epoch 1, Batch 111, Loss: 2.299669
Gradient norm: 6.548079967498779
Rank 2, Epoch 1, Batch 112, Loss: 2.311148
Gradient norm: 11.322402954101562
Rank 2, Epoch 1, Batch 113, Loss: 2.318069
Gradient norm: 9.85955810546875
Rank 2, Epoch 1, Batch 114, Loss: 2.351025
Gradient norm: 6.991322994232178
Rank 2, Epoch 1, Batch 115, Loss: 2.307195
Gradient norm: 9.171330451965332
Rank 2, Epoch 1, Batch 116, Loss: 2.274045
Gradient norm: 9.468544006347656
Rank 2, Epoch 1, Batch 117, Loss: 2.336941
Gradient norm: 7.039214134216309
Rank 2, Epoch 1, Batch 118, Loss: 2.274101
Gradient norm: 7.2818803787231445
Rank 2, Epoch 1, Batch 119, Loss: 2.316487
Gradient norm: 8.209404945373535
Rank 2, Epoch 1, Batch 120, Loss: 2.333514
Gradient norm: 9.619736671447754
Rank 2, Epoch 1, Batch 121, Loss: 2.325979
Gradient norm: 9.229623794555664
Rank 2, Epoch 1, Batch 122, Loss: 2.293342
Gradient norm: 5.400469779968262
Rank 2, Epoch 1, Batch 123, Loss: 2.256901
Gradient norm: 11.107388496398926
Rank 2, Epoch 1, Batch 124, Loss: 2.349124
Gradient norm: 10.756189346313477
Rank 2, Epoch 1, Batch 125, Loss: 2.237722
Gradient norm: 8.764494895935059
Rank 2, Epoch 1, Batch 126, Loss: 2.285170
Gradient norm: 9.74166488647461
Rank 2, Epoch 1, Batch 127, Loss: 2.364918
Gradient norm: 11.400666236877441
Rank 2, Epoch 1, Batch 128, Loss: 2.340349
Gradient norm: 6.526996612548828
Rank 2, Epoch 1, Batch 129, Loss: 2.298692
Gradient norm: 10.637346267700195
Rank 2, Epoch 1, Batch 130, Loss: 2.299805
Gradient norm: 13.116352081298828
Rank 2, Epoch 1, Batch 131, Loss: 2.318658
Gradient norm: 10.157512664794922
Rank 2, Epoch 1, Batch 132, Loss: 2.322761
Gradient norm: 11.543034553527832
Rank 2, Epoch 1, Batch 133, Loss: 2.311837
Gradient norm: 6.667015552520752
Rank 2, Epoch 1, Batch 134, Loss: 2.293703
Gradient norm: 10.993096351623535
Rank 2, Epoch 1, Batch 135, Loss: 2.306502
Gradient norm: 8.077530860900879
Rank 2, Epoch 1, Batch 136, Loss: 2.309031
Gradient norm: 12.607563018798828
Rank 2, Epoch 1, Batch 137, Loss: 2.351449
Gradient norm: 13.951173782348633
Rank 2, Epoch 1, Batch 138, Loss: 2.340278
Gradient norm: 12.686142921447754
Rank 2, Epoch 1, Batch 139, Loss: 2.298832
Gradient norm: 14.110710144042969
Rank 2, Epoch 1, Batch 140, Loss: 2.318498
Gradient norm: 15.37826156616211
Rank 2, Epoch 1, Batch 141, Loss: 2.367210
Gradient norm: 17.995073318481445
Rank 2, Epoch 1, Batch 142, Loss: 2.360682
Gradient norm: 17.003381729125977
Rank 2, Epoch 1, Batch 143, Loss: 2.373206
Gradient norm: 17.502277374267578
Rank 2, Epoch 1, Batch 144, Loss: 2.330451
Gradient norm: 17.878128051757812
Rank 2, Epoch 1, Batch 145, Loss: 2.305931
Gradient norm: 11.809069633483887
Rank 2, Epoch 1, Batch 146, Loss: 2.309453
Gradient norm: 12.597797393798828
Rank 2, Epoch 1, Batch 147, Loss: 2.291534
Gradient norm: 12.131441116333008
Rank 2, Epoch 1, Batch 148, Loss: 2.270875
Gradient norm: 17.887601852416992
Rank 2, Epoch 1, Batch 149, Loss: 2.358490
Gradient norm: 17.0496768951416
Rank 2, Epoch 1, Batch 150, Loss: 2.333886
Gradient norm: 25.427791595458984
Rank 2, Epoch 1, Batch 151, Loss: 2.413927
Gradient norm: 12.71308422088623
Rank 2, Epoch 1, Batch 152, Loss: 2.312760
Gradient norm: 21.52115821838379
Rank 2, Epoch 1, Batch 153, Loss: 2.447496
Gradient norm: 24.137792587280273
Rank 2, Epoch 1, Batch 154, Loss: 2.436795
Gradient norm: 21.65013885498047
Rank 2, Epoch 1, Batch 155, Loss: 2.411461
Gradient norm: 20.399255752563477
Rank 2, Epoch 1, Batch 156, Loss: 2.366749
Gradient norm: 20.08462905883789
Rank 2, Epoch 1, Batch 157, Loss: 2.404456
Gradient norm: 13.783079147338867
Rank 2, Epoch 1, Batch 158, Loss: 2.269864
Gradient norm: 30.555545806884766
Rank 2, Epoch 1, Batch 159, Loss: 2.555253
Gradient norm: 17.97193717956543
Rank 2, Epoch 1, Batch 160, Loss: 2.363578
Gradient norm: 19.16573143005371
Rank 2, Epoch 1, Batch 161, Loss: 2.358287
Gradient norm: 31.819608688354492
Rank 2, Epoch 1, Batch 162, Loss: 2.491655
Gradient norm: 41.397151947021484
Rank 2, Epoch 1, Batch 163, Loss: 2.565697
Gradient norm: 46.58437728881836
Rank 2, Epoch 1, Batch 164, Loss: 2.851216
Gradient norm: 29.6823673248291
Rank 2, Epoch 1, Batch 165, Loss: 2.662708
Gradient norm: 27.041887283325195
Rank 2, Epoch 1, Batch 166, Loss: 2.673119
Gradient norm: 19.810623168945312
Rank 2, Epoch 1, Batch 167, Loss: 2.272656
Gradient norm: 36.983062744140625
Rank 2, Epoch 1, Batch 168, Loss: 2.592574
Gradient norm: 41.259429931640625
Rank 2, Epoch 1, Batch 169, Loss: 2.501128
Gradient norm: 37.91230773925781
Rank 2, Epoch 1, Batch 170, Loss: 2.786852
Gradient norm: 28.75722885131836
Rank 2, Epoch 1, Batch 171, Loss: 2.476232
Gradient norm: 48.984352111816406
Rank 2, Epoch 1, Batch 172, Loss: 2.739730
Gradient norm: 43.426998138427734
Rank 2, Epoch 1, Batch 173, Loss: 2.906370
Gradient norm: 28.965797424316406
Rank 2, Epoch 1, Batch 174, Loss: 2.774691
Gradient norm: 35.36832809448242
Rank 2, Epoch 1, Batch 175, Loss: 2.557698
Gradient norm: 42.647640228271484
Rank 2, Epoch 1, Batch 176, Loss: 2.684521
Gradient norm: 27.246658325195312
Rank 2, Epoch 1, Batch 177, Loss: 2.618368
Gradient norm: 28.29378890991211
Rank 2, Epoch 1, Batch 178, Loss: 2.460090
Gradient norm: 26.043556213378906
Rank 2, Epoch 1, Batch 179, Loss: 2.343739
Gradient norm: 44.04278564453125
Rank 2, Epoch 1, Batch 180, Loss: 2.588622
Gradient norm: 32.18434524536133
Rank 2, Epoch 1, Batch 181, Loss: 2.845842
Gradient norm: 27.90825080871582
Rank 2, Epoch 1, Batch 182, Loss: 2.448147
Gradient norm: 25.56544303894043
Rank 2, Epoch 1, Batch 183, Loss: 2.416046
Gradient norm: 73.57512664794922
Rank 2, Epoch 1, Batch 184, Loss: 2.747502
Gradient norm: 65.57563018798828
Rank 2, Epoch 1, Batch 185, Loss: 3.746444
Gradient norm: 35.224491119384766
Rank 2, Epoch 1, Batch 186, Loss: 2.646929
Gradient norm: 79.00224304199219
Rank 2, Epoch 1, Batch 187, Loss: 4.468721
Gradient norm: 64.34085083007812
Rank 2, Epoch 1, Batch 188, Loss: 4.085452
Gradient norm: 64.26702880859375
Rank 2, Epoch 1, Batch 189, Loss: 4.049715
Gradient norm: 65.16563415527344
Rank 2, Epoch 1, Batch 190, Loss: 2.897376
Gradient norm: 43.2423210144043
Rank 2, Epoch 1, Batch 191, Loss: 3.583989
Gradient norm: 76.5674057006836
Rank 2, Epoch 1, Batch 192, Loss: 4.227139
Gradient norm: 73.09711456298828
Rank 2, Epoch 1, Batch 193, Loss: 3.557172
Gradient norm: 65.97269439697266
Rank 2, Epoch 1, Batch 194, Loss: 4.071301
Gradient norm: 97.36429595947266
Rank 2, Epoch 1, Batch 195, Loss: 4.977361
Gradient norm: 51.23341369628906
Rank 2, Epoch 1, Batch 196, Loss: 3.893452
Gradient norm: 53.38011169433594
Rank 2, Epoch 1, Batch 197, Loss: 3.028342
Gradient norm: 56.622737884521484
Rank 2, Epoch 1, Batch 198, Loss: 3.110705
Gradient norm: 121.4101333618164
Rank 2, Epoch 1, Batch 199, Loss: 4.733712
Gradient norm: 63.50336456298828
Rank 2, Epoch 1, Batch 200, Loss: 4.260849
Gradient norm: 69.85420989990234
Rank 2, Epoch 1, Batch 201, Loss: 4.526037
Gradient norm: 121.20824432373047
Rank 2, Epoch 1, Batch 202, Loss: 5.958144
Gradient norm: 40.3880729675293
Rank 2, Epoch 1, Batch 203, Loss: 3.596193
Gradient norm: 64.32396697998047
Rank 2, Epoch 1, Batch 204, Loss: 4.696088
Gradient norm: 139.1015625
Rank 2, Epoch 1, Batch 205, Loss: 6.041543
Gradient norm: 103.48963928222656
Rank 2, Epoch 1, Batch 206, Loss: 5.232277
Gradient norm: 106.41107177734375
Rank 2, Epoch 1, Batch 207, Loss: 7.078716
Gradient norm: 66.37267303466797
Rank 2, Epoch 1, Batch 208, Loss: 6.413394
Gradient norm: 100.37689971923828
Rank 2, Epoch 1, Batch 209, Loss: 6.478029
Gradient norm: 114.97508239746094
Rank 2, Epoch 1, Batch 210, Loss: 6.887806
Gradient norm: 90.77139282226562
Rank 2, Epoch 1, Batch 211, Loss: 6.768909
Gradient norm: 156.66175842285156
Rank 2, Epoch 1, Batch 212, Loss: 7.672893
Gradient norm: 111.6743392944336
Rank 2, Epoch 1, Batch 213, Loss: 7.408556
Gradient norm: 162.95423889160156
Rank 2, Epoch 1, Batch 214, Loss: 9.158754
Gradient norm: 5.432600975036621
Rank 0, Epoch 1, Batch 108, Loss: 2.282221
Gradient norm: 7.060153961181641
Rank 0, Epoch 1, Batch 109, Loss: 2.348600
Gradient norm: 7.992204666137695
Rank 0, Epoch 1, Batch 110, Loss: 2.273432
Gradient norm: 9.03602409362793
Rank 0, Epoch 1, Batch 111, Loss: 2.331089
Gradient norm: 5.865020275115967
Rank 0, Epoch 1, Batch 112, Loss: 2.277617
Gradient norm: 7.276764869689941
Rank 0, Epoch 1, Batch 113, Loss: 2.260139
Gradient norm: 6.4376091957092285
Rank 0, Epoch 1, Batch 114, Loss: 2.273209
Gradient norm: 3.392303228378296
Rank 0, Epoch 1, Batch 115, Loss: 2.245078
Gradient norm: 10.266584396362305
Rank 0, Epoch 1, Batch 116, Loss: 2.225848
Gradient norm: 11.019798278808594
Rank 0, Epoch 1, Batch 117, Loss: 2.332894
Gradient norm: 10.279269218444824
Rank 0, Epoch 1, Batch 118, Loss: 2.325111
Gradient norm: 6.327846527099609
Rank 0, Epoch 1, Batch 119, Loss: 2.272978
Gradient norm: 8.823342323303223
Rank 0, Epoch 1, Batch 120, Loss: 2.331508
Gradient norm: 11.089386940002441
Rank 0, Epoch 1, Batch 121, Loss: 2.380721
Gradient norm: 11.515460014343262
Rank 0, Epoch 1, Batch 122, Loss: 2.314169
Gradient norm: 10.47953987121582
Rank 0, Epoch 1, Batch 123, Loss: 2.361018
Gradient norm: 12.705734252929688
Rank 0, Epoch 1, Batch 124, Loss: 2.282132
Gradient norm: 9.402572631835938
Rank 0, Epoch 1, Batch 125, Loss: 2.308640
Gradient norm: 9.718415260314941
Rank 0, Epoch 1, Batch 126, Loss: 2.268949
Gradient norm: 13.637804985046387
Rank 0, Epoch 1, Batch 127, Loss: 2.398065
Gradient norm: 7.409400939941406
Rank 0, Epoch 1, Batch 128, Loss: 2.296039
Gradient norm: 14.178160667419434
Rank 0, Epoch 1, Batch 129, Loss: 2.341426
Gradient norm: 12.213180541992188
Rank 0, Epoch 1, Batch 130, Loss: 2.364825
Gradient norm: 10.819879531860352
Rank 0, Epoch 1, Batch 131, Loss: 2.327320
Gradient norm: 9.843713760375977
Rank 0, Epoch 1, Batch 132, Loss: 2.284628
Gradient norm: 13.618368148803711
Rank 0, Epoch 1, Batch 133, Loss: 2.369029
Gradient norm: 14.501916885375977
Rank 0, Epoch 1, Batch 134, Loss: 2.305129
Gradient norm: 20.4388370513916
Rank 0, Epoch 1, Batch 135, Loss: 2.432689
Gradient norm: 15.477700233459473
Rank 0, Epoch 1, Batch 136, Loss: 2.340953
Gradient norm: 11.387797355651855
Rank 0, Epoch 1, Batch 137, Loss: 2.314383
Gradient norm: 11.513385772705078
Rank 0, Epoch 1, Batch 138, Loss: 2.317372
Gradient norm: 11.090886116027832
Rank 0, Epoch 1, Batch 139, Loss: 2.315225
Gradient norm: 14.30364990234375
Rank 0, Epoch 1, Batch 140, Loss: 2.335709
Gradient norm: 16.82686996459961
Rank 0, Epoch 1, Batch 141, Loss: 2.303103
Gradient norm: 26.681602478027344
Rank 0, Epoch 1, Batch 142, Loss: 2.495467
Gradient norm: 16.051315307617188
Rank 0, Epoch 1, Batch 143, Loss: 2.403121
Gradient norm: 11.092506408691406
Rank 0, Epoch 1, Batch 144, Loss: 2.295032
Gradient norm: 14.736811637878418
Rank 0, Epoch 1, Batch 145, Loss: 2.314064
Gradient norm: 26.63575553894043
Rank 0, Epoch 1, Batch 146, Loss: 2.497808
Gradient norm: 18.39887809753418
Rank 0, Epoch 1, Batch 147, Loss: 2.440809
Gradient norm: 14.955704689025879
Rank 0, Epoch 1, Batch 148, Loss: 2.317177
Gradient norm: 18.315298080444336
Rank 0, Epoch 1, Batch 149, Loss: 2.366208
Gradient norm: 14.753968238830566
Rank 0, Epoch 1, Batch 150, Loss: 2.364410
Gradient norm: 14.367217063903809
Rank 0, Epoch 1, Batch 151, Loss: 2.337457
Gradient norm: 19.066686630249023
Rank 0, Epoch 1, Batch 152, Loss: 2.341335
Gradient norm: 19.459728240966797
Rank 0, Epoch 1, Batch 153, Loss: 2.400344
Gradient norm: 24.2381591796875
Rank 0, Epoch 1, Batch 154, Loss: 2.334241
Gradient norm: 40.45438003540039
Rank 0, Epoch 1, Batch 155, Loss: 2.638182
Gradient norm: 28.044466018676758
Rank 0, Epoch 1, Batch 156, Loss: 2.623171
Gradient norm: 21.834487915039062
Rank 0, Epoch 1, Batch 157, Loss: 2.354215
Gradient norm: 28.94123649597168
Rank 0, Epoch 1, Batch 158, Loss: 2.397714
Gradient norm: 34.332523345947266
Rank 0, Epoch 1, Batch 159, Loss: 2.548132
Gradient norm: 19.116445541381836
Rank 0, Epoch 1, Batch 160, Loss: 2.391647
Gradient norm: 25.32642364501953
Rank 0, Epoch 1, Batch 161, Loss: 2.522931
Gradient norm: 21.631553649902344
Rank 0, Epoch 1, Batch 162, Loss: 2.413739
Gradient norm: 16.377681732177734
Rank 0, Epoch 1, Batch 163, Loss: 2.325275
Gradient norm: 27.13235092163086
Rank 0, Epoch 1, Batch 164, Loss: 2.453279
Gradient norm: 34.15786361694336
Rank 0, Epoch 1, Batch 165, Loss: 2.588711
Gradient norm: 24.834583282470703
Rank 0, Epoch 1, Batch 166, Loss: 2.382325
Gradient norm: 31.314651489257812
Rank 0, Epoch 1, Batch 167, Loss: 2.426138
Gradient norm: 36.75019836425781
Rank 0, Epoch 1, Batch 168, Loss: 2.691239
Gradient norm: 34.70867919921875
Rank 0, Epoch 1, Batch 169, Loss: 2.608910
Gradient norm: 56.39284896850586
Rank 0, Epoch 1, Batch 170, Loss: 2.856446
Gradient norm: 42.46424865722656
Rank 0, Epoch 1, Batch 171, Loss: 2.465620
Gradient norm: 45.75852584838867
Rank 0, Epoch 1, Batch 172, Loss: 3.392313
Gradient norm: 48.36225128173828
Rank 0, Epoch 1, Batch 173, Loss: 3.147471
Gradient norm: 52.1907844543457
Rank 0, Epoch 1, Batch 174, Loss: 2.718055
Gradient norm: 50.65185546875
Rank 0, Epoch 1, Batch 175, Loss: 3.620399
Gradient norm: 33.32392883300781
Rank 0, Epoch 1, Batch 176, Loss: 2.681057
Gradient norm: 30.545276641845703
Rank 0, Epoch 1, Batch 177, Loss: 2.494295
Gradient norm: 31.19681739807129
Rank 0, Epoch 1, Batch 178, Loss: 2.493053
Gradient norm: 48.179420471191406
Rank 0, Epoch 1, Batch 179, Loss: 2.791392
Gradient norm: 54.114654541015625
Rank 0, Epoch 1, Batch 180, Loss: 3.034567
Gradient norm: 60.434635162353516
Rank 0, Epoch 1, Batch 181, Loss: 3.335958
Gradient norm: 40.78252410888672
Rank 0, Epoch 1, Batch 182, Loss: 3.129325
Gradient norm: 36.20668029785156
Rank 0, Epoch 1, Batch 183, Loss: 2.638687
Gradient norm: 30.48097801208496
Rank 0, Epoch 1, Batch 184, Loss: 2.566596
Gradient norm: 19.34841537475586
Rank 0, Epoch 1, Batch 185, Loss: 2.274861
Gradient norm: 51.19308853149414
Rank 0, Epoch 1, Batch 186, Loss: 2.698236
Gradient norm: 56.567657470703125
Rank 0, Epoch 1, Batch 187, Loss: 2.983575
Gradient norm: 88.34104919433594
Rank 0, Epoch 1, Batch 188, Loss: 3.830266
Gradient norm: 41.788299560546875
Rank 0, Epoch 1, Batch 189, Loss: 3.467571
Gradient norm: 73.77222442626953
Rank 0, Epoch 1, Batch 190, Loss: 4.112440
Gradient norm: 99.69068908691406
Rank 0, Epoch 1, Batch 191, Loss: 4.633710
Gradient norm: 51.017547607421875
Rank 0, Epoch 1, Batch 192, Loss: 3.690143
Gradient norm: 69.86632537841797
Rank 0, Epoch 1, Batch 193, Loss: 5.150390
Gradient norm: 120.78157043457031
Rank 0, Epoch 1, Batch 194, Loss: 5.036361
Gradient norm: 84.37236785888672
Rank 0, Epoch 1, Batch 195, Loss: 4.530881
Gradient norm: 60.22254180908203
Rank 0, Epoch 1, Batch 196, Loss: 5.157057
Gradient norm: 58.45817947387695
Rank 0, Epoch 1, Batch 197, Loss: 4.904697
Gradient norm: 79.78945922851562
Rank 0, Epoch 1, Batch 198, Loss: 3.710665
Gradient norm: 87.49644470214844
Rank 0, Epoch 1, Batch 199, Loss: 4.603559
Gradient norm: 91.71558380126953
Rank 0, Epoch 1, Batch 200, Loss: 4.808730
Gradient norm: 64.79862976074219
Rank 0, Epoch 1, Batch 201, Loss: 4.611104
Gradient norm: 71.43811798095703
Rank 0, Epoch 1, Batch 202, Loss: 4.453261
Gradient norm: 127.63499450683594
Rank 0, Epoch 1, Batch 203, Loss: 5.317659
Gradient norm: 72.16724395751953
Rank 0, Epoch 1, Batch 204, Loss: 4.495887
Gradient norm: 51.42633056640625
Rank 0, Epoch 1, Batch 205, Loss: 4.388629
Gradient norm: 69.03949737548828
Rank 0, Epoch 1, Batch 206, Loss: 4.778498
Gradient norm: 91.11431884765625
Rank 0, Epoch 1, Batch 207, Loss: 4.963959
Gradient norm: 109.70498657226562
Rank 0, Epoch 1, Batch 208, Loss: 5.359949
Gradient norm: 79.71873474121094
Rank 0, Epoch 1, Batch 209, Loss: 4.746443
Gradient norm: 82.36015319824219
Rank 0, Epoch 1, Batch 210, Loss: 5.779223
Gradient norm: 111.47354888916016
Rank 0, Epoch 1, Batch 211, Loss: 5.282746
Gradient norm: 85.88573455810547
Rank 0, Epoch 1, Batch 212, Loss: 4.949928
Gradient norm: 139.1365966796875
Rank 0, Epoch 1, Batch 213, Loss: 6.345440
Gradient norm: 110.02591705322266
Rank 0, Epoch 1, Batch 214, Loss: 7.412670
Gradient norm: 113.99278259277344
Rank 0, Epoch 1, Batch 215, Loss: 7.773087
Gradient norm: 121.53053283691406
Rank 0, Epoch 1, Batch 216, Loss: 9.839078
Gradient norm: 111.7038345336914
Rank 0, Epoch 1, Batch 217, Loss: 9.681519
Gradient norm: 90.63292694091797
Rank 0, Epoch 1, Batch 218, Loss: 6.377898
Gradient norm: 136.20201110839844
Rank 0, Epoch 1, Batch 219, Loss: 5.169380
Gradient norm: 73.73844146728516
Rank 0, Epoch 1, Batch 220, Loss: 7.821076
Gradient norm: 113.08604431152344
Rank 0, Epoch 1, Batch 221, Loss: 7.281260
Gradient norm: 92.73039245605469
Rank 0, Epoch 1, Batch 222, Loss: 4.455723
Gradient norm: 116.67536926269531
Rank 0, Epoch 1, Batch 223, Loss: 4.658346
Gradient norm: 115.52664184570312
Rank 0, Epoch 1, Batch 224, Loss: 8.865707
Gradient norm: 170.21456909179688
Rank 0, Epoch 1, Batch 225, Loss: 8.242094
Gradient norm: 121.12165832519531
Rank 0, Epoch 1, Batch 226, Loss: 11.797859
Gradient norm: 137.2158660888672
Rank 0, Epoch 1, Batch 227, Loss: 11.939376
Gradient norm: 120.25467681884766
Rank 0, Epoch 1, Batch 228, Loss: 10.652946
Gradient norm: 115.97257995605469
Rank 0, Epoch 1, Batch 229, Loss: 7.385885
Gradient norm: 165.8768310546875
Rank 0, Epoch 1, Batch 230, Loss: 8.142819
Gradient norm: 127.03073120117188
Rank 0, Epoch 1, Batch 231, Loss: 9.599018
Gradient norm: 118.99260711669922
Rank 0, Epoch 1, Batch 232, Loss: 11.217548
Gradient norm: 182.21133422851562
Rank 0, Epoch 1, Batch 233, Loss: 10.556952
Gradient norm: 139.73538208007812
Rank 0, Epoch 1, Batch 234, Loss: 11.343713
Gradient norm: 201.99774169921875
Rank 0, Epoch 1, Batch 235, Loss: 10.320435
Gradient norm: 112.14837646484375
Rank 0, Epoch 1, Batch 236, Loss: 8.286701
Gradient norm: 134.15219116210938
Rank 0, Epoch 1, Batch 237, Loss: 13.186825
Gradient norm: 136.72137451171875
Rank 0, Epoch 1, Batch 238, Loss: 17.856699
Gradient norm: 197.5453338623047
Rank 0, Epoch 1, Batch 239, Loss: 13.955933
Gradient norm: 146.49090576171875
Rank 0, Epoch 1, Batch 240, Loss: 12.299916
Gradient norm: 178.0677032470703
Rank 0, Epoch 1, Batch 241, Loss: 16.959888
Gradient norm: 220.34011840820312
Rank 0, Epoch 1, Batch 242, Loss: 13.535534
Gradient norm: 220.68344116210938
Rank 0, Epoch 1, Batch 243, Loss: 19.920313
Gradient norm: 230.69412231445312
Rank 0, Epoch 1, Batch 244, Loss: 25.335789
Gradient norm: 139.3528594970703
Rank 0, Epoch 1, Batch 245, Loss: 18.326397
Gradient norm: 148.77967834472656
Rank 0, Epoch 1, Batch 246, Loss: 14.817753
Gradient norm: 191.6847686767578
Rank 0, Epoch 1, Batch 247, Loss: 16.188822
Gradient norm: 226.8062286376953
Rank 0, Epoch 1, Batch 248, Loss: 14.521570
Gradient norm: 229.5820770263672
Rank 0, Epoch 1, Batch 249, Loss: 24.414713
Gradient norm: 124.28488159179688
Rank 0, Epoch 1, Batch 250, Loss: 20.583002
Gradient norm: 223.19866943359375
Rank 0, Epoch 1, Batch 251, Loss: 20.541697
Gradient norm: 226.5696563720703
Rank 0, Epoch 1, Batch 252, Loss: 19.999886
Gradient norm: 170.7405242919922
Rank 0, Epoch 1, Batch 253, Loss: 16.792191
Gradient norm: 184.76490783691406
Rank 0, Epoch 1, Batch 254, Loss: 18.764069
Gradient norm: 147.42758178710938
Rank 0, Epoch 1, Batch 255, Loss: 19.661648
Gradient norm: 250.20721435546875
Rank 0, Epoch 1, Batch 256, Loss: 20.881973
Gradient norm: 244.0364227294922
Rank 0, Epoch 1, Batch 257, Loss: 26.242973
Gradient norm: 235.5217742919922
Rank 0, Epoch 1, Batch 258, Loss: 19.195642
Gradient norm: 221.89096069335938
Rank 0, Epoch 1, Batch 259, Loss: 25.892414
Gradient norm: 254.7744598388672
Rank 0, Epoch 1, Batch 260, Loss: 21.041859
Rank 0, Epoch 1, Val Loss: 7.8557, Val Acc: 0.1000, Time: 211.34s
Rank 1, Epoch 1, Batch 214, Loss: 48.393925
Gradient norm: 323.2822265625
Rank 1, Epoch 1, Batch 215, Loss: 52.492687
Gradient norm: 319.7298583984375
Rank 1, Epoch 1, Batch 216, Loss: 52.217438
Gradient norm: 344.8053283691406
Rank 1, Epoch 1, Batch 217, Loss: 61.072010
Gradient norm: 327.54339599609375
Rank 1, Epoch 1, Batch 218, Loss: 43.745079
Gradient norm: 342.58026123046875
Rank 1, Epoch 1, Batch 219, Loss: 53.221260
Gradient norm: 351.43389892578125
Rank 1, Epoch 1, Batch 220, Loss: 70.778091
Gradient norm: 353.4917907714844
Rank 1, Epoch 1, Batch 221, Loss: 73.884590
Gradient norm: 362.5368347167969
Rank 1, Epoch 1, Batch 222, Loss: 69.515709
Gradient norm: 388.3077392578125
Rank 1, Epoch 1, Batch 223, Loss: 42.506443
Gradient norm: 371.47808837890625
Rank 1, Epoch 1, Batch 224, Loss: 40.216526
Gradient norm: 300.0992431640625
Rank 1, Epoch 1, Batch 225, Loss: 39.487759
Gradient norm: 222.49374389648438
Rank 1, Epoch 1, Batch 226, Loss: 47.649555
Gradient norm: 370.845947265625
Rank 1, Epoch 1, Batch 227, Loss: 52.110859
Gradient norm: 331.8203430175781
Rank 1, Epoch 1, Batch 228, Loss: 63.363384
Gradient norm: 315.9518737792969
Rank 1, Epoch 1, Batch 229, Loss: 56.573902
Gradient norm: 378.830078125
Rank 1, Epoch 1, Batch 230, Loss: 65.864265
Gradient norm: 350.8023681640625
Rank 1, Epoch 1, Batch 231, Loss: 66.266785
Gradient norm: 364.87420654296875
Rank 1, Epoch 1, Batch 232, Loss: 80.545341
Gradient norm: 386.3562316894531
Rank 1, Epoch 1, Batch 233, Loss: 71.009796
Gradient norm: 413.4178161621094
Rank 1, Epoch 1, Batch 234, Loss: 92.926735
Gradient norm: 431.1352233886719
Rank 1, Epoch 1, Batch 235, Loss: 78.360809
Gradient norm: 357.070068359375
Rank 1, Epoch 1, Batch 236, Loss: 54.534893
Gradient norm: 408.91473388671875
Rank 1, Epoch 1, Batch 237, Loss: 58.425777
Gradient norm: 381.1801452636719
Rank 1, Epoch 1, Batch 238, Loss: 63.822754
Gradient norm: 389.33795166015625
Rank 1, Epoch 1, Batch 239, Loss: 69.076447
Gradient norm: 412.9798583984375
Rank 1, Epoch 1, Batch 240, Loss: 101.640205
Gradient norm: 421.7975769042969
Rank 1, Epoch 1, Batch 241, Loss: 101.984329
Gradient norm: 426.54229736328125
Rank 1, Epoch 1, Batch 242, Loss: 100.564087
Gradient norm: 439.3117370605469
Rank 1, Epoch 1, Batch 243, Loss: 85.481476
Gradient norm: 416.5284423828125
Rank 1, Epoch 1, Batch 244, Loss: 79.075211
Gradient norm: 377.3324279785156
Rank 1, Epoch 1, Batch 245, Loss: 59.093529
Gradient norm: 434.3410339355469
Rank 1, Epoch 1, Batch 246, Loss: 71.932159
Gradient norm: 417.9695739746094
Rank 1, Epoch 1, Batch 247, Loss: 76.021538
Gradient norm: 397.2520751953125
Rank 1, Epoch 1, Batch 248, Loss: 91.403687
Gradient norm: 459.33013916015625
Rank 1, Epoch 1, Batch 249, Loss: 110.776154
Gradient norm: 478.9149475097656
Rank 1, Epoch 1, Batch 250, Loss: 111.032829
Gradient norm: 474.0746765136719
Rank 1, Epoch 1, Batch 251, Loss: 119.432419
Gradient norm: 428.01904296875
Rank 1, Epoch 1, Batch 252, Loss: 67.624603
Gradient norm: 424.75439453125
Rank 1, Epoch 1, Batch 253, Loss: 69.952492
Gradient norm: 407.98651123046875
Rank 1, Epoch 1, Batch 254, Loss: 84.698967
Gradient norm: 487.2110290527344
Rank 1, Epoch 1, Batch 255, Loss: 107.885086
Gradient norm: 477.2250061035156
Rank 1, Epoch 1, Batch 256, Loss: 114.880394
Gradient norm: 486.580810546875
Rank 1, Epoch 1, Batch 257, Loss: 127.324699
Gradient norm: 449.0050048828125
Rank 1, Epoch 1, Batch 258, Loss: 121.855270
Gradient norm: 486.2754211425781
Rank 1, Epoch 1, Batch 259, Loss: 108.548424
Gradient norm: 485.1321716308594
Rank 1, Epoch 1, Batch 260, Loss: 97.469368
Rank 1, Epoch 1, Val Loss: 2093.6847, Val Acc: 0.1000, Time: 212.34s
Gradient norm: 105.02803802490234
Rank 2, Epoch 1, Batch 215, Loss: 7.952663
Gradient norm: 108.17374420166016
Rank 2, Epoch 1, Batch 216, Loss: 9.683286
Gradient norm: 125.19091033935547
Rank 2, Epoch 1, Batch 217, Loss: 7.440211
Gradient norm: 139.97531127929688
Rank 2, Epoch 1, Batch 218, Loss: 10.838202
Gradient norm: 143.74937438964844
Rank 2, Epoch 1, Batch 219, Loss: 9.958301
Gradient norm: 134.9375762939453
Rank 2, Epoch 1, Batch 220, Loss: 12.055678
Gradient norm: 159.5045928955078
Rank 2, Epoch 1, Batch 221, Loss: 7.261317
Gradient norm: 164.30763244628906
Rank 2, Epoch 1, Batch 222, Loss: 12.056236
Gradient norm: 140.01109313964844
Rank 2, Epoch 1, Batch 223, Loss: 11.660796
Gradient norm: 132.56289672851562
Rank 2, Epoch 1, Batch 224, Loss: 8.579470
Gradient norm: 175.03311157226562
Rank 2, Epoch 1, Batch 225, Loss: 14.283522
Gradient norm: 160.6697540283203
Rank 2, Epoch 1, Batch 226, Loss: 13.204345
Gradient norm: 87.58316040039062
Rank 2, Epoch 1, Batch 227, Loss: 11.749564
Gradient norm: 122.60614776611328
Rank 2, Epoch 1, Batch 228, Loss: 9.345409
Gradient norm: 151.60118103027344
Rank 2, Epoch 1, Batch 229, Loss: 8.987817
Gradient norm: 130.5006561279297
Rank 2, Epoch 1, Batch 230, Loss: 8.492387
Gradient norm: 100.2037582397461
Rank 2, Epoch 1, Batch 231, Loss: 9.950166
Gradient norm: 158.31124877929688
Rank 2, Epoch 1, Batch 232, Loss: 8.925497
Gradient norm: 149.80178833007812
Rank 2, Epoch 1, Batch 233, Loss: 8.858902
Gradient norm: 195.2779998779297
Rank 2, Epoch 1, Batch 234, Loss: 12.252148
Gradient norm: 113.21466827392578
Rank 2, Epoch 1, Batch 235, Loss: 14.992937
Gradient norm: 155.0465087890625
Rank 2, Epoch 1, Batch 236, Loss: 15.177953
Gradient norm: 121.43968963623047
Rank 2, Epoch 1, Batch 237, Loss: 9.029613
Gradient norm: 222.0647735595703
Rank 2, Epoch 1, Batch 238, Loss: 12.385081
Gradient norm: 139.1503448486328
Rank 2, Epoch 1, Batch 239, Loss: 11.533864
Gradient norm: 193.1023406982422
Rank 2, Epoch 1, Batch 240, Loss: 19.081768
Gradient norm: 185.3373565673828
Rank 2, Epoch 1, Batch 241, Loss: 22.155687
Gradient norm: 195.6822967529297
Rank 2, Epoch 1, Batch 242, Loss: 15.480680
Gradient norm: 210.12106323242188
Rank 2, Epoch 1, Batch 243, Loss: 17.429379
Gradient norm: 213.57046508789062
Rank 2, Epoch 1, Batch 244, Loss: 17.037437
Gradient norm: 197.20562744140625
Rank 2, Epoch 1, Batch 245, Loss: 19.235895
Gradient norm: 195.94509887695312
Rank 2, Epoch 1, Batch 246, Loss: 20.208931
Gradient norm: 208.57778930664062
Rank 2, Epoch 1, Batch 247, Loss: 19.874125
Gradient norm: 203.72509765625
Rank 2, Epoch 1, Batch 248, Loss: 23.609692
Gradient norm: 174.071044921875
Rank 2, Epoch 1, Batch 249, Loss: 20.861914
Gradient norm: 214.9095458984375
Rank 2, Epoch 1, Batch 250, Loss: 21.556349
Gradient norm: 173.2745819091797
Rank 2, Epoch 1, Batch 251, Loss: 21.002531
Gradient norm: 227.38917541503906
Rank 2, Epoch 1, Batch 252, Loss: 23.373384
Gradient norm: 188.13870239257812
Rank 2, Epoch 1, Batch 253, Loss: 14.444219
Gradient norm: 251.2870635986328
Rank 2, Epoch 1, Batch 254, Loss: 25.067118
Gradient norm: 169.3197784423828
Rank 2, Epoch 1, Batch 255, Loss: 17.765242
Gradient norm: 172.1207275390625
Rank 2, Epoch 1, Batch 256, Loss: 16.840586
Gradient norm: 228.77984619140625
Rank 2, Epoch 1, Batch 257, Loss: 22.754501
Gradient norm: 261.2685852050781
Rank 2, Epoch 1, Batch 258, Loss: 21.249350
Gradient norm: 149.78628540039062
Rank 2, Epoch 1, Batch 259, Loss: 24.188793
Gradient norm: 249.87168884277344
Rank 2, Epoch 1, Batch 260, Loss: 27.336617
Rank 2, Epoch 1, Val Loss: 7.8421, Val Acc: 0.1000, Time: 212.40s
Gradient norm: 208.65682983398438
Rank 0, Epoch 2, Batch 0, Loss: 25.600990
Gradient norm: 147.740966796875
Rank 0, Epoch 2, Batch 1, Loss: 23.830223
Gradient norm: 259.7872619628906
Rank 0, Epoch 2, Batch 2, Loss: 31.564795
Gradient norm: 240.12535095214844
Rank 0, Epoch 2, Batch 3, Loss: 37.883278
Gradient norm: 250.6136474609375
Rank 0, Epoch 2, Batch 4, Loss: 30.074821
Gradient norm: 251.0675811767578
Rank 0, Epoch 2, Batch 5, Loss: 24.586224
Gradient norm: 251.9285888671875
Rank 0, Epoch 2, Batch 6, Loss: 24.681662
Gradient norm: 267.6676025390625
Rank 0, Epoch 2, Batch 7, Loss: 26.349689
Gradient norm: 215.55335998535156
Rank 0, Epoch 2, Batch 8, Loss: 34.232941
Gradient norm: 178.536376953125
Rank 0, Epoch 2, Batch 9, Loss: 23.208868
Gradient norm: 231.54933166503906
Rank 0, Epoch 2, Batch 10, Loss: 22.802755
Gradient norm: 189.4681854248047
Rank 0, Epoch 2, Batch 11, Loss: 28.002609
Gradient norm: 227.2162322998047
Rank 0, Epoch 2, Batch 12, Loss: 23.748064
Gradient norm: 236.66371154785156
Rank 0, Epoch 2, Batch 13, Loss: 23.027435
Gradient norm: 265.6805114746094
Rank 0, Epoch 2, Batch 14, Loss: 22.173700
Gradient norm: 194.57577514648438
Rank 0, Epoch 2, Batch 15, Loss: 19.193796
Gradient norm: 233.669189453125
Rank 0, Epoch 2, Batch 16, Loss: 26.716269
Gradient norm: 248.41729736328125
Rank 0, Epoch 2, Batch 17, Loss: 26.367168
Gradient norm: 231.67172241210938
Rank 0, Epoch 2, Batch 18, Loss: 32.037075
Gradient norm: 286.6843566894531
Rank 0, Epoch 2, Batch 19, Loss: 41.023758
Gradient norm: 241.7064971923828
Rank 0, Epoch 2, Batch 20, Loss: 23.539009
Gradient norm: 184.22463989257812
Rank 0, Epoch 2, Batch 21, Loss: 25.543591
Gradient norm: 299.0050354003906
Rank 0, Epoch 2, Batch 22, Loss: 36.224915
Gradient norm: 301.1444396972656
Rank 0, Epoch 2, Batch 23, Loss: 42.462578
Gradient norm: 294.4884338378906
Rank 0, Epoch 2, Batch 24, Loss: 44.584381
Gradient norm: 284.69677734375
Rank 0, Epoch 2, Batch 25, Loss: 31.739613
Gradient norm: 301.4267883300781
Rank 0, Epoch 2, Batch 26, Loss: 30.595196
Gradient norm: 286.82781982421875
Rank 0, Epoch 2, Batch 27, Loss: 44.888165
Gradient norm: 276.8846435546875
Rank 0, Epoch 2, Batch 28, Loss: 37.229630
Gradient norm: 215.32041931152344
Rank 0, Epoch 2, Batch 29, Loss: 39.442959
Gradient norm: 275.98876953125
Rank 0, Epoch 2, Batch 30, Loss: 44.469894
Gradient norm: 293.1078186035156
Rank 0, Epoch 2, Batch 31, Loss: 42.498013
Gradient norm: 314.0950622558594
Rank 0, Epoch 2, Batch 32, Loss: 45.136551
Gradient norm: 250.8811798095703
Rank 0, Epoch 2, Batch 33, Loss: 32.698788
Gradient norm: 294.47418212890625
Rank 0, Epoch 2, Batch 34, Loss: 23.322788
Gradient norm: 317.86981201171875
Rank 0, Epoch 2, Batch 35, Loss: 33.180676
Gradient norm: 278.3588562011719
Rank 0, Epoch 2, Batch 36, Loss: 41.739899
Gradient norm: 324.4826354980469
Rank 0, Epoch 2, Batch 37, Loss: 53.453228
Gradient norm: 332.1779479980469
Rank 0, Epoch 2, Batch 38, Loss: 44.855690
Gradient norm: 328.7064514160156
Rank 0, Epoch 2, Batch 39, Loss: 47.980282
Gradient norm: 335.47491455078125
Rank 0, Epoch 2, Batch 40, Loss: 58.982563
Gradient norm: 171.91336059570312
Rank 0, Epoch 2, Batch 41, Loss: 41.878757
Gradient norm: 329.5386047363281
Rank 0, Epoch 2, Batch 42, Loss: 36.515076
Gradient norm: 311.5242614746094
Rank 0, Epoch 2, Batch 43, Loss: 46.818943
Gradient norm: 318.9305114746094
Rank 0, Epoch 2, Batch 44, Loss: 54.467205
Gradient norm: 355.2916564941406
Rank 0, Epoch 2, Batch 45, Loss: 65.174187
Gradient norm: 326.0798645019531
Rank 0, Epoch 2, Batch 46, Loss: 53.602013
Gradient norm: 350.483642578125
Rank 0, Epoch 2, Batch 47, Loss: 59.365265
Gradient norm: 353.2138977050781
Rank 0, Epoch 2, Batch 48, Loss: 39.313404
Gradient norm: 335.28094482421875
Rank 0, Epoch 2, Batch 49, Loss: 48.650890
Gradient norm: 358.83526611328125
Rank 0, Epoch 2, Batch 50, Loss: 76.413971
Gradient norm: 304.4237365722656
Rank 0, Epoch 2, Batch 51, Loss: 54.054962
Gradient norm: 299.5175476074219
Rank 0, Epoch 2, Batch 52, Loss: 54.917114
Gradient norm: 363.3053894042969
Rank 0, Epoch 2, Batch 53, Loss: 69.521271
Gradient norm: 330.9164733886719
Rank 0, Epoch 2, Batch 54, Loss: 56.550392
Gradient norm: 308.6933898925781
Rank 0, Epoch 2, Batch 55, Loss: 32.412281
Gradient norm: 358.1397705078125
Rank 0, Epoch 2, Batch 56, Loss: 33.676353
Gradient norm: 354.9397277832031
Rank 0, Epoch 2, Batch 57, Loss: 51.482376
Gradient norm: 338.78717041015625
Rank 0, Epoch 2, Batch 58, Loss: 58.803997
Gradient norm: 344.3288269042969
Rank 0, Epoch 2, Batch 59, Loss: 73.557144
Gradient norm: 297.0224609375
Rank 0, Epoch 2, Batch 60, Loss: 69.259781
Gradient norm: 343.1894836425781
Rank 0, Epoch 2, Batch 61, Loss: 52.020046
Gradient norm: 382.6531066894531
Rank 0, Epoch 2, Batch 62, Loss: 52.526867
Gradient norm: 396.13641357421875
Rank 0, Epoch 2, Batch 63, Loss: 56.846210
Gradient norm: 311.7681884765625
Rank 0, Epoch 2, Batch 64, Loss: 51.633812
Gradient norm: 368.1044006347656
Rank 0, Epoch 2, Batch 65, Loss: 74.067444
Gradient norm: 396.6600646972656
Rank 0, Epoch 2, Batch 66, Loss: 75.437866
Gradient norm: 389.29437255859375
Rank 0, Epoch 2, Batch 67, Loss: 80.390015
Gradient norm: 391.2613830566406
Rank 0, Epoch 2, Batch 68, Loss: 62.162518
Gradient norm: 278.6119689941406
Rank 0, Epoch 2, Batch 69, Loss: 58.264492
Gradient norm: 391.2171325683594
Rank 0, Epoch 2, Batch 70, Loss: 48.963284
Gradient norm: 334.2982482910156
Rank 0, Epoch 2, Batch 71, Loss: 59.719944
Gradient norm: 345.2520446777344
Rank 0, Epoch 2, Batch 72, Loss: 62.714996
Gradient norm: 359.6562194824219
Rank 0, Epoch 2, Batch 73, Loss: 73.423843
Gradient norm: 396.3236389160156
Rank 0, Epoch 2, Batch 74, Loss: 83.073906
Gradient norm: 409.8815002441406
Rank 0, Epoch 2, Batch 75, Loss: 71.355843
Gradient norm: 417.0909729003906
Rank 0, Epoch 2, Batch 76, Loss: 65.362312
Gradient norm: 397.1960144042969
Rank 0, Epoch 2, Batch 77, Loss: 75.237312
Gradient norm: 371.9200744628906
Rank 0, Epoch 2, Batch 78, Loss: 60.446827
Gradient norm: 375.1036071777344
Rank 0, Epoch 2, Batch 79, Loss: 61.779697
Gradient norm: 413.344482421875
Rank 0, Epoch 2, Batch 80, Loss: 71.100861
Gradient norm: 399.62274169921875
Rank 0, Epoch 2, Batch 81, Loss: 89.082741
Gradient norm: 345.6826171875
Rank 0, Epoch 2, Batch 82, Loss: 95.419281
Gradient norm: 419.2848205566406
Rank 0, Epoch 2, Batch 83, Loss: 120.298134
Gradient norm: 449.69122314453125
Rank 0, Epoch 2, Batch 84, Loss: 55.610973
Gradient norm: 384.5172119140625
Rank 0, Epoch 2, Batch 85, Loss: 51.153988
Gradient norm: 397.5979919433594
Rank 0, Epoch 2, Batch 86, Loss: 69.675766
Gradient norm: 400.759033203125
Rank 0, Epoch 2, Batch 87, Loss: 85.580254
Gradient norm: 430.2462463378906
Rank 0, Epoch 2, Batch 88, Loss: 88.005150
Gradient norm: 429.634033203125
Rank 0, Epoch 2, Batch 89, Loss: 117.387848
Gradient norm: 443.6693420410156
Rank 0, Epoch 2, Batch 90, Loss: 70.790756
Gradient norm: 432.2049865722656
Rank 0, Epoch 2, Batch 91, Loss: 88.037476
Gradient norm: 450.7593688964844
Rank 0, Epoch 2, Batch 92, Loss: 82.871216
Gradient norm: 465.2426452636719
Rank 0, Epoch 2, Batch 93, Loss: 101.288040
Gradient norm: 445.96722412109375
Rank 0, Epoch 2, Batch 94, Loss: 90.722382
Gradient norm: 391.3941345214844
Rank 0, Epoch 2, Batch 95, Loss: 90.506516
Gradient norm: 425.7786865234375
Rank 0, Epoch 2, Batch 96, Loss: 119.866455
Gradient norm: 414.7414245605469
Rank 0, Epoch 2, Batch 97, Loss: 105.657333
Gradient norm: 459.8976135253906
Rank 0, Epoch 2, Batch 98, Loss: 74.468521
Gradient norm: 401.49493408203125
Rank 0, Epoch 2, Batch 99, Loss: 59.252342
Gradient norm: 444.95458984375
Rank 0, Epoch 2, Batch 100, Loss: 88.456940
Gradient norm: 408.4156799316406
Rank 0, Epoch 2, Batch 101, Loss: 76.911499
Gradient norm: 452.18804931640625
Rank 0, Epoch 2, Batch 102, Loss: 85.250641
Gradient norm: 446.73577880859375
Rank 0, Epoch 2, Batch 103, Loss: 102.356224
Gradient norm: 488.1429138183594
Rank 0, Epoch 2, Batch 104, Loss: 120.411865
Gradient norm: 487.30706787109375
Rank 0, Epoch 2, Batch 105, Loss: 100.435883
Gradient norm: 474.0209045410156
Rank 0, Epoch 2, Batch 106, Loss: 105.787880
Gradient norm: 493.63409423828125
Gradient norm: 253.43264770507812
Rank 2, Epoch 2, Batch 0, Loss: 28.331085
Gradient norm: 221.67323303222656
Rank 2, Epoch 2, Batch 1, Loss: 16.176407
Gradient norm: 206.8563232421875
Rank 2, Epoch 2, Batch 2, Loss: 19.911371
Gradient norm: 184.00975036621094
Rank 2, Epoch 2, Batch 3, Loss: 20.693415
Gradient norm: 204.9395294189453
Rank 2, Epoch 2, Batch 4, Loss: 29.009762
Gradient norm: 224.91326904296875
Rank 2, Epoch 2, Batch 5, Loss: 25.960300
Gradient norm: 254.49713134765625
Rank 2, Epoch 2, Batch 6, Loss: 28.428928
Gradient norm: 261.8759765625
Rank 2, Epoch 2, Batch 7, Loss: 29.195667
Gradient norm: 259.53387451171875
Rank 2, Epoch 2, Batch 8, Loss: 36.349289
Gradient norm: 255.0809783935547
Rank 2, Epoch 2, Batch 9, Loss: 42.009850
Gradient norm: 277.53924560546875
Rank 2, Epoch 2, Batch 10, Loss: 29.970623
Gradient norm: 259.66680908203125
Rank 2, Epoch 2, Batch 11, Loss: 28.419781
Gradient norm: 248.430419921875
Rank 2, Epoch 2, Batch 12, Loss: 27.819118
Gradient norm: 246.3452606201172
Rank 2, Epoch 2, Batch 13, Loss: 19.079453
Gradient norm: 243.22390747070312
Rank 2, Epoch 2, Batch 14, Loss: 28.467344
Gradient norm: 286.8936462402344
Rank 2, Epoch 2, Batch 15, Loss: 41.433460
Gradient norm: 249.6995086669922
Rank 2, Epoch 2, Batch 16, Loss: 37.746941
Gradient norm: 300.5960693359375
Rank 2, Epoch 2, Batch 17, Loss: 45.108246
Gradient norm: 248.69729614257812
Rank 2, Epoch 2, Batch 18, Loss: 35.819683
Gradient norm: 256.0848083496094
Rank 2, Epoch 2, Batch 19, Loss: 31.874029
Gradient norm: 291.7227783203125
Rank 2, Epoch 2, Batch 20, Loss: 28.921833
Gradient norm: 288.0672607421875
Rank 2, Epoch 2, Batch 21, Loss: 44.812954
Gradient norm: 228.51876831054688
Rank 2, Epoch 2, Batch 22, Loss: 25.240656
Gradient norm: 293.6277770996094
Rank 2, Epoch 2, Batch 23, Loss: 46.709644
Gradient norm: 312.131591796875
Rank 2, Epoch 2, Batch 24, Loss: 51.538467
Gradient norm: 314.9371337890625
Rank 2, Epoch 2, Batch 25, Loss: 45.626408
Gradient norm: 310.1067199707031
Rank 2, Epoch 2, Batch 26, Loss: 42.414864
Gradient norm: 313.2261047363281
Rank 2, Epoch 2, Batch 27, Loss: 32.746395
Gradient norm: 212.25860595703125
Rank 2, Epoch 2, Batch 28, Loss: 22.484549
Gradient norm: 298.54034423828125
Rank 2, Epoch 2, Batch 29, Loss: 37.588638
Gradient norm: 324.6968994140625
Rank 2, Epoch 2, Batch 30, Loss: 34.089561
Gradient norm: 295.4054870605469
Rank 2, Epoch 2, Batch 31, Loss: 43.471344
Gradient norm: 315.6409912109375
Rank 2, Epoch 2, Batch 32, Loss: 59.140041
Gradient norm: 318.45367431640625
Rank 2, Epoch 2, Batch 33, Loss: 63.330475
Gradient norm: 309.46875
Rank 2, Epoch 2, Batch 34, Loss: 56.577877
Gradient norm: 318.1726379394531
Rank 2, Epoch 2, Batch 35, Loss: 59.078922
Gradient norm: 306.5218200683594
Rank 2, Epoch 2, Batch 36, Loss: 56.138515
Gradient norm: 300.82366943359375
Rank 2, Epoch 2, Batch 37, Loss: 36.651962
Gradient norm: 290.0767822265625
Rank 2, Epoch 2, Batch 38, Loss: 29.067535
Gradient norm: 319.68341064453125
Rank 2, Epoch 2, Batch 39, Loss: 35.418499
Gradient norm: 225.8954620361328
Rank 2, Epoch 2, Batch 40, Loss: 32.296310
Gradient norm: 270.65838623046875
Rank 2, Epoch 2, Batch 41, Loss: 37.155296
Gradient norm: 249.4487762451172
Rank 2, Epoch 2, Batch 42, Loss: 36.486168
Gradient norm: 285.9396057128906
Rank 2, Epoch 2, Batch 43, Loss: 42.847466
Gradient norm: 339.40478515625
Rank 2, Epoch 2, Batch 44, Loss: 46.200600
Gradient norm: 350.3654479980469
Rank 2, Epoch 2, Batch 45, Loss: 54.245678
Gradient norm: 280.1880187988281
Rank 2, Epoch 2, Batch 46, Loss: 41.276119
Gradient norm: 344.9704895019531
Rank 2, Epoch 2, Batch 47, Loss: 63.338482
Gradient norm: 325.2063293457031
Rank 2, Epoch 2, Batch 48, Loss: 54.182590
Gradient norm: 311.1798400878906
Rank 2, Epoch 2, Batch 49, Loss: 54.005272
Gradient norm: 290.8100891113281
Rank 2, Epoch 2, Batch 50, Loss: 32.770756
Gradient norm: 314.5510559082031
Rank 2, Epoch 2, Batch 51, Loss: 37.869419
Gradient norm: 332.82672119140625
Rank 2, Epoch 2, Batch 52, Loss: 59.032063
Gradient norm: 350.436767578125
Rank 2, Epoch 2, Batch 53, Loss: 71.799179
Gradient norm: 347.0712585449219
Rank 2, Epoch 2, Batch 54, Loss: 52.956539
Gradient norm: 358.6959228515625
Rank 2, Epoch 2, Batch 55, Loss: 52.344337
Gradient norm: 281.7674560546875
Rank 2, Epoch 2, Batch 56, Loss: 38.154778
Gradient norm: 310.3247985839844
Rank 2, Epoch 2, Batch 57, Loss: 49.482574
Gradient norm: 344.4288635253906
Rank 2, Epoch 2, Batch 58, Loss: 59.769558
Gradient norm: 234.60557556152344
Rank 2, Epoch 2, Batch 59, Loss: 51.618885
Gradient norm: 369.81365966796875
Rank 2, Epoch 2, Batch 60, Loss: 54.459263
Gradient norm: 365.550048828125
Rank 2, Epoch 2, Batch 61, Loss: 51.916676
Gradient norm: 403.57000732421875
Rank 2, Epoch 2, Batch 62, Loss: 52.004784
Gradient norm: 238.29202270507812
Rank 2, Epoch 2, Batch 63, Loss: 30.024870
Gradient norm: 319.1307067871094
Rank 2, Epoch 2, Batch 64, Loss: 69.290596
Gradient norm: 345.10687255859375
Rank 2, Epoch 2, Batch 65, Loss: 49.352367
Gradient norm: 293.0819091796875
Rank 2, Epoch 2, Batch 66, Loss: 54.255726
Gradient norm: 383.2877197265625
Rank 2, Epoch 2, Batch 67, Loss: 59.986015
Gradient norm: 367.5582275390625
Rank 2, Epoch 2, Batch 68, Loss: 64.013817
Gradient norm: 386.35418701171875
Rank 2, Epoch 2, Batch 69, Loss: 77.678848
Gradient norm: 376.2884216308594
Rank 2, Epoch 2, Batch 70, Loss: 81.842667
Gradient norm: 382.8672180175781
Rank 2, Epoch 2, Batch 71, Loss: 83.134140
Gradient norm: 294.4437561035156
Rank 2, Epoch 2, Batch 72, Loss: 32.627834
Gradient norm: 377.8481140136719
Rank 2, Epoch 2, Batch 73, Loss: 50.651962
Gradient norm: 369.91192626953125
Rank 2, Epoch 2, Batch 74, Loss: 66.884682
Gradient norm: 411.8814697265625
Rank 2, Epoch 2, Batch 75, Loss: 65.194176
Gradient norm: 404.39251708984375
Rank 2, Epoch 2, Batch 76, Loss: 87.866127
Gradient norm: 400.4847106933594
Rank 2, Epoch 2, Batch 77, Loss: 79.305984
Gradient norm: 413.040283203125
Rank 2, Epoch 2, Batch 78, Loss: 87.020279
Gradient norm: 406.7673645019531
Rank 2, Epoch 2, Batch 79, Loss: 73.818703
Gradient norm: 425.4798889160156
Rank 2, Epoch 2, Batch 80, Loss: 75.695869
Gradient norm: 360.22003173828125
Rank 2, Epoch 2, Batch 81, Loss: 62.766579
Gradient norm: 433.2477111816406
Rank 2, Epoch 2, Batch 82, Loss: 92.154358
Gradient norm: 386.8823547363281
Rank 2, Epoch 2, Batch 83, Loss: 92.787109
Gradient norm: 413.5978088378906
Rank 2, Epoch 2, Batch 84, Loss: 90.442345
Gradient norm: 417.33367919921875
Rank 2, Epoch 2, Batch 85, Loss: 64.651100
Gradient norm: 289.610595703125
Rank 2, Epoch 2, Batch 86, Loss: 58.782562
Gradient norm: 430.0290832519531
Rank 2, Epoch 2, Batch 87, Loss: 73.565086
Gradient norm: 432.6186218261719
Rank 2, Epoch 2, Batch 88, Loss: 93.332115
Gradient norm: 402.4937744140625
Rank 2, Epoch 2, Batch 89, Loss: 93.149277
Gradient norm: 440.5019226074219
Rank 2, Epoch 2, Batch 90, Loss: 70.337448
Gradient norm: 470.2424621582031
Rank 2, Epoch 2, Batch 91, Loss: 71.414986
Gradient norm: 395.6182861328125
Rank 2, Epoch 2, Batch 92, Loss: 69.484146
Gradient norm: 446.88330078125
Rank 2, Epoch 2, Batch 93, Loss: 88.843796
Gradient norm: 448.8481750488281
Rank 2, Epoch 2, Batch 94, Loss: 149.986267
Gradient norm: 444.3369140625
Rank 2, Epoch 2, Batch 95, Loss: 110.776207
Gradient norm: 442.61029052734375
Rank 2, Epoch 2, Batch 96, Loss: 130.572723
Gradient norm: 435.58709716796875
Rank 2, Epoch 2, Batch 97, Loss: 54.796410
Gradient norm: 460.86627197265625
Rank 2, Epoch 2, Batch 98, Loss: 80.046043
Gradient norm: 418.74560546875
Rank 2, Epoch 2, Batch 99, Loss: 72.037788
Gradient norm: 448.92022705078125
Rank 2, Epoch 2, Batch 100, Loss: 106.436462
Gradient norm: 416.73797607421875
Rank 2, Epoch 2, Batch 101, Loss: 87.309456
Gradient norm: 488.9991149902344
Rank 2, Epoch 2, Batch 102, Loss: 102.531212
Gradient norm: 301.0103759765625
Rank 2, Epoch 2, Batch 103, Loss: 98.692368
Gradient norm: 449.8891906738281
Rank 2, Epoch 2, Batch 104, Loss: 86.149254
Gradient norm: 463.7293395996094
Rank 2, Epoch 2, Batch 105, Loss: 72.657898
Gradient norm: 364.33477783203125
Rank 2, Epoch 2, Batch 106, Loss: 63.902676
Gradient norm: 411.62384033203125
Gradient norm: 495.5254211425781
Rank 1, Epoch 2, Batch 0, Loss: 117.107452
Gradient norm: 387.4779357910156
Rank 1, Epoch 2, Batch 1, Loss: 82.343758
Gradient norm: 398.85064697265625
Rank 1, Epoch 2, Batch 2, Loss: 99.425774
Gradient norm: 476.8574523925781
Rank 1, Epoch 2, Batch 3, Loss: 101.118828
Gradient norm: 396.1147155761719
Rank 1, Epoch 2, Batch 4, Loss: 86.345604
Gradient norm: 484.9004821777344
Rank 1, Epoch 2, Batch 5, Loss: 104.035522
Gradient norm: 405.0086975097656
Rank 1, Epoch 2, Batch 6, Loss: 83.747169
Gradient norm: 425.4539794921875
Rank 1, Epoch 2, Batch 7, Loss: 101.355347
Gradient norm: 511.6126708984375
Rank 1, Epoch 2, Batch 8, Loss: 92.067657
Gradient norm: 520.2108154296875
Rank 1, Epoch 2, Batch 9, Loss: 105.912575
Gradient norm: 497.5570068359375
Rank 1, Epoch 2, Batch 10, Loss: 94.719345
Gradient norm: 539.3575439453125
Rank 1, Epoch 2, Batch 11, Loss: 109.311691
Gradient norm: 498.1690979003906
Rank 1, Epoch 2, Batch 12, Loss: 131.021729
Gradient norm: 523.5606079101562
Rank 1, Epoch 2, Batch 13, Loss: 141.893845
Gradient norm: 549.88330078125
Rank 1, Epoch 2, Batch 14, Loss: 197.546600
Gradient norm: 532.2146606445312
Rank 1, Epoch 2, Batch 15, Loss: 158.956314
Gradient norm: 503.61505126953125
Rank 1, Epoch 2, Batch 16, Loss: 157.239563
Gradient norm: 487.81011962890625
Rank 1, Epoch 2, Batch 17, Loss: 75.104057
Gradient norm: 525.4700317382812
Rank 1, Epoch 2, Batch 18, Loss: 111.791199
Gradient norm: 568.2627563476562
Rank 1, Epoch 2, Batch 19, Loss: 140.778809
Gradient norm: 510.3816223144531
Rank 1, Epoch 2, Batch 20, Loss: 105.754005
Gradient norm: 556.1041259765625
Rank 1, Epoch 2, Batch 21, Loss: 146.465637
Gradient norm: 509.3937072753906
Rank 1, Epoch 2, Batch 22, Loss: 120.292282
Gradient norm: 536.9153442382812
Rank 1, Epoch 2, Batch 23, Loss: 162.349915
Gradient norm: 497.693603515625
Rank 1, Epoch 2, Batch 24, Loss: 110.605522
Gradient norm: 552.2996826171875
Rank 1, Epoch 2, Batch 25, Loss: 181.677338
Gradient norm: 577.226806640625
Rank 1, Epoch 2, Batch 26, Loss: 181.947937
Gradient norm: 574.4063110351562
Rank 1, Epoch 2, Batch 27, Loss: 189.950882
Gradient norm: 575.7388305664062
Rank 1, Epoch 2, Batch 28, Loss: 101.206421
Gradient norm: 574.1190795898438
Rank 1, Epoch 2, Batch 29, Loss: 98.594559
Gradient norm: 629.065185546875
Rank 1, Epoch 2, Batch 30, Loss: 137.042999
Gradient norm: 595.3658447265625
Rank 1, Epoch 2, Batch 31, Loss: 143.852402
Gradient norm: 572.40185546875
Rank 1, Epoch 2, Batch 32, Loss: 241.871048
Gradient norm: 555.1132202148438
Rank 1, Epoch 2, Batch 33, Loss: 187.057617
Gradient norm: 565.3253784179688
Rank 1, Epoch 2, Batch 34, Loss: 192.721664
Gradient norm: 623.421875
Rank 1, Epoch 2, Batch 35, Loss: 170.245728
Gradient norm: 574.736328125
Rank 1, Epoch 2, Batch 36, Loss: 191.652847
Gradient norm: 624.3486938476562
Rank 1, Epoch 2, Batch 37, Loss: 125.575218
Gradient norm: 494.67352294921875
Rank 1, Epoch 2, Batch 38, Loss: 94.135880
Gradient norm: 547.9722900390625
Rank 1, Epoch 2, Batch 39, Loss: 118.083511
Gradient norm: 572.2254028320312
Rank 1, Epoch 2, Batch 40, Loss: 115.556038
Gradient norm: 603.2716064453125
Rank 1, Epoch 2, Batch 41, Loss: 177.946518
Gradient norm: 625.1975708007812
Rank 1, Epoch 2, Batch 42, Loss: 236.155319
Gradient norm: 681.7742309570312
Rank 1, Epoch 2, Batch 43, Loss: 296.168640
Gradient norm: 667.3829345703125
Rank 1, Epoch 2, Batch 44, Loss: 201.526031
Gradient norm: 630.1976318359375
Rank 1, Epoch 2, Batch 45, Loss: 206.726196
Gradient norm: 662.8584594726562
Rank 1, Epoch 2, Batch 46, Loss: 175.080338
Gradient norm: 625.90869140625
Rank 1, Epoch 2, Batch 47, Loss: 186.758636
Gradient norm: 665.1405029296875
Rank 1, Epoch 2, Batch 48, Loss: 177.058746
Gradient norm: 618.2295532226562
Rank 1, Epoch 2, Batch 49, Loss: 179.143097
Gradient norm: 682.3232421875
Rank 1, Epoch 2, Batch 50, Loss: 157.316666
Gradient norm: 685.8452758789062
Rank 1, Epoch 2, Batch 51, Loss: 257.540070
Gradient norm: 660.4955444335938
Rank 1, Epoch 2, Batch 52, Loss: 220.652893
Gradient norm: 659.7913208007812
Rank 1, Epoch 2, Batch 53, Loss: 194.046051
Gradient norm: 660.949951171875
Rank 1, Epoch 2, Batch 54, Loss: 262.842834
Gradient norm: 695.2944946289062
Rank 1, Epoch 2, Batch 55, Loss: 302.509460
Gradient norm: 693.9456176757812
Rank 1, Epoch 2, Batch 56, Loss: 226.166733
Gradient norm: 670.1376953125
Rank 1, Epoch 2, Batch 57, Loss: 235.102020
Gradient norm: 653.3434448242188
Rank 1, Epoch 2, Batch 58, Loss: 121.677879
Gradient norm: 717.4851684570312
Rank 1, Epoch 2, Batch 59, Loss: 190.299927
Gradient norm: 651.2662963867188
Rank 1, Epoch 2, Batch 60, Loss: 218.935364
Gradient norm: 676.615966796875
Rank 1, Epoch 2, Batch 61, Loss: 205.488373
Gradient norm: 708.3065795898438
Rank 1, Epoch 2, Batch 62, Loss: 243.439941
Gradient norm: 710.9711303710938
Rank 1, Epoch 2, Batch 63, Loss: 229.534546
Gradient norm: 713.5909423828125
Rank 1, Epoch 2, Batch 64, Loss: 273.281982
Gradient norm: 764.7418823242188
Rank 1, Epoch 2, Batch 65, Loss: 287.228027
Gradient norm: 669.7560424804688
Rank 1, Epoch 2, Batch 66, Loss: 223.702682
Gradient norm: 744.8766479492188
Rank 1, Epoch 2, Batch 67, Loss: 340.195038
Gradient norm: 706.03076171875
Rank 1, Epoch 2, Batch 68, Loss: 241.999542
Gradient norm: 718.5298461914062
Rank 1, Epoch 2, Batch 69, Loss: 192.755402
Gradient norm: 754.1654052734375
Rank 1, Epoch 2, Batch 70, Loss: 333.015503
Gradient norm: 684.076416015625
Rank 1, Epoch 2, Batch 71, Loss: 228.745972
Gradient norm: 693.1766967773438
Rank 1, Epoch 2, Batch 72, Loss: 210.177246
Gradient norm: 687.9525146484375
Rank 1, Epoch 2, Batch 73, Loss: 209.862823
Gradient norm: 771.1928100585938
Rank 1, Epoch 2, Batch 74, Loss: 247.225647
Gradient norm: 735.9122924804688
Rank 1, Epoch 2, Batch 75, Loss: 175.252472
Gradient norm: 792.84521484375
Rank 1, Epoch 2, Batch 76, Loss: 318.678406
Gradient norm: 744.5947875976562
Rank 1, Epoch 2, Batch 77, Loss: 270.721924
Gradient norm: 668.86865234375
Rank 1, Epoch 2, Batch 78, Loss: 265.767181
Gradient norm: 789.2716674804688
Rank 1, Epoch 2, Batch 79, Loss: 312.234497
Gradient norm: 727.800048828125
Rank 1, Epoch 2, Batch 80, Loss: 212.675674
Gradient norm: 804.8353881835938
Rank 1, Epoch 2, Batch 81, Loss: 370.638184
Gradient norm: 757.247314453125
Rank 1, Epoch 2, Batch 82, Loss: 242.593231
Gradient norm: 807.6925048828125
Rank 1, Epoch 2, Batch 83, Loss: 360.000031
Gradient norm: 683.5598754882812
Rank 1, Epoch 2, Batch 84, Loss: 279.398682
Gradient norm: 774.151123046875
Rank 1, Epoch 2, Batch 85, Loss: 283.762848
Gradient norm: 872.4498901367188
Rank 1, Epoch 2, Batch 86, Loss: 253.779068
Gradient norm: 862.9215087890625
Rank 1, Epoch 2, Batch 87, Loss: 320.832275
Gradient norm: 827.3468627929688
Rank 1, Epoch 2, Batch 88, Loss: 349.857513
Gradient norm: 815.8539428710938
Rank 1, Epoch 2, Batch 89, Loss: 372.712433
Gradient norm: 873.2691650390625
Rank 1, Epoch 2, Batch 90, Loss: 334.309021
Gradient norm: 796.8236694335938
Rank 1, Epoch 2, Batch 91, Loss: 296.811707
Gradient norm: 880.95068359375
Rank 1, Epoch 2, Batch 92, Loss: 404.680908
Gradient norm: 811.9956665039062
Rank 1, Epoch 2, Batch 93, Loss: 324.116455
Gradient norm: 810.5572509765625
Rank 1, Epoch 2, Batch 94, Loss: 352.849548
Gradient norm: 780.3021240234375
Rank 1, Epoch 2, Batch 95, Loss: 316.524475
Gradient norm: 870.3777465820312
Rank 1, Epoch 2, Batch 96, Loss: 295.777405
Gradient norm: 827.4353637695312
Rank 1, Epoch 2, Batch 97, Loss: 268.958740
Gradient norm: 828.1093139648438
Rank 1, Epoch 2, Batch 98, Loss: 350.996033
Gradient norm: 918.728759765625
Rank 1, Epoch 2, Batch 99, Loss: 395.142639
Gradient norm: 820.2091064453125
Rank 1, Epoch 2, Batch 100, Loss: 371.081085
Gradient norm: 856.3574829101562
Rank 1, Epoch 2, Batch 101, Loss: 461.325409
Gradient norm: 801.3665161132812
Rank 1, Epoch 2, Batch 102, Loss: 278.828003
Gradient norm: 866.3495483398438
Rank 1, Epoch 2, Batch 103, Loss: 318.021912
Gradient norm: 865.9515380859375
Rank 1, Epoch 2, Batch 104, Loss: 338.047302
Gradient norm: 930.7343139648438
Rank 1, Epoch 2, Batch 105, Loss: 248.065536
Gradient norm: 756.0126342773438
Rank 1, Epoch 2, Batch 106, Loss: 269.494263
Gradient norm: 611.8396606445312
Rank 1, Epoch 2, Batch 107, Loss: 428.489655
Gradient norm: 951.5451049804688
Rank 1, Epoch 2, Batch 108, Loss: 312.387238
Gradient norm: 793.0828857421875
Rank 1, Epoch 2, Batch 109, Loss: 374.582489
Gradient norm: 948.8750610351562
Rank 1, Epoch 2, Batch 110, Loss: 403.916260
Gradient norm: 920.5511474609375
Rank 1, Epoch 2, Batch 111, Loss: 314.746094
Gradient norm: 870.1239624023438
Rank 1, Epoch 2, Batch 112, Loss: 285.279022
Gradient norm: 898.0151977539062
Rank 1, Epoch 2, Batch 113, Loss: 486.623352
Gradient norm: 961.1536865234375
Rank 1, Epoch 2, Batch 114, Loss: 375.211884
Gradient norm: 870.922607421875
Rank 1, Epoch 2, Batch 115, Loss: 406.018555
Gradient norm: 896.6832885742188
Rank 1, Epoch 2, Batch 116, Loss: 613.242004
Gradient norm: 860.5259399414062
Rank 1, Epoch 2, Batch 117, Loss: 407.115356
Gradient norm: 1009.5059814453125
Rank 1, Epoch 2, Batch 118, Loss: 310.495728
Gradient norm: 887.9444580078125
Rank 1, Epoch 2, Batch 119, Loss: 409.351379
Gradient norm: 878.914306640625
Rank 1, Epoch 2, Batch 120, Loss: 276.496368
Gradient norm: 956.9161987304688
Rank 1, Epoch 2, Batch 121, Loss: 374.792175
Gradient norm: 922.6024780273438
Rank 1, Epoch 2, Batch 122, Loss: 451.632629
Gradient norm: 1010.0244750976562
Rank 1, Epoch 2, Batch 123, Loss: 394.923187
Gradient norm: 936.8471069335938
Rank 1, Epoch 2, Batch 124, Loss: 432.529510
Gradient norm: 981.562255859375
Rank 1, Epoch 2, Batch 125, Loss: 579.764587
Gradient norm: 999.3681030273438
Rank 1, Epoch 2, Batch 126, Loss: 679.336121
Gradient norm: 1019.614990234375
Rank 1, Epoch 2, Batch 127, Loss: 417.938446
Gradient norm: 981.206298828125
Rank 1, Epoch 2, Batch 128, Loss: 361.079193
Gradient norm: 996.075927734375
Rank 1, Epoch 2, Batch 129, Loss: 491.605377
Gradient norm: 1052.5079345703125
Rank 1, Epoch 2, Batch 130, Loss: 434.015076
Gradient norm: 1035.8125
Rank 1, Epoch 2, Batch 131, Loss: 433.175659
Gradient norm: 1028.4979248046875
Rank 1, Epoch 2, Batch 132, Loss: 520.646545
Gradient norm: 994.0128173828125
Rank 1, Epoch 2, Batch 133, Loss: 483.401978
Gradient norm: 933.4201049804688
Rank 1, Epoch 2, Batch 134, Loss: 498.342804
Gradient norm: 1058.526123046875
Rank 1, Epoch 2, Batch 135, Loss: 651.951599
Gradient norm: 1034.189697265625
Rank 1, Epoch 2, Batch 136, Loss: 569.607483
Gradient norm: 1092.556884765625
Rank 1, Epoch 2, Batch 137, Loss: 518.412720
Gradient norm: 981.4713745117188
Rank 1, Epoch 2, Batch 138, Loss: 487.573273
Gradient norm: 1029.9691162109375
Rank 1, Epoch 2, Batch 139, Loss: 396.944336
Gradient norm: 978.28515625
Rank 1, Epoch 2, Batch 140, Loss: 439.702423
Gradient norm: 1077.766845703125
Rank 1, Epoch 2, Batch 141, Loss: 452.060760
Gradient norm: 1061.3662109375
Rank 1, Epoch 2, Batch 142, Loss: 610.935486
Gradient norm: 1081.429931640625
Rank 1, Epoch 2, Batch 143, Loss: 685.510620
Gradient norm: 959.5599975585938
Rank 1, Epoch 2, Batch 144, Loss: 595.468018
Gradient norm: 947.6141967773438
Rank 1, Epoch 2, Batch 145, Loss: 546.263123
Gradient norm: 1066.7880859375
Rank 1, Epoch 2, Batch 146, Loss: 404.674988
Gradient norm: 1084.045654296875
Rank 1, Epoch 2, Batch 147, Loss: 523.090759
Gradient norm: 1128.8385009765625
Rank 1, Epoch 2, Batch 148, Loss: 673.495483
Gradient norm: 1110.0526123046875
Rank 1, Epoch 2, Batch 149, Loss: 430.850342
Gradient norm: 1042.930419921875
Rank 1, Epoch 2, Batch 150, Loss: 317.394562
Gradient norm: 1025.0255126953125
Rank 1, Epoch 2, Batch 151, Loss: 437.405975
Gradient norm: 1029.582763671875
Rank 1, Epoch 2, Batch 152, Loss: 413.353516
Gradient norm: 957.6229248046875
Rank 1, Epoch 2, Batch 153, Loss: 562.767212
Gradient norm: 1053.6510009765625
Rank 1, Epoch 2, Batch 154, Loss: 651.535034
Gradient norm: 1194.4189453125
Rank 1, Epoch 2, Batch 155, Loss: 951.442505
Gradient norm: 1051.1943359375
Rank 1, Epoch 2, Batch 156, Loss: 737.901855
Gradient norm: 1045.1202392578125
Rank 1, Epoch 2, Batch 157, Loss: 353.090454
Gradient norm: 1098.168212890625
Rank 1, Epoch 2, Batch 158, Loss: 406.811554
Gradient norm: 940.2403564453125
Rank 1, Epoch 2, Batch 159, Loss: 405.740417
Gradient norm: 1156.4173583984375
Rank 1, Epoch 2, Batch 160, Loss: 571.819519
Gradient norm: 1017.8464965820312
Rank 1, Epoch 2, Batch 161, Loss: 491.555847
Gradient norm: 1134.6326904296875
Rank 1, Epoch 2, Batch 162, Loss: 655.531677
Gradient norm: 1143.141357421875
Rank 1, Epoch 2, Batch 163, Loss: 649.336548
Gradient norm: 1211.299072265625
Rank 1, Epoch 2, Batch 164, Loss: 628.769470
Gradient norm: 1110.1612548828125
Rank 1, Epoch 2, Batch 165, Loss: 758.246948
Gradient norm: 959.7935180664062
Rank 1, Epoch 2, Batch 166, Loss: 430.808197
Gradient norm: 1180.5859375
Rank 1, Epoch 2, Batch 167, Loss: 677.440674
Gradient norm: 1093.1973876953125
Rank 1, Epoch 2, Batch 168, Loss: 652.170776
Gradient norm: 1205.8741455078125
Rank 1, Epoch 2, Batch 169, Loss: 355.844116
Gradient norm: 1222.328857421875
Rank 1, Epoch 2, Batch 170, Loss: 645.837646
Gradient norm: 1239.1331787109375
Rank 1, Epoch 2, Batch 171, Loss: 972.084473
Gradient norm: 1154.79052734375
Rank 1, Epoch 2, Batch 172, Loss: 580.651367
Gradient norm: 1064.095703125
Rank 1, Epoch 2, Batch 173, Loss: 534.133972
Gradient norm: 1131.468017578125
Rank 1, Epoch 2, Batch 174, Loss: 753.165161
Gradient norm: 1234.8515625
Rank 1, Epoch 2, Batch 175, Loss: 869.026367
Gradient norm: 1270.88818359375
Rank 1, Epoch 2, Batch 176, Loss: 555.873169
Gradient norm: 1204.9111328125
Rank 1, Epoch 2, Batch 177, Loss: 833.837524
Gradient norm: 1236.856201171875
Rank 1, Epoch 2, Batch 178, Loss: 715.562378
Gradient norm: 1043.3470458984375
Rank 1, Epoch 2, Batch 179, Loss: 466.826050
Gradient norm: 1173.3717041015625
Rank 1, Epoch 2, Batch 180, Loss: 567.813538
Gradient norm: 1185.544921875
Rank 1, Epoch 2, Batch 181, Loss: 837.758850
Gradient norm: 1247.5108642578125
Rank 1, Epoch 2, Batch 182, Loss: 900.567627
Gradient norm: 1210.4827880859375
Rank 1, Epoch 2, Batch 183, Loss: 807.225708
Gradient norm: 1235.0242919921875
Rank 1, Epoch 2, Batch 184, Loss: 556.043396
Gradient norm: 1288.2620849609375
Rank 1, Epoch 2, Batch 185, Loss: 564.110596
Gradient norm: 1113.050537109375
Rank 1, Epoch 2, Batch 186, Loss: 437.190247
Gradient norm: 1099.8985595703125
Rank 1, Epoch 2, Batch 187, Loss: 842.856140
Gradient norm: 1230.10498046875
Rank 1, Epoch 2, Batch 188, Loss: 719.713257
Gradient norm: 1293.666015625
Rank 1, Epoch 2, Batch 189, Loss: 598.997070
Gradient norm: 1189.1246337890625
Rank 1, Epoch 2, Batch 190, Loss: 616.774231
Gradient norm: 1257.094970703125
Rank 1, Epoch 2, Batch 191, Loss: 788.612671
Gradient norm: 1257.8739013671875
Rank 1, Epoch 2, Batch 192, Loss: 719.600586
Gradient norm: 1320.1644287109375
Rank 1, Epoch 2, Batch 193, Loss: 707.645325
Gradient norm: 1223.577392578125
Rank 1, Epoch 2, Batch 194, Loss: 921.385010
Gradient norm: 1246.6790771484375
Rank 1, Epoch 2, Batch 195, Loss: 1226.860352
Gradient norm: 1274.4775390625
Rank 1, Epoch 2, Batch 196, Loss: 805.150574
Gradient norm: 1214.530517578125
Rank 1, Epoch 2, Batch 197, Loss: 781.661865
Gradient norm: 1335.5787353515625
Rank 1, Epoch 2, Batch 198, Loss: 597.860107
Gradient norm: 1393.5616455078125
Rank 1, Epoch 2, Batch 199, Loss: 706.147705
Gradient norm: 1236.5467529296875
Rank 1, Epoch 2, Batch 200, Loss: 692.212219
Gradient norm: 1298.3427734375
Rank 1, Epoch 2, Batch 201, Loss: 630.289551
Gradient norm: 1378.2451171875
Rank 1, Epoch 2, Batch 202, Loss: 924.649597
Gradient norm: 1332.6407470703125
Rank 1, Epoch 2, Batch 203, Loss: 954.887329
Gradient norm: 1260.3150634765625
Rank 1, Epoch 2, Batch 204, Loss: 1011.024963
Gradient norm: 1315.3741455078125
Rank 1, Epoch 2, Batch 205, Loss: 1075.377319
Gradient norm: 1340.0225830078125
Rank 1, Epoch 2, Batch 206, Loss: 900.716553
Gradient norm: 1259.428466796875
Rank 1, Epoch 2, Batch 207, Loss: 759.764160
Gradient norm: 1301.4434814453125
Rank 1, Epoch 2, Batch 208, Loss: 633.217163
Gradient norm: 1231.021484375
Rank 1, Epoch 2, Batch 209, Loss: 538.548950
Gradient norm: 1443.7979736328125
Rank 1, Epoch 2, Batch 210, Loss: 999.764893
Gradient norm: 1390.134765625
Rank 1, Epoch 2, Batch 211, Loss: 839.609985
Rank 2, Epoch 2, Batch 107, Loss: 85.523796
Gradient norm: 463.1471862792969
Rank 2, Epoch 2, Batch 108, Loss: 92.283546
Gradient norm: 470.3580322265625
Rank 2, Epoch 2, Batch 109, Loss: 110.505638
Gradient norm: 491.92144775390625
Rank 2, Epoch 2, Batch 110, Loss: 111.591766
Gradient norm: 479.97540283203125
Rank 2, Epoch 2, Batch 111, Loss: 106.492691
Gradient norm: 498.5273742675781
Rank 2, Epoch 2, Batch 112, Loss: 121.290924
Gradient norm: 388.6903076171875
Rank 2, Epoch 2, Batch 113, Loss: 110.731987
Gradient norm: 507.0538635253906
Rank 2, Epoch 2, Batch 114, Loss: 123.131027
Gradient norm: 392.7868347167969
Rank 2, Epoch 2, Batch 115, Loss: 113.018295
Gradient norm: 530.2407836914062
Rank 2, Epoch 2, Batch 116, Loss: 75.130043
Gradient norm: 474.5362548828125
Rank 2, Epoch 2, Batch 117, Loss: 84.340843
Gradient norm: 499.9883117675781
Rank 2, Epoch 2, Batch 118, Loss: 68.564339
Gradient norm: 369.76654052734375
Rank 2, Epoch 2, Batch 119, Loss: 94.082954
Gradient norm: 506.596923828125
Rank 2, Epoch 2, Batch 120, Loss: 94.425575
Gradient norm: 467.5829162597656
Rank 2, Epoch 2, Batch 121, Loss: 127.317642
Gradient norm: 505.4869384765625
Rank 2, Epoch 2, Batch 122, Loss: 140.707535
Gradient norm: 461.3206481933594
Rank 2, Epoch 2, Batch 123, Loss: 123.282448
Gradient norm: 520.2295532226562
Rank 2, Epoch 2, Batch 124, Loss: 164.916168
Gradient norm: 520.554443359375
Rank 2, Epoch 2, Batch 125, Loss: 168.597534
Gradient norm: 466.48858642578125
Rank 2, Epoch 2, Batch 126, Loss: 91.051804
Gradient norm: 484.0095520019531
Rank 2, Epoch 2, Batch 127, Loss: 109.493858
Gradient norm: 485.5531921386719
Rank 2, Epoch 2, Batch 128, Loss: 63.541260
Gradient norm: 519.6962890625
Rank 2, Epoch 2, Batch 129, Loss: 93.665382
Gradient norm: 524.0189819335938
Rank 2, Epoch 2, Batch 130, Loss: 139.612717
Gradient norm: 543.7076416015625
Rank 2, Epoch 2, Batch 131, Loss: 126.590858
Gradient norm: 504.8550720214844
Rank 2, Epoch 2, Batch 132, Loss: 117.884506
Gradient norm: 555.4331665039062
Rank 2, Epoch 2, Batch 133, Loss: 138.380707
Gradient norm: 520.0284423828125
Rank 2, Epoch 2, Batch 134, Loss: 159.170929
Gradient norm: 558.3130493164062
Rank 2, Epoch 2, Batch 135, Loss: 221.573395
Gradient norm: 555.3532104492188
Rank 2, Epoch 2, Batch 136, Loss: 160.729874
Gradient norm: 527.4194946289062
Rank 2, Epoch 2, Batch 137, Loss: 139.220886
Gradient norm: 494.63348388671875
Rank 2, Epoch 2, Batch 138, Loss: 114.600372
Gradient norm: 564.3575439453125
Rank 2, Epoch 2, Batch 139, Loss: 126.984680
Gradient norm: 519.5352172851562
Rank 2, Epoch 2, Batch 140, Loss: 97.223724
Gradient norm: 537.0154418945312
Rank 2, Epoch 2, Batch 141, Loss: 115.418694
Gradient norm: 449.4994201660156
Rank 2, Epoch 2, Batch 142, Loss: 148.512390
Gradient norm: 575.9286499023438
Rank 2, Epoch 2, Batch 143, Loss: 121.750854
Gradient norm: 555.0398559570312
Rank 2, Epoch 2, Batch 144, Loss: 116.838806
Gradient norm: 546.82080078125
Rank 2, Epoch 2, Batch 145, Loss: 175.799194
Gradient norm: 538.1660766601562
Rank 2, Epoch 2, Batch 146, Loss: 181.736084
Gradient norm: 587.5418090820312
Rank 2, Epoch 2, Batch 147, Loss: 174.944260
Gradient norm: 539.3461303710938
Rank 2, Epoch 2, Batch 148, Loss: 134.602875
Gradient norm: 555.8380737304688
Rank 2, Epoch 2, Batch 149, Loss: 144.847839
Gradient norm: 608.6891479492188
Rank 2, Epoch 2, Batch 150, Loss: 161.635590
Gradient norm: 476.3039855957031
Rank 2, Epoch 2, Batch 151, Loss: 121.894493
Gradient norm: 585.8331298828125
Rank 2, Epoch 2, Batch 152, Loss: 176.414978
Gradient norm: 546.0650024414062
Rank 2, Epoch 2, Batch 153, Loss: 121.927567
Gradient norm: 543.7555541992188
Rank 2, Epoch 2, Batch 154, Loss: 114.078949
Gradient norm: 489.525146484375
Rank 2, Epoch 2, Batch 155, Loss: 149.470718
Gradient norm: 587.1538696289062
Rank 2, Epoch 2, Batch 156, Loss: 165.246155
Gradient norm: 513.5785522460938
Rank 2, Epoch 2, Batch 157, Loss: 180.889389
Gradient norm: 579.6194458007812
Rank 2, Epoch 2, Batch 158, Loss: 140.604095
Gradient norm: 572.2294921875
Rank 2, Epoch 2, Batch 159, Loss: 132.602356
Gradient norm: 582.4729614257812
Rank 2, Epoch 2, Batch 160, Loss: 152.781189
Gradient norm: 599.544677734375
Rank 2, Epoch 2, Batch 161, Loss: 122.582230
Gradient norm: 597.1480102539062
Rank 2, Epoch 2, Batch 162, Loss: 145.676422
Gradient norm: 531.9325561523438
Rank 2, Epoch 2, Batch 163, Loss: 148.464096
Gradient norm: 604.779296875
Rank 2, Epoch 2, Batch 164, Loss: 165.814346
Gradient norm: 638.5209350585938
Rank 2, Epoch 2, Batch 165, Loss: 242.215332
Gradient norm: 598.042236328125
Rank 2, Epoch 2, Batch 166, Loss: 223.842545
Gradient norm: 620.8716430664062
Rank 2, Epoch 2, Batch 167, Loss: 190.522064
Gradient norm: 558.2164306640625
Rank 2, Epoch 2, Batch 168, Loss: 175.153992
Gradient norm: 587.65185546875
Rank 2, Epoch 2, Batch 169, Loss: 182.724976
Gradient norm: 624.3258056640625
Rank 2, Epoch 2, Batch 170, Loss: 141.912506
Gradient norm: 632.24072265625
Rank 2, Epoch 2, Batch 171, Loss: 178.160095
Gradient norm: 660.966552734375
Rank 2, Epoch 2, Batch 172, Loss: 175.423920
Gradient norm: 606.3275146484375
Rank 2, Epoch 2, Batch 173, Loss: 136.617828
Gradient norm: 668.4625244140625
Rank 2, Epoch 2, Batch 174, Loss: 193.360107
Gradient norm: 665.4529418945312
Rank 2, Epoch 2, Batch 175, Loss: 242.284607
Gradient norm: 647.7525024414062
Rank 2, Epoch 2, Batch 176, Loss: 166.142059
Gradient norm: 536.5685424804688
Rank 2, Epoch 2, Batch 177, Loss: 183.813293
Gradient norm: 575.7444458007812
Rank 2, Epoch 2, Batch 178, Loss: 292.866333
Gradient norm: 592.3727416992188
Rank 2, Epoch 2, Batch 179, Loss: 134.152924
Gradient norm: 701.8768310546875
Rank 2, Epoch 2, Batch 180, Loss: 194.602798
Gradient norm: 472.63714599609375
Rank 2, Epoch 2, Batch 181, Loss: 111.185059
Gradient norm: 639.8602905273438
Rank 2, Epoch 2, Batch 182, Loss: 129.505661
Gradient norm: 673.3056640625
Rank 2, Epoch 2, Batch 183, Loss: 136.376205
Gradient norm: 660.38330078125
Rank 2, Epoch 2, Batch 184, Loss: 240.347824
Gradient norm: 644.4573364257812
Rank 2, Epoch 2, Batch 185, Loss: 245.558670
Gradient norm: 573.2625122070312
Rank 2, Epoch 2, Batch 186, Loss: 236.901947
Gradient norm: 691.0816040039062
Rank 2, Epoch 2, Batch 187, Loss: 294.038513
Gradient norm: 536.4307861328125
Rank 2, Epoch 2, Batch 188, Loss: 166.732086
Gradient norm: 665.1451416015625
Rank 2, Epoch 2, Batch 189, Loss: 126.323723
Gradient norm: 664.7982788085938
Rank 2, Epoch 2, Batch 190, Loss: 127.109337
Gradient norm: 687.4169921875
Rank 2, Epoch 2, Batch 191, Loss: 128.529282
Gradient norm: 637.2850341796875
Rank 2, Epoch 2, Batch 192, Loss: 174.864243
Gradient norm: 678.8944702148438
Rank 2, Epoch 2, Batch 193, Loss: 223.140076
Gradient norm: 711.3768920898438
Rank 2, Epoch 2, Batch 194, Loss: 283.176056
Gradient norm: 657.632080078125
Rank 2, Epoch 2, Batch 195, Loss: 276.864166
Gradient norm: 686.707763671875
Rank 2, Epoch 2, Batch 196, Loss: 276.223633
Gradient norm: 665.4956665039062
Rank 2, Epoch 2, Batch 197, Loss: 300.743103
Gradient norm: 718.6427612304688
Rank 2, Epoch 2, Batch 198, Loss: 276.926178
Gradient norm: 666.2100830078125
Rank 2, Epoch 2, Batch 199, Loss: 190.598892
Gradient norm: 753.9973754882812
Rank 2, Epoch 2, Batch 200, Loss: 198.166489
Gradient norm: 689.51025390625
Rank 2, Epoch 2, Batch 201, Loss: 189.730560
Gradient norm: 700.9315795898438
Rank 2, Epoch 2, Batch 202, Loss: 181.933624
Gradient norm: 693.0057373046875
Rank 2, Epoch 2, Batch 203, Loss: 197.397888
Gradient norm: 673.1011352539062
Rank 2, Epoch 2, Batch 204, Loss: 176.580475
Gradient norm: 637.15576171875
Rank 2, Epoch 2, Batch 205, Loss: 240.904755
Gradient norm: 757.082275390625
Rank 2, Epoch 2, Batch 206, Loss: 299.097656
Gradient norm: 697.1218872070312
Rank 2, Epoch 2, Batch 207, Loss: 283.878571
Gradient norm: 649.398193359375
Rank 2, Epoch 2, Batch 208, Loss: 310.253510
Gradient norm: 758.8248901367188
Rank 2, Epoch 2, Batch 209, Loss: 352.479065
Gradient norm: 748.0064697265625
Rank 2, Epoch 2, Batch 210, Loss: 200.597183
Gradient norm: 769.3391723632812
Rank 2, Epoch 2, Batch 211, Loss: 178.444382
Gradient norm: 757.41552734375
Rank 2, Epoch 2, Batch 212, Loss: 256.964844
Rank 0, Epoch 2, Batch 107, Loss: 136.278595
Gradient norm: 446.75152587890625
Rank 0, Epoch 2, Batch 108, Loss: 123.257553
Gradient norm: 422.4855651855469
Rank 0, Epoch 2, Batch 109, Loss: 101.669968
Gradient norm: 486.1846008300781
Rank 0, Epoch 2, Batch 110, Loss: 142.063843
Gradient norm: 472.18707275390625
Rank 0, Epoch 2, Batch 111, Loss: 84.073387
Gradient norm: 462.6188659667969
Rank 0, Epoch 2, Batch 112, Loss: 48.242912
Gradient norm: 512.3115844726562
Rank 0, Epoch 2, Batch 113, Loss: 91.551849
Gradient norm: 470.44158935546875
Rank 0, Epoch 2, Batch 114, Loss: 83.813622
Gradient norm: 372.9227600097656
Rank 0, Epoch 2, Batch 115, Loss: 80.833145
Gradient norm: 505.7167053222656
Rank 0, Epoch 2, Batch 116, Loss: 167.180328
Gradient norm: 516.6102905273438
Rank 0, Epoch 2, Batch 117, Loss: 151.984909
Gradient norm: 509.68408203125
Rank 0, Epoch 2, Batch 118, Loss: 149.366028
Gradient norm: 490.22296142578125
Rank 0, Epoch 2, Batch 119, Loss: 92.424957
Gradient norm: 477.1476745605469
Rank 0, Epoch 2, Batch 120, Loss: 96.967674
Gradient norm: 477.1939697265625
Rank 0, Epoch 2, Batch 121, Loss: 113.475464
Gradient norm: 526.053955078125
Rank 0, Epoch 2, Batch 122, Loss: 104.249664
Gradient norm: 454.9313659667969
Rank 0, Epoch 2, Batch 123, Loss: 105.979691
Gradient norm: 501.4091796875
Rank 0, Epoch 2, Batch 124, Loss: 149.152679
Gradient norm: 521.71435546875
Rank 0, Epoch 2, Batch 125, Loss: 103.298592
Gradient norm: 450.1377868652344
Rank 0, Epoch 2, Batch 126, Loss: 120.662064
Gradient norm: 501.72869873046875
Rank 0, Epoch 2, Batch 127, Loss: 116.936195
Gradient norm: 514.1348266601562
Rank 0, Epoch 2, Batch 128, Loss: 104.933113
Gradient norm: 437.4169921875
Rank 0, Epoch 2, Batch 129, Loss: 114.247978
Gradient norm: 485.20269775390625
Rank 0, Epoch 2, Batch 130, Loss: 106.383308
Gradient norm: 512.4152221679688
Rank 0, Epoch 2, Batch 131, Loss: 134.464417
Gradient norm: 472.89813232421875
Rank 0, Epoch 2, Batch 132, Loss: 129.543091
Gradient norm: 531.2218017578125
Rank 0, Epoch 2, Batch 133, Loss: 157.959015
Gradient norm: 529.8214721679688
Rank 0, Epoch 2, Batch 134, Loss: 114.909470
Gradient norm: 522.4996337890625
Rank 0, Epoch 2, Batch 135, Loss: 127.365814
Gradient norm: 512.0048217773438
Rank 0, Epoch 2, Batch 136, Loss: 106.416885
Gradient norm: 412.25860595703125
Rank 0, Epoch 2, Batch 137, Loss: 117.062782
Gradient norm: 506.8329162597656
Rank 0, Epoch 2, Batch 138, Loss: 122.722221
Gradient norm: 549.1103515625
Rank 0, Epoch 2, Batch 139, Loss: 112.633698
Gradient norm: 573.3399658203125
Rank 0, Epoch 2, Batch 140, Loss: 112.980377
Gradient norm: 535.6637573242188
Rank 0, Epoch 2, Batch 141, Loss: 131.705307
Gradient norm: 546.7412109375
Rank 0, Epoch 2, Batch 142, Loss: 163.100296
Gradient norm: 541.9326171875
Rank 0, Epoch 2, Batch 143, Loss: 155.233337
Gradient norm: 454.7677001953125
Rank 0, Epoch 2, Batch 144, Loss: 134.657379
Gradient norm: 562.0398559570312
Rank 0, Epoch 2, Batch 145, Loss: 157.989166
Gradient norm: 559.558349609375
Rank 0, Epoch 2, Batch 146, Loss: 168.971146
Gradient norm: 524.224609375
Rank 0, Epoch 2, Batch 147, Loss: 170.198746
Gradient norm: 580.1631469726562
Rank 0, Epoch 2, Batch 148, Loss: 99.548309
Gradient norm: 608.851318359375
Rank 0, Epoch 2, Batch 149, Loss: 107.974205
Gradient norm: 533.6592407226562
Rank 0, Epoch 2, Batch 150, Loss: 128.519226
Gradient norm: 516.8396606445312
Rank 0, Epoch 2, Batch 151, Loss: 138.810486
Gradient norm: 576.3627319335938
Rank 0, Epoch 2, Batch 152, Loss: 162.902786
Gradient norm: 584.636962890625
Rank 0, Epoch 2, Batch 153, Loss: 155.977417
Gradient norm: 532.166015625
Rank 0, Epoch 2, Batch 154, Loss: 165.631973
Gradient norm: 381.7557373046875
Rank 0, Epoch 2, Batch 155, Loss: 181.840927
Gradient norm: 607.0965576171875
Rank 0, Epoch 2, Batch 156, Loss: 173.604706
Gradient norm: 607.1450805664062
Rank 0, Epoch 2, Batch 157, Loss: 103.789612
Gradient norm: 541.8416137695312
Rank 0, Epoch 2, Batch 158, Loss: 126.154335
Gradient norm: 595.784423828125
Rank 0, Epoch 2, Batch 159, Loss: 151.302505
Gradient norm: 573.9542236328125
Rank 0, Epoch 2, Batch 160, Loss: 137.679108
Gradient norm: 624.4476318359375
Rank 0, Epoch 2, Batch 161, Loss: 119.956055
Gradient norm: 513.1554565429688
Rank 0, Epoch 2, Batch 162, Loss: 186.093277
Gradient norm: 540.1544799804688
Rank 0, Epoch 2, Batch 163, Loss: 200.042007
Gradient norm: 592.5414428710938
Rank 0, Epoch 2, Batch 164, Loss: 234.844971
Gradient norm: 631.3421020507812
Rank 0, Epoch 2, Batch 165, Loss: 214.906494
Gradient norm: 621.3797607421875
Rank 0, Epoch 2, Batch 166, Loss: 124.730209
Gradient norm: 598.4100341796875
Rank 0, Epoch 2, Batch 167, Loss: 120.237373
Gradient norm: 616.8834228515625
Rank 0, Epoch 2, Batch 168, Loss: 137.014771
Gradient norm: 627.4912109375
Rank 0, Epoch 2, Batch 169, Loss: 163.944687
Gradient norm: 605.979736328125
Rank 0, Epoch 2, Batch 170, Loss: 197.962265
Gradient norm: 570.9572143554688
Rank 0, Epoch 2, Batch 171, Loss: 222.857941
Gradient norm: 555.041748046875
Rank 0, Epoch 2, Batch 172, Loss: 173.598221
Gradient norm: 641.7455444335938
Rank 0, Epoch 2, Batch 173, Loss: 233.507156
Gradient norm: 606.72412109375
Rank 0, Epoch 2, Batch 174, Loss: 183.404083
Gradient norm: 606.3441162109375
Rank 0, Epoch 2, Batch 175, Loss: 175.710617
Gradient norm: 635.197509765625
Rank 0, Epoch 2, Batch 176, Loss: 156.000336
Gradient norm: 651.0316162109375
Rank 0, Epoch 2, Batch 177, Loss: 207.956390
Gradient norm: 646.1944580078125
Rank 0, Epoch 2, Batch 178, Loss: 193.160690
Gradient norm: 584.5930786132812
Rank 0, Epoch 2, Batch 179, Loss: 133.091507
Gradient norm: 629.2130737304688
Rank 0, Epoch 2, Batch 180, Loss: 214.908905
Gradient norm: 615.8840942382812
Rank 0, Epoch 2, Batch 181, Loss: 208.755829
Gradient norm: 674.12646484375
Rank 0, Epoch 2, Batch 182, Loss: 218.673492
Gradient norm: 653.7069091796875
Rank 0, Epoch 2, Batch 183, Loss: 183.828690
Gradient norm: 587.78857421875
Rank 0, Epoch 2, Batch 184, Loss: 202.111908
Gradient norm: 652.2252197265625
Rank 0, Epoch 2, Batch 185, Loss: 295.191620
Gradient norm: 663.2992553710938
Rank 0, Epoch 2, Batch 186, Loss: 93.110359
Gradient norm: 700.958740234375
Rank 0, Epoch 2, Batch 187, Loss: 159.332428
Gradient norm: 676.10791015625
Rank 0, Epoch 2, Batch 188, Loss: 197.688385
Gradient norm: 699.0244140625
Rank 0, Epoch 2, Batch 189, Loss: 187.032166
Gradient norm: 688.01025390625
Rank 0, Epoch 2, Batch 190, Loss: 151.188614
Gradient norm: 657.8240966796875
Rank 0, Epoch 2, Batch 191, Loss: 278.740845
Gradient norm: 715.1932983398438
Rank 0, Epoch 2, Batch 192, Loss: 300.128326
Gradient norm: 634.9257202148438
Rank 0, Epoch 2, Batch 193, Loss: 313.191650
Gradient norm: 662.4627075195312
Rank 0, Epoch 2, Batch 194, Loss: 284.870697
Gradient norm: 706.387451171875
Rank 0, Epoch 2, Batch 195, Loss: 273.352234
Gradient norm: 658.2750854492188
Rank 0, Epoch 2, Batch 196, Loss: 148.221786
Gradient norm: 682.979248046875
Rank 0, Epoch 2, Batch 197, Loss: 120.294594
Gradient norm: 622.6879272460938
Rank 0, Epoch 2, Batch 198, Loss: 188.362320
Gradient norm: 628.7222900390625
Rank 0, Epoch 2, Batch 199, Loss: 222.389954
Gradient norm: 685.7822265625
Rank 0, Epoch 2, Batch 200, Loss: 246.011597
Gradient norm: 708.3511352539062
Rank 0, Epoch 2, Batch 201, Loss: 268.242249
Gradient norm: 614.0169677734375
Rank 0, Epoch 2, Batch 202, Loss: 217.863983
Gradient norm: 760.0166015625
Rank 0, Epoch 2, Batch 203, Loss: 181.274933
Gradient norm: 702.3098754882812
Rank 0, Epoch 2, Batch 204, Loss: 264.031586
Gradient norm: 715.9954833984375
Rank 0, Epoch 2, Batch 205, Loss: 174.850677
Gradient norm: 636.0821533203125
Rank 0, Epoch 2, Batch 206, Loss: 210.649048
Gradient norm: 705.1397094726562
Rank 0, Epoch 2, Batch 207, Loss: 248.610580
Gradient norm: 756.1090087890625
Rank 0, Epoch 2, Batch 208, Loss: 325.196960
Gradient norm: 743.2434692382812
Rank 0, Epoch 2, Batch 209, Loss: 225.125916
Gradient norm: 739.5521240234375
Rank 0, Epoch 2, Batch 210, Loss: 233.717316
Gradient norm: 715.6851196289062
Rank 0, Epoch 2, Batch 211, Loss: 301.034119
Gradient norm: 696.9529418945312
Rank 0, Epoch 2, Batch 212, Loss: 211.749542
Gradient norm: 1249.6915283203125
Rank 1, Epoch 2, Batch 212, Loss: 830.301392
Gradient norm: 1374.9957275390625
Rank 1, Epoch 2, Batch 213, Loss: 715.244507
Gradient norm: 1408.0924072265625
Rank 1, Epoch 2, Batch 214, Loss: 1114.734741
Gradient norm: 1254.5308837890625
Rank 1, Epoch 2, Batch 215, Loss: 1036.170898
Gradient norm: 1417.68408203125
Rank 1, Epoch 2, Batch 216, Loss: 1124.144043
Gradient norm: 1411.326904296875
Rank 1, Epoch 2, Batch 217, Loss: 824.064331
Gradient norm: 1442.8836669921875
Rank 1, Epoch 2, Batch 218, Loss: 1050.672852
Gradient norm: 1388.6029052734375
Rank 1, Epoch 2, Batch 219, Loss: 697.539307
Gradient norm: 1360.32958984375
Rank 1, Epoch 2, Batch 220, Loss: 726.237183
Gradient norm: 1436.9464111328125
Rank 1, Epoch 2, Batch 221, Loss: 757.584351
Gradient norm: 1414.8392333984375
Rank 1, Epoch 2, Batch 222, Loss: 1243.197510
Gradient norm: 1450.90966796875
Rank 1, Epoch 2, Batch 223, Loss: 1122.984985
Gradient norm: 1445.9765625
Rank 1, Epoch 2, Batch 224, Loss: 1109.749512
Gradient norm: 1502.8984375
Rank 1, Epoch 2, Batch 225, Loss: 955.360901
Gradient norm: 1398.2288818359375
Rank 1, Epoch 2, Batch 226, Loss: 1316.500732
Gradient norm: 1422.2451171875
Rank 1, Epoch 2, Batch 227, Loss: 706.402222
Gradient norm: 1341.2701416015625
Rank 1, Epoch 2, Batch 228, Loss: 725.599243
Gradient norm: 1467.1688232421875
Rank 1, Epoch 2, Batch 229, Loss: 864.935974
Gradient norm: 1473.8507080078125
Rank 1, Epoch 2, Batch 230, Loss: 1013.065918
Gradient norm: 1507.97119140625
Rank 1, Epoch 2, Batch 231, Loss: 1083.418091
Gradient norm: 1546.7529296875
Rank 1, Epoch 2, Batch 232, Loss: 1184.277100
Gradient norm: 1591.5330810546875
Rank 1, Epoch 2, Batch 233, Loss: 1384.092041
Gradient norm: 1453.3070068359375
Rank 1, Epoch 2, Batch 234, Loss: 1054.438110
Gradient norm: 1450.2052001953125
Rank 1, Epoch 2, Batch 235, Loss: 1104.558105
Gradient norm: 1360.7012939453125
Rank 1, Epoch 2, Batch 236, Loss: 1069.553833
Gradient norm: 1343.60009765625
Rank 1, Epoch 2, Batch 237, Loss: 1116.870972
Gradient norm: 1573.8121337890625
Rank 1, Epoch 2, Batch 238, Loss: 1258.129883
Gradient norm: 1539.7166748046875
Rank 1, Epoch 2, Batch 239, Loss: 520.118164
Gradient norm: 1487.1199951171875
Rank 1, Epoch 2, Batch 240, Loss: 817.148438
Gradient norm: 1407.9410400390625
Rank 1, Epoch 2, Batch 241, Loss: 948.413208
Gradient norm: 1580.3284912109375
Rank 1, Epoch 2, Batch 242, Loss: 1000.695801
Gradient norm: 1511.9215087890625
Rank 1, Epoch 2, Batch 243, Loss: 1093.980469
Gradient norm: 1509.071533203125
Rank 1, Epoch 2, Batch 244, Loss: 1387.047363
Gradient norm: 1544.209716796875
Rank 1, Epoch 2, Batch 245, Loss: 1259.884033
Gradient norm: 1566.23486328125
Rank 1, Epoch 2, Batch 246, Loss: 1691.037842
Gradient norm: 1528.6173095703125
Rank 1, Epoch 2, Batch 247, Loss: 1505.521240
Gradient norm: 1493.3079833984375
Rank 1, Epoch 2, Batch 248, Loss: 665.736267
Gradient norm: 1476.484130859375
Rank 1, Epoch 2, Batch 249, Loss: 699.873474
Gradient norm: 1561.2156982421875
Rank 1, Epoch 2, Batch 250, Loss: 974.128418
Gradient norm: 1501.8631591796875
Rank 1, Epoch 2, Batch 251, Loss: 995.371338
Gradient norm: 1517.8856201171875
Rank 1, Epoch 2, Batch 252, Loss: 784.536133
Gradient norm: 1463.3160400390625
Rank 1, Epoch 2, Batch 253, Loss: 1270.692505
Gradient norm: 1548.673095703125
Rank 1, Epoch 2, Batch 254, Loss: 1517.542969
Gradient norm: 1483.0496826171875
Rank 1, Epoch 2, Batch 255, Loss: 1400.759399
Gradient norm: 1494.5616455078125
Rank 1, Epoch 2, Batch 256, Loss: 1499.059326
Gradient norm: 1561.038330078125
Rank 1, Epoch 2, Batch 257, Loss: 1259.631104
Gradient norm: 1523.166015625
Rank 1, Epoch 2, Batch 258, Loss: 511.596802
Gradient norm: 1686.413818359375
Rank 1, Epoch 2, Batch 259, Loss: 932.089294
Gradient norm: 1782.09033203125
Rank 1, Epoch 2, Batch 260, Loss: 1228.387451
Rank 1, Epoch 2, Val Loss: 17173.9521, Val Acc: 0.1000, Time: 206.62s
Gradient norm: 734.986328125
Rank 0, Epoch 2, Batch 213, Loss: 367.051147
Gradient norm: 675.9005126953125
Rank 0, Epoch 2, Batch 214, Loss: 254.436310
Gradient norm: 692.0142822265625
Rank 0, Epoch 2, Batch 215, Loss: 219.757477
Gradient norm: 759.251708984375
Rank 0, Epoch 2, Batch 216, Loss: 235.096039
Gradient norm: 779.193603515625
Rank 0, Epoch 2, Batch 217, Loss: 213.708984
Gradient norm: 751.6404418945312
Rank 0, Epoch 2, Batch 218, Loss: 216.262726
Gradient norm: 719.9849243164062
Rank 0, Epoch 2, Batch 219, Loss: 189.329514
Gradient norm: 759.36181640625
Rank 0, Epoch 2, Batch 220, Loss: 308.445007
Gradient norm: 812.6954956054688
Rank 0, Epoch 2, Batch 221, Loss: 335.866516
Gradient norm: 678.7147216796875
Rank 0, Epoch 2, Batch 222, Loss: 310.577332
Gradient norm: 791.060302734375
Rank 0, Epoch 2, Batch 223, Loss: 266.254761
Gradient norm: 783.4492797851562
Rank 0, Epoch 2, Batch 224, Loss: 357.757477
Gradient norm: 772.0165405273438
Rank 0, Epoch 2, Batch 225, Loss: 299.791260
Gradient norm: 729.6514892578125
Rank 0, Epoch 2, Batch 226, Loss: 325.164673
Gradient norm: 709.219482421875
Rank 0, Epoch 2, Batch 227, Loss: 195.725433
Gradient norm: 769.3396606445312
Rank 0, Epoch 2, Batch 228, Loss: 243.030609
Gradient norm: 768.05859375
Rank 0, Epoch 2, Batch 229, Loss: 287.275787
Gradient norm: 830.679443359375
Rank 0, Epoch 2, Batch 230, Loss: 283.651367
Gradient norm: 797.1181030273438
Rank 0, Epoch 2, Batch 231, Loss: 329.216614
Gradient norm: 723.3248291015625
Rank 0, Epoch 2, Batch 232, Loss: 226.234955
Gradient norm: 770.6856689453125
Rank 0, Epoch 2, Batch 233, Loss: 257.948181
Gradient norm: 803.7237548828125
Rank 0, Epoch 2, Batch 234, Loss: 266.908508
Gradient norm: 738.8005981445312
Rank 0, Epoch 2, Batch 235, Loss: 328.800842
Gradient norm: 836.8627319335938
Rank 0, Epoch 2, Batch 236, Loss: 360.717346
Gradient norm: 777.0855712890625
Rank 0, Epoch 2, Batch 237, Loss: 418.688385
Gradient norm: 826.6722412109375
Rank 0, Epoch 2, Batch 238, Loss: 339.010803
Gradient norm: 767.5338134765625
Rank 0, Epoch 2, Batch 239, Loss: 261.645966
Gradient norm: 835.22998046875
Rank 0, Epoch 2, Batch 240, Loss: 208.948700
Gradient norm: 787.7933349609375
Rank 0, Epoch 2, Batch 241, Loss: 264.957550
Gradient norm: 704.3909912109375
Rank 0, Epoch 2, Batch 242, Loss: 241.127029
Gradient norm: 884.6777954101562
Rank 0, Epoch 2, Batch 243, Loss: 310.843140
Gradient norm: 798.0905151367188
Rank 0, Epoch 2, Batch 244, Loss: 316.157318
Gradient norm: 863.324951171875
Rank 0, Epoch 2, Batch 245, Loss: 433.278717
Gradient norm: 853.5804443359375
Rank 0, Epoch 2, Batch 246, Loss: 411.156738
Gradient norm: 874.4404907226562
Rank 0, Epoch 2, Batch 247, Loss: 313.224335
Gradient norm: 739.3574829101562
Rank 0, Epoch 2, Batch 248, Loss: 405.415344
Gradient norm: 855.2031860351562
Rank 0, Epoch 2, Batch 249, Loss: 345.964996
Gradient norm: 854.5476684570312
Rank 0, Epoch 2, Batch 250, Loss: 379.555237
Gradient norm: 792.25244140625
Rank 0, Epoch 2, Batch 251, Loss: 237.894592
Gradient norm: 804.72021484375
Rank 0, Epoch 2, Batch 252, Loss: 226.156113
Gradient norm: 755.316650390625
Rank 0, Epoch 2, Batch 253, Loss: 361.907227
Gradient norm: 862.0802001953125
Rank 0, Epoch 2, Batch 254, Loss: 305.403961
Gradient norm: 828.299560546875
Rank 0, Epoch 2, Batch 255, Loss: 330.309723
Gradient norm: 824.7127075195312
Rank 0, Epoch 2, Batch 256, Loss: 361.536682
Gradient norm: 878.6788940429688
Rank 0, Epoch 2, Batch 257, Loss: 414.442749
Gradient norm: 857.7769775390625
Rank 0, Epoch 2, Batch 258, Loss: 309.983459
Gradient norm: 821.6435546875
Rank 0, Epoch 2, Batch 259, Loss: 322.450043
Gradient norm: 926.5270385742188
Rank 0, Epoch 2, Batch 260, Loss: 356.793396
Rank 0, Epoch 2, Val Loss: 110.2018, Val Acc: 0.1000, Time: 211.05s
Gradient norm: 725.6000366210938
Rank 2, Epoch 2, Batch 213, Loss: 235.612427
Gradient norm: 764.5990600585938
Rank 2, Epoch 2, Batch 214, Loss: 336.120789
Gradient norm: 714.828125
Rank 2, Epoch 2, Batch 215, Loss: 259.240417
Gradient norm: 690.271240234375
Rank 2, Epoch 2, Batch 216, Loss: 277.292480
Gradient norm: 755.9607543945312
Rank 2, Epoch 2, Batch 217, Loss: 293.407776
Gradient norm: 748.6063842773438
Rank 2, Epoch 2, Batch 218, Loss: 204.182281
Gradient norm: 708.2262573242188
Rank 2, Epoch 2, Batch 219, Loss: 194.025360
Gradient norm: 708.4163818359375
Rank 2, Epoch 2, Batch 220, Loss: 265.966248
Gradient norm: 784.3387451171875
Rank 2, Epoch 2, Batch 221, Loss: 310.297241
Gradient norm: 707.8081665039062
Rank 2, Epoch 2, Batch 222, Loss: 263.068726
Gradient norm: 768.8645629882812
Rank 2, Epoch 2, Batch 223, Loss: 387.176514
Gradient norm: 739.3358154296875
Rank 2, Epoch 2, Batch 224, Loss: 202.034195
Gradient norm: 765.9854125976562
Rank 2, Epoch 2, Batch 225, Loss: 184.761887
Gradient norm: 734.5747680664062
Rank 2, Epoch 2, Batch 226, Loss: 271.436859
Gradient norm: 785.8565673828125
Rank 2, Epoch 2, Batch 227, Loss: 354.392639
Gradient norm: 772.8412475585938
Rank 2, Epoch 2, Batch 228, Loss: 331.302094
Gradient norm: 803.51904296875
Rank 2, Epoch 2, Batch 229, Loss: 317.191467
Gradient norm: 761.4359741210938
Rank 2, Epoch 2, Batch 230, Loss: 276.977814
Gradient norm: 717.6695556640625
Rank 2, Epoch 2, Batch 231, Loss: 224.393799
Gradient norm: 825.3922729492188
Rank 2, Epoch 2, Batch 232, Loss: 272.902924
Gradient norm: 807.3977661132812
Rank 2, Epoch 2, Batch 233, Loss: 297.376038
Gradient norm: 788.83837890625
Rank 2, Epoch 2, Batch 234, Loss: 237.568024
Gradient norm: 722.0960083007812
Rank 2, Epoch 2, Batch 235, Loss: 214.150833
Gradient norm: 753.0565185546875
Rank 2, Epoch 2, Batch 236, Loss: 318.395966
Gradient norm: 832.8837890625
Rank 2, Epoch 2, Batch 237, Loss: 328.515106
Gradient norm: 808.6768188476562
Rank 2, Epoch 2, Batch 238, Loss: 371.527435
Gradient norm: 807.5684814453125
Rank 2, Epoch 2, Batch 239, Loss: 344.887543
Gradient norm: 761.0462036132812
Rank 2, Epoch 2, Batch 240, Loss: 314.346252
Gradient norm: 741.6372680664062
Rank 2, Epoch 2, Batch 241, Loss: 259.253998
Gradient norm: 840.526611328125
Rank 2, Epoch 2, Batch 242, Loss: 426.606079
Gradient norm: 773.6588134765625
Rank 2, Epoch 2, Batch 243, Loss: 260.606903
Gradient norm: 844.7854614257812
Rank 2, Epoch 2, Batch 244, Loss: 254.653091
Gradient norm: 723.7459716796875
Rank 2, Epoch 2, Batch 245, Loss: 231.861465
Gradient norm: 797.8544921875
Rank 2, Epoch 2, Batch 246, Loss: 287.349579
Gradient norm: 828.4010620117188
Rank 2, Epoch 2, Batch 247, Loss: 240.644669
Gradient norm: 871.2398681640625
Rank 2, Epoch 2, Batch 248, Loss: 396.360901
Gradient norm: 811.439208984375
Rank 2, Epoch 2, Batch 249, Loss: 335.228577
Gradient norm: 880.7158203125
Rank 2, Epoch 2, Batch 250, Loss: 482.846771
Gradient norm: 852.4330444335938
Rank 2, Epoch 2, Batch 251, Loss: 374.028412
Gradient norm: 918.2838745117188
Rank 2, Epoch 2, Batch 252, Loss: 450.253998
Gradient norm: 755.4803466796875
Rank 2, Epoch 2, Batch 253, Loss: 300.422791
Gradient norm: 818.596435546875
Rank 2, Epoch 2, Batch 254, Loss: 320.513336
Gradient norm: 840.0987548828125
Rank 2, Epoch 2, Batch 255, Loss: 336.628662
Gradient norm: 852.7718505859375
Rank 2, Epoch 2, Batch 256, Loss: 259.739014
Gradient norm: 789.4961547851562
Rank 2, Epoch 2, Batch 257, Loss: 231.846375
Gradient norm: 866.71630859375
Rank 2, Epoch 2, Batch 258, Loss: 290.942322
Gradient norm: 885.8605346679688
Rank 2, Epoch 2, Batch 259, Loss: 302.684082
Gradient norm: 933.4187622070312
Rank 2, Epoch 2, Batch 260, Loss: 485.231415
Rank 2, Epoch 2, Val Loss: 130.1228, Val Acc: 0.1000, Time: 210.59s
Gradient norm: 1597.99951171875
Rank 1, Epoch 3, Batch 0, Loss: 1324.221436
Gradient norm: 1403.40283203125
Rank 1, Epoch 3, Batch 1, Loss: 1132.103394
Gradient norm: 1582.47607421875
Rank 1, Epoch 3, Batch 2, Loss: 1387.905640
Gradient norm: 1426.94384765625
Rank 1, Epoch 3, Batch 3, Loss: 1188.101685
Gradient norm: 1552.2999267578125
Rank 1, Epoch 3, Batch 4, Loss: 1644.139160
Gradient norm: 1699.6719970703125
Rank 1, Epoch 3, Batch 5, Loss: 1059.698486
Gradient norm: 1700.9134521484375
Rank 1, Epoch 3, Batch 6, Loss: 1003.344727
Gradient norm: 1652.03955078125
Rank 1, Epoch 3, Batch 7, Loss: 1021.923645
Gradient norm: 1546.39501953125
Rank 1, Epoch 3, Batch 8, Loss: 1571.548462
Gradient norm: 1369.6651611328125
Rank 1, Epoch 3, Batch 9, Loss: 1041.274658
Gradient norm: 1557.7252197265625
Rank 1, Epoch 3, Batch 10, Loss: 1172.792725
Gradient norm: 1721.160888671875
Rank 1, Epoch 3, Batch 11, Loss: 1433.873291
Gradient norm: 1695.3380126953125
Rank 1, Epoch 3, Batch 12, Loss: 1067.047607
Gradient norm: 1690.0184326171875
Rank 1, Epoch 3, Batch 13, Loss: 1583.241943
Gradient norm: 1665.1221923828125
Rank 1, Epoch 3, Batch 14, Loss: 1346.919922
Gradient norm: 1749.98291015625
Rank 1, Epoch 3, Batch 15, Loss: 1777.194458
Gradient norm: 1718.5489501953125
Rank 1, Epoch 3, Batch 16, Loss: 1613.475220
Gradient norm: 1662.8638916015625
Rank 1, Epoch 3, Batch 17, Loss: 1105.491943
Gradient norm: 1651.737548828125
Rank 1, Epoch 3, Batch 18, Loss: 727.472046
Gradient norm: 1580.1622314453125
Rank 1, Epoch 3, Batch 19, Loss: 1050.753662
Gradient norm: 1654.865966796875
Rank 1, Epoch 3, Batch 20, Loss: 1573.568848
Gradient norm: 1778.8653564453125
Rank 1, Epoch 3, Batch 21, Loss: 1444.700439
Gradient norm: 1571.06005859375
Rank 1, Epoch 3, Batch 22, Loss: 1602.901611
Gradient norm: 1833.7510986328125
Rank 1, Epoch 3, Batch 23, Loss: 1754.197510
Gradient norm: 1826.658203125
Rank 1, Epoch 3, Batch 24, Loss: 1090.243774
Gradient norm: 1700.565673828125
Rank 1, Epoch 3, Batch 25, Loss: 1146.890503
Gradient norm: 1549.4417724609375
Rank 1, Epoch 3, Batch 26, Loss: 1116.234741
Gradient norm: 1590.4791259765625
Rank 1, Epoch 3, Batch 27, Loss: 1850.383911
Gradient norm: 1719.0970458984375
Rank 1, Epoch 3, Batch 28, Loss: 1257.066650
Gradient norm: 1762.399169921875
Rank 1, Epoch 3, Batch 29, Loss: 1458.185547
Gradient norm: 1857.4381103515625
Rank 1, Epoch 3, Batch 30, Loss: 1797.988037
Gradient norm: 1906.8699951171875
Rank 1, Epoch 3, Batch 31, Loss: 1086.954590
Gradient norm: 1493.9112548828125
Rank 1, Epoch 3, Batch 32, Loss: 1687.958374
Gradient norm: 1770.832275390625
Rank 1, Epoch 3, Batch 33, Loss: 1473.709473
Gradient norm: 1567.79443359375
Rank 1, Epoch 3, Batch 34, Loss: 1295.994873
Gradient norm: 1819.118896484375
Rank 1, Epoch 3, Batch 35, Loss: 1584.792358
Gradient norm: 1824.649169921875
Rank 1, Epoch 3, Batch 36, Loss: 1255.291138
Gradient norm: 1745.709228515625
Rank 1, Epoch 3, Batch 37, Loss: 1247.859497
Gradient norm: 1792.857177734375
Rank 1, Epoch 3, Batch 38, Loss: 1435.373413
Gradient norm: 1755.060546875
Rank 1, Epoch 3, Batch 39, Loss: 1991.938477
Gradient norm: 1741.333740234375
Rank 1, Epoch 3, Batch 40, Loss: 1946.441895
Gradient norm: 1586.7122802734375
Rank 1, Epoch 3, Batch 41, Loss: 1382.797607
Gradient norm: 1844.84912109375
Rank 1, Epoch 3, Batch 42, Loss: 1513.654785
Gradient norm: 1891.832763671875
Rank 1, Epoch 3, Batch 43, Loss: 1569.305420
Gradient norm: 1918.17041015625
Rank 1, Epoch 3, Batch 44, Loss: 1428.601807
Gradient norm: 1898.141357421875
Rank 1, Epoch 3, Batch 45, Loss: 1296.878784
Gradient norm: 1891.395263671875
Rank 1, Epoch 3, Batch 46, Loss: 858.682617
Gradient norm: 1881.9432373046875
Rank 1, Epoch 3, Batch 47, Loss: 1645.454834
Gradient norm: 1880.061279296875
Rank 1, Epoch 3, Batch 48, Loss: 2001.038330
Gradient norm: 1785.292724609375
Rank 1, Epoch 3, Batch 49, Loss: 2144.061279
Gradient norm: 1777.3651123046875
Rank 1, Epoch 3, Batch 50, Loss: 2361.097656
Gradient norm: 1789.1409912109375
Rank 1, Epoch 3, Batch 51, Loss: 1988.510498
Gradient norm: 1943.744384765625
Rank 1, Epoch 3, Batch 52, Loss: 1570.791260
Gradient norm: 1723.3277587890625
Rank 1, Epoch 3, Batch 53, Loss: 1223.897583
Gradient norm: 1939.2662353515625
Rank 1, Epoch 3, Batch 54, Loss: 1280.433105
Gradient norm: 1977.383056640625
Rank 1, Epoch 3, Batch 55, Loss: 976.262146
Gradient norm: 1976.5155029296875
Rank 1, Epoch 3, Batch 56, Loss: 1377.450195
Gradient norm: 1843.954345703125
Rank 1, Epoch 3, Batch 57, Loss: 1523.361816
Gradient norm: 1964.9071044921875
Rank 1, Epoch 3, Batch 58, Loss: 1775.905762
Gradient norm: 1991.278564453125
Rank 1, Epoch 3, Batch 59, Loss: 2658.926270
Gradient norm: 1977.0206298828125
Rank 1, Epoch 3, Batch 60, Loss: 3446.693848
Gradient norm: 2026.8468017578125
Rank 1, Epoch 3, Batch 61, Loss: 2386.660156
Gradient norm: 1934.9825439453125
Rank 1, Epoch 3, Batch 62, Loss: 1433.160645
Gradient norm: 2061.12841796875
Rank 1, Epoch 3, Batch 63, Loss: 1230.923218
Gradient norm: 1723.3507080078125
Rank 1, Epoch 3, Batch 64, Loss: 1304.644653
Gradient norm: 2014.7003173828125
Rank 1, Epoch 3, Batch 65, Loss: 1474.379272
Gradient norm: 1991.7210693359375
Rank 1, Epoch 3, Batch 66, Loss: 1968.613525
Gradient norm: 1915.3480224609375
Rank 1, Epoch 3, Batch 67, Loss: 1316.820190
Gradient norm: 2002.9713134765625
Rank 1, Epoch 3, Batch 68, Loss: 2320.043701
Gradient norm: 1873.8505859375
Rank 1, Epoch 3, Batch 69, Loss: 2554.963379
Gradient norm: 1976.5966796875
Rank 1, Epoch 3, Batch 70, Loss: 2461.360840
Gradient norm: 1985.6595458984375
Rank 1, Epoch 3, Batch 71, Loss: 2078.970459
Gradient norm: 2056.682861328125
Rank 1, Epoch 3, Batch 72, Loss: 2091.423584
Gradient norm: 1836.39453125
Rank 1, Epoch 3, Batch 73, Loss: 1374.587402
Gradient norm: 1924.400634765625
Rank 1, Epoch 3, Batch 74, Loss: 1596.325806
Gradient norm: 1980.6243896484375
Rank 1, Epoch 3, Batch 75, Loss: 1240.042480
Gradient norm: 1845.0494384765625
Rank 1, Epoch 3, Batch 76, Loss: 1585.190918
Gradient norm: 2051.05810546875
Rank 1, Epoch 3, Batch 77, Loss: 1616.519287
Gradient norm: 2002.136962890625
Rank 1, Epoch 3, Batch 78, Loss: 2018.398438
Gradient norm: 1980.95703125
Rank 1, Epoch 3, Batch 79, Loss: 1372.744141
Gradient norm: 1944.99853515625
Rank 1, Epoch 3, Batch 80, Loss: 1912.083618
Gradient norm: 1894.713623046875
Rank 1, Epoch 3, Batch 81, Loss: 2604.729004
Gradient norm: 1990.5650634765625
Rank 1, Epoch 3, Batch 82, Loss: 3363.823975
Gradient norm: 2071.807861328125
Rank 1, Epoch 3, Batch 83, Loss: 2084.701660
Gradient norm: 2135.0546875
Rank 1, Epoch 3, Batch 84, Loss: 1108.550415
Gradient norm: 2075.988525390625
Rank 1, Epoch 3, Batch 85, Loss: 1505.270996
Gradient norm: 1669.1597900390625
Rank 1, Epoch 3, Batch 86, Loss: 1481.849854
Gradient norm: 2106.62255859375
Rank 1, Epoch 3, Batch 87, Loss: 1580.545166
Gradient norm: 2083.4189453125
Rank 1, Epoch 3, Batch 88, Loss: 2273.640625
Gradient norm: 1890.7227783203125
Rank 1, Epoch 3, Batch 89, Loss: 1861.086182
Gradient norm: 2141.95654296875
Rank 1, Epoch 3, Batch 90, Loss: 2472.969727
Gradient norm: 2006.904052734375
Rank 1, Epoch 3, Batch 91, Loss: 2033.652344
Gradient norm: 1866.0572509765625
Rank 1, Epoch 3, Batch 92, Loss: 1885.337402
Gradient norm: 1973.531494140625
Rank 1, Epoch 3, Batch 93, Loss: 2359.078613
Gradient norm: 2212.222412109375
Rank 1, Epoch 3, Batch 94, Loss: 2699.841064
Gradient norm: 2243.280517578125
Rank 1, Epoch 3, Batch 95, Loss: 2185.317139
Gradient norm: 2093.886474609375
Rank 1, Epoch 3, Batch 96, Loss: 1482.506470
Gradient norm: 2125.712890625
Rank 1, Epoch 3, Batch 97, Loss: 1812.351318
Gradient norm: 2185.15576171875
Rank 1, Epoch 3, Batch 98, Loss: 2214.895020
Gradient norm: 2138.273681640625
Rank 1, Epoch 3, Batch 99, Loss: 2414.268066
Gradient norm: 2271.241943359375
Rank 1, Epoch 3, Batch 100, Loss: 2239.038330
Gradient norm: 2069.4716796875
Rank 1, Epoch 3, Batch 101, Loss: 2307.553711
Gradient norm: 2204.828857421875
Rank 1, Epoch 3, Batch 102, Loss: 2296.370361
Gradient norm: 2031.4691162109375
Rank 1, Epoch 3, Batch 103, Loss: 3007.917480
Gradient norm: 2166.17724609375
Rank 1, Epoch 3, Batch 104, Loss: 2051.380371
Gradient norm: 807.864013671875
Rank 0, Epoch 3, Batch 0, Loss: 296.739197
Gradient norm: 843.40673828125
Rank 0, Epoch 3, Batch 1, Loss: 412.929901
Gradient norm: 922.2980346679688
Rank 0, Epoch 3, Batch 2, Loss: 452.533081
Gradient norm: 855.1931762695312
Rank 0, Epoch 3, Batch 3, Loss: 476.963074
Gradient norm: 942.6395263671875
Rank 0, Epoch 3, Batch 4, Loss: 312.121399
Gradient norm: 864.6300659179688
Rank 0, Epoch 3, Batch 5, Loss: 340.144409
Gradient norm: 804.8986206054688
Rank 0, Epoch 3, Batch 6, Loss: 366.986237
Gradient norm: 906.271484375
Rank 0, Epoch 3, Batch 7, Loss: 363.613129
Gradient norm: 816.4362182617188
Rank 0, Epoch 3, Batch 8, Loss: 296.551514
Gradient norm: 954.0032348632812
Rank 0, Epoch 3, Batch 9, Loss: 368.361969
Gradient norm: 930.7195434570312
Rank 0, Epoch 3, Batch 10, Loss: 434.728943
Gradient norm: 825.4368896484375
Rank 0, Epoch 3, Batch 11, Loss: 308.219604
Gradient norm: 878.2184448242188
Rank 0, Epoch 3, Batch 12, Loss: 335.871399
Gradient norm: 973.3272705078125
Rank 0, Epoch 3, Batch 13, Loss: 353.146271
Gradient norm: 789.9154663085938
Rank 0, Epoch 3, Batch 14, Loss: 422.806824
Gradient norm: 889.3201293945312
Rank 0, Epoch 3, Batch 15, Loss: 516.225342
Gradient norm: 883.6674194335938
Rank 0, Epoch 3, Batch 16, Loss: 352.966400
Gradient norm: 969.7437744140625
Rank 0, Epoch 3, Batch 17, Loss: 496.871429
Gradient norm: 884.352294921875
Rank 0, Epoch 3, Batch 18, Loss: 391.937042
Gradient norm: 920.6246948242188
Rank 0, Epoch 3, Batch 19, Loss: 370.369629
Gradient norm: 828.2864379882812
Rank 0, Epoch 3, Batch 20, Loss: 298.278259
Gradient norm: 965.3228759765625
Rank 0, Epoch 3, Batch 21, Loss: 470.457336
Gradient norm: 911.8712768554688
Rank 0, Epoch 3, Batch 22, Loss: 427.426575
Gradient norm: 944.8831787109375
Rank 0, Epoch 3, Batch 23, Loss: 302.294556
Gradient norm: 967.0526733398438
Rank 0, Epoch 3, Batch 24, Loss: 415.667267
Gradient norm: 998.3159790039062
Rank 0, Epoch 3, Batch 25, Loss: 461.165405
Gradient norm: 925.2832641601562
Rank 0, Epoch 3, Batch 26, Loss: 387.843903
Gradient norm: 903.3959350585938
Rank 0, Epoch 3, Batch 27, Loss: 368.908997
Gradient norm: 867.405029296875
Rank 0, Epoch 3, Batch 28, Loss: 455.973999
Gradient norm: 977.6258544921875
Rank 0, Epoch 3, Batch 29, Loss: 554.474854
Gradient norm: 895.8995361328125
Rank 0, Epoch 3, Batch 30, Loss: 615.127625
Gradient norm: 991.7876586914062
Rank 0, Epoch 3, Batch 31, Loss: 366.530273
Gradient norm: 879.2412719726562
Rank 0, Epoch 3, Batch 32, Loss: 296.145905
Gradient norm: 1006.8397216796875
Rank 0, Epoch 3, Batch 33, Loss: 384.484924
Gradient norm: 970.1932983398438
Rank 0, Epoch 3, Batch 34, Loss: 453.048096
Gradient norm: 888.0677490234375
Rank 0, Epoch 3, Batch 35, Loss: 296.717041
Gradient norm: 955.3272094726562
Rank 0, Epoch 3, Batch 36, Loss: 394.629120
Gradient norm: 979.2359619140625
Rank 0, Epoch 3, Batch 37, Loss: 529.413574
Gradient norm: 924.2572631835938
Rank 0, Epoch 3, Batch 38, Loss: 441.057739
Gradient norm: 1030.0557861328125
Rank 0, Epoch 3, Batch 39, Loss: 553.104004
Gradient norm: 975.1452026367188
Rank 0, Epoch 3, Batch 40, Loss: 492.812683
Gradient norm: 1012.2296142578125
Rank 0, Epoch 3, Batch 41, Loss: 545.459045
Gradient norm: 907.514892578125
Rank 0, Epoch 3, Batch 42, Loss: 419.652161
Gradient norm: 1040.26220703125
Rank 0, Epoch 3, Batch 43, Loss: 441.686737
Gradient norm: 998.3897705078125
Rank 0, Epoch 3, Batch 44, Loss: 613.900391
Gradient norm: 946.8944091796875
Rank 0, Epoch 3, Batch 45, Loss: 364.079315
Gradient norm: 892.2617797851562
Rank 0, Epoch 3, Batch 46, Loss: 303.706482
Gradient norm: 1046.406982421875
Rank 0, Epoch 3, Batch 47, Loss: 359.418915
Gradient norm: 877.1644897460938
Rank 0, Epoch 3, Batch 48, Loss: 562.829224
Gradient norm: 918.3212890625
Rank 0, Epoch 3, Batch 49, Loss: 479.807007
Gradient norm: 853.052978515625
Rank 0, Epoch 3, Batch 50, Loss: 436.754730
Gradient norm: 1021.0938110351562
Rank 0, Epoch 3, Batch 51, Loss: 421.847412
Gradient norm: 1024.162353515625
Rank 0, Epoch 3, Batch 52, Loss: 344.847656
Gradient norm: 926.4959106445312
Rank 0, Epoch 3, Batch 53, Loss: 377.402618
Gradient norm: 963.6884765625
Rank 0, Epoch 3, Batch 54, Loss: 552.735901
Gradient norm: 1062.9556884765625
Rank 0, Epoch 3, Batch 55, Loss: 542.047180
Gradient norm: 903.0511474609375
Rank 0, Epoch 3, Batch 56, Loss: 336.657410
Gradient norm: 1046.4151611328125
Rank 0, Epoch 3, Batch 57, Loss: 678.045288
Gradient norm: 1092.751708984375
Rank 0, Epoch 3, Batch 58, Loss: 533.349487
Gradient norm: 995.8728637695312
Rank 0, Epoch 3, Batch 59, Loss: 437.819153
Gradient norm: 983.0930786132812
Rank 0, Epoch 3, Batch 60, Loss: 444.723022
Gradient norm: 994.642333984375
Rank 0, Epoch 3, Batch 61, Loss: 538.938782
Gradient norm: 1039.6800537109375
Rank 0, Epoch 3, Batch 62, Loss: 542.705383
Gradient norm: 994.135498046875
Rank 0, Epoch 3, Batch 63, Loss: 408.712646
Gradient norm: 1103.6842041015625
Rank 0, Epoch 3, Batch 64, Loss: 535.958435
Gradient norm: 1125.930908203125
Rank 0, Epoch 3, Batch 65, Loss: 554.278809
Gradient norm: 1087.2642822265625
Rank 0, Epoch 3, Batch 66, Loss: 621.831177
Gradient norm: 1131.1376953125
Rank 0, Epoch 3, Batch 67, Loss: 534.294434
Gradient norm: 1075.1480712890625
Rank 0, Epoch 3, Batch 68, Loss: 538.806946
Gradient norm: 1058.942626953125
Rank 0, Epoch 3, Batch 69, Loss: 451.198700
Gradient norm: 1068.5106201171875
Rank 0, Epoch 3, Batch 70, Loss: 717.081970
Gradient norm: 1104.2862548828125
Rank 0, Epoch 3, Batch 71, Loss: 686.482788
Gradient norm: 1093.2967529296875
Rank 0, Epoch 3, Batch 72, Loss: 807.424133
Gradient norm: 1139.012451171875
Rank 0, Epoch 3, Batch 73, Loss: 576.661011
Gradient norm: 1099.1585693359375
Rank 0, Epoch 3, Batch 74, Loss: 505.149475
Gradient norm: 1061.688720703125
Rank 0, Epoch 3, Batch 75, Loss: 372.155212
Gradient norm: 1097.3714599609375
Rank 0, Epoch 3, Batch 76, Loss: 356.460266
Gradient norm: 1142.1475830078125
Rank 0, Epoch 3, Batch 77, Loss: 733.979126
Gradient norm: 1070.635498046875
Rank 0, Epoch 3, Batch 78, Loss: 608.316467
Gradient norm: 1165.1676025390625
Rank 0, Epoch 3, Batch 79, Loss: 740.824341
Gradient norm: 1124.706787109375
Rank 0, Epoch 3, Batch 80, Loss: 720.042664
Gradient norm: 1116.5040283203125
Rank 0, Epoch 3, Batch 81, Loss: 666.352539
Gradient norm: 1089.574462890625
Rank 0, Epoch 3, Batch 82, Loss: 471.113892
Gradient norm: 1091.839111328125
Rank 0, Epoch 3, Batch 83, Loss: 490.499603
Gradient norm: 1021.2872314453125
Rank 0, Epoch 3, Batch 84, Loss: 568.938538
Gradient norm: 1083.3896484375
Rank 0, Epoch 3, Batch 85, Loss: 744.715454
Gradient norm: 1121.71826171875
Rank 0, Epoch 3, Batch 86, Loss: 657.743652
Gradient norm: 1103.3492431640625
Rank 0, Epoch 3, Batch 87, Loss: 561.973755
Gradient norm: 1145.7076416015625
Rank 0, Epoch 3, Batch 88, Loss: 541.175110
Gradient norm: 1161.6177978515625
Rank 0, Epoch 3, Batch 89, Loss: 540.255981
Gradient norm: 1069.099853515625
Rank 0, Epoch 3, Batch 90, Loss: 517.761292
Gradient norm: 1120.0845947265625
Rank 0, Epoch 3, Batch 91, Loss: 509.700439
Gradient norm: 1043.2176513671875
Rank 0, Epoch 3, Batch 92, Loss: 556.703308
Gradient norm: 1209.4102783203125
Rank 0, Epoch 3, Batch 93, Loss: 669.954773
Gradient norm: 1152.761962890625
Rank 0, Epoch 3, Batch 94, Loss: 678.758057
Gradient norm: 1179.5562744140625
Rank 0, Epoch 3, Batch 95, Loss: 719.353210
Gradient norm: 1092.832763671875
Rank 0, Epoch 3, Batch 96, Loss: 645.364014
Gradient norm: 1139.316162109375
Rank 0, Epoch 3, Batch 97, Loss: 669.950317
Gradient norm: 1168.8309326171875
Rank 0, Epoch 3, Batch 98, Loss: 613.072388
Gradient norm: 1150.5306396484375
Rank 0, Epoch 3, Batch 99, Loss: 755.602356
Gradient norm: 1104.789306640625
Rank 0, Epoch 3, Batch 100, Loss: 604.812988
Gradient norm: 1131.2852783203125
Rank 0, Epoch 3, Batch 101, Loss: 549.552368
Gradient norm: 1199.7767333984375
Rank 0, Epoch 3, Batch 102, Loss: 779.601013
Gradient norm: 984.3848266601562
Rank 0, Epoch 3, Batch 103, Loss: 531.045166
Gradient norm: 1182.2445068359375
Rank 0, Epoch 3, Batch 104, Loss: 644.226379
Gradient norm: 1181.7750244140625
Rank 0, Epoch 3, Batch 105, Loss: 687.273682
Gradient norm: 749.2997436523438
Rank 2, Epoch 3, Batch 0, Loss: 326.146027
Gradient norm: 831.102294921875
Rank 2, Epoch 3, Batch 1, Loss: 276.376831
Gradient norm: 877.4561767578125
Rank 2, Epoch 3, Batch 2, Loss: 477.678528
Gradient norm: 839.0342407226562
Rank 2, Epoch 3, Batch 3, Loss: 506.827820
Gradient norm: 886.4457397460938
Rank 2, Epoch 3, Batch 4, Loss: 373.106567
Gradient norm: 865.0380859375
Rank 2, Epoch 3, Batch 5, Loss: 297.820679
Gradient norm: 820.09521484375
Rank 2, Epoch 3, Batch 6, Loss: 276.171234
Gradient norm: 775.8701782226562
Rank 2, Epoch 3, Batch 7, Loss: 275.190063
Gradient norm: 860.5142822265625
Rank 2, Epoch 3, Batch 8, Loss: 380.636658
Gradient norm: 824.7177124023438
Rank 2, Epoch 3, Batch 9, Loss: 230.957382
Gradient norm: 815.7310180664062
Rank 2, Epoch 3, Batch 10, Loss: 441.587830
Gradient norm: 899.0056762695312
Rank 2, Epoch 3, Batch 11, Loss: 359.733612
Gradient norm: 918.2589721679688
Rank 2, Epoch 3, Batch 12, Loss: 467.866302
Gradient norm: 897.1411743164062
Rank 2, Epoch 3, Batch 13, Loss: 288.798676
Gradient norm: 899.099853515625
Rank 2, Epoch 3, Batch 14, Loss: 272.806976
Gradient norm: 819.0382690429688
Rank 2, Epoch 3, Batch 15, Loss: 389.829987
Gradient norm: 893.2704467773438
Rank 2, Epoch 3, Batch 16, Loss: 422.799622
Gradient norm: 889.1607055664062
Rank 2, Epoch 3, Batch 17, Loss: 356.962189
Gradient norm: 937.4353637695312
Rank 2, Epoch 3, Batch 18, Loss: 460.524475
Gradient norm: 801.66064453125
Rank 2, Epoch 3, Batch 19, Loss: 459.360840
Gradient norm: 807.019775390625
Rank 2, Epoch 3, Batch 20, Loss: 233.365753
Gradient norm: 954.2808837890625
Rank 2, Epoch 3, Batch 21, Loss: 394.838806
Gradient norm: 886.9295654296875
Rank 2, Epoch 3, Batch 22, Loss: 510.523895
Gradient norm: 930.3470458984375
Rank 2, Epoch 3, Batch 23, Loss: 346.929260
Gradient norm: 978.1715698242188
Rank 2, Epoch 3, Batch 24, Loss: 395.325714
Gradient norm: 961.1367797851562
Rank 2, Epoch 3, Batch 25, Loss: 386.336395
Gradient norm: 969.9177856445312
Rank 2, Epoch 3, Batch 26, Loss: 272.527130
Gradient norm: 947.8106079101562
Rank 2, Epoch 3, Batch 27, Loss: 467.000702
Gradient norm: 951.9590454101562
Rank 2, Epoch 3, Batch 28, Loss: 399.770813
Gradient norm: 897.4280395507812
Rank 2, Epoch 3, Batch 29, Loss: 442.469330
Gradient norm: 865.8482055664062
Rank 2, Epoch 3, Batch 30, Loss: 595.575562
Gradient norm: 986.6185913085938
Rank 2, Epoch 3, Batch 31, Loss: 574.492737
Gradient norm: 937.7030639648438
Rank 2, Epoch 3, Batch 32, Loss: 470.095062
Gradient norm: 1007.521728515625
Rank 2, Epoch 3, Batch 33, Loss: 282.522858
Gradient norm: 1004.0812377929688
Rank 2, Epoch 3, Batch 34, Loss: 405.120300
Gradient norm: 976.8721313476562
Rank 2, Epoch 3, Batch 35, Loss: 416.441956
Gradient norm: 941.0007934570312
Rank 2, Epoch 3, Batch 36, Loss: 430.077332
Gradient norm: 949.5438842773438
Rank 2, Epoch 3, Batch 37, Loss: 410.426697
Gradient norm: 958.4912719726562
Rank 2, Epoch 3, Batch 38, Loss: 427.290100
Gradient norm: 950.5203247070312
Rank 2, Epoch 3, Batch 39, Loss: 582.443665
Gradient norm: 969.5177001953125
Rank 2, Epoch 3, Batch 40, Loss: 615.022461
Gradient norm: 989.97021484375
Rank 2, Epoch 3, Batch 41, Loss: 356.650574
Gradient norm: 1045.013427734375
Rank 2, Epoch 3, Batch 42, Loss: 448.540771
Gradient norm: 980.8515014648438
Rank 2, Epoch 3, Batch 43, Loss: 436.033295
Gradient norm: 1003.885009765625
Rank 2, Epoch 3, Batch 44, Loss: 595.007080
Gradient norm: 1019.5391235351562
Rank 2, Epoch 3, Batch 45, Loss: 563.139465
Gradient norm: 1067.717529296875
Rank 2, Epoch 3, Batch 46, Loss: 443.204773
Gradient norm: 915.2911987304688
Rank 2, Epoch 3, Batch 47, Loss: 520.577820
Gradient norm: 1050.2213134765625
Rank 2, Epoch 3, Batch 48, Loss: 592.570923
Gradient norm: 984.3565673828125
Rank 2, Epoch 3, Batch 49, Loss: 321.192322
Gradient norm: 1016.0581665039062
Rank 2, Epoch 3, Batch 50, Loss: 472.466736
Gradient norm: 1009.7353515625
Rank 2, Epoch 3, Batch 51, Loss: 511.465881
Gradient norm: 1042.544189453125
Rank 2, Epoch 3, Batch 52, Loss: 486.904663
Gradient norm: 1062.7099609375
Rank 2, Epoch 3, Batch 53, Loss: 545.345581
Gradient norm: 1021.5591430664062
Rank 2, Epoch 3, Batch 54, Loss: 398.190338
Gradient norm: 1020.7247924804688
Rank 2, Epoch 3, Batch 55, Loss: 426.050995
Gradient norm: 1074.5784912109375
Rank 2, Epoch 3, Batch 56, Loss: 443.295044
Gradient norm: 991.4248657226562
Rank 2, Epoch 3, Batch 57, Loss: 693.632690
Gradient norm: 1059.7017822265625
Rank 2, Epoch 3, Batch 58, Loss: 676.419434
Gradient norm: 997.67041015625
Rank 2, Epoch 3, Batch 59, Loss: 807.201538
Gradient norm: 1055.7313232421875
Rank 2, Epoch 3, Batch 60, Loss: 428.925781
Gradient norm: 896.5740356445312
Rank 2, Epoch 3, Batch 61, Loss: 224.823334
Gradient norm: 1018.0401611328125
Rank 2, Epoch 3, Batch 62, Loss: 278.558411
Gradient norm: 1017.6221923828125
Rank 2, Epoch 3, Batch 63, Loss: 409.460449
Gradient norm: 1043.05322265625
Rank 2, Epoch 3, Batch 64, Loss: 313.256287
Gradient norm: 1039.54833984375
Rank 2, Epoch 3, Batch 65, Loss: 753.238892
Gradient norm: 1060.8526611328125
Rank 2, Epoch 3, Batch 66, Loss: 477.144501
Gradient norm: 1055.617919921875
Rank 2, Epoch 3, Batch 67, Loss: 696.721375
Gradient norm: 1097.886962890625
Rank 2, Epoch 3, Batch 68, Loss: 645.699341
Gradient norm: 1131.6402587890625
Rank 2, Epoch 3, Batch 69, Loss: 634.459229
Gradient norm: 1016.4252319335938
Rank 2, Epoch 3, Batch 70, Loss: 580.060974
Gradient norm: 1052.32177734375
Rank 2, Epoch 3, Batch 71, Loss: 388.816528
Gradient norm: 1055.574462890625
Rank 2, Epoch 3, Batch 72, Loss: 457.761292
Gradient norm: 1147.0491943359375
Rank 2, Epoch 3, Batch 73, Loss: 746.831482
Gradient norm: 1027.5863037109375
Rank 2, Epoch 3, Batch 74, Loss: 546.734375
Gradient norm: 1064.121337890625
Rank 2, Epoch 3, Batch 75, Loss: 763.677856
Gradient norm: 1049.7125244140625
Rank 2, Epoch 3, Batch 76, Loss: 525.292297
Gradient norm: 1141.9619140625
Rank 2, Epoch 3, Batch 77, Loss: 620.561401
Gradient norm: 1053.76513671875
Rank 2, Epoch 3, Batch 78, Loss: 270.609985
Gradient norm: 1109.1348876953125
Rank 2, Epoch 3, Batch 79, Loss: 507.522034
Gradient norm: 984.4632568359375
Rank 2, Epoch 3, Batch 80, Loss: 464.921021
Gradient norm: 1009.2499389648438
Rank 2, Epoch 3, Batch 81, Loss: 435.372589
Gradient norm: 1113.0921630859375
Rank 2, Epoch 3, Batch 82, Loss: 478.536835
Gradient norm: 1145.6871337890625
Rank 2, Epoch 3, Batch 83, Loss: 596.632324
Gradient norm: 1160.462158203125
Rank 2, Epoch 3, Batch 84, Loss: 721.636353
Gradient norm: 1122.6044921875
Rank 2, Epoch 3, Batch 85, Loss: 896.121094
Gradient norm: 1027.555908203125
Rank 2, Epoch 3, Batch 86, Loss: 875.945740
Gradient norm: 944.7546997070312
Rank 2, Epoch 3, Batch 87, Loss: 378.082062
Gradient norm: 1116.9534912109375
Rank 2, Epoch 3, Batch 88, Loss: 426.962891
Gradient norm: 1156.29443359375
Rank 2, Epoch 3, Batch 89, Loss: 432.106293
Gradient norm: 1077.310302734375
Rank 2, Epoch 3, Batch 90, Loss: 576.685791
Gradient norm: 1117.6937255859375
Rank 2, Epoch 3, Batch 91, Loss: 521.971497
Gradient norm: 1004.5455932617188
Rank 2, Epoch 3, Batch 92, Loss: 511.352142
Gradient norm: 1154.7674560546875
Rank 2, Epoch 3, Batch 93, Loss: 551.508484
Gradient norm: 1068.6171875
Rank 2, Epoch 3, Batch 94, Loss: 674.323914
Gradient norm: 1138.608642578125
Rank 2, Epoch 3, Batch 95, Loss: 817.479492
Gradient norm: 1185.8419189453125
Rank 2, Epoch 3, Batch 96, Loss: 787.153320
Gradient norm: 1157.1229248046875
Rank 2, Epoch 3, Batch 97, Loss: 844.199646
Gradient norm: 1145.1322021484375
Rank 2, Epoch 3, Batch 98, Loss: 373.742096
Gradient norm: 1161.798583984375
Rank 2, Epoch 3, Batch 99, Loss: 304.612366
Gradient norm: 1186.134033203125
Rank 2, Epoch 3, Batch 100, Loss: 455.239380
Gradient norm: 1207.007080078125
Rank 2, Epoch 3, Batch 101, Loss: 672.307617
Gradient norm: 1154.7806396484375
Rank 2, Epoch 3, Batch 102, Loss: 704.780396
Gradient norm: 1056.7154541015625
Rank 2, Epoch 3, Batch 103, Loss: 882.529785
Gradient norm: 1080.374755859375
Rank 2, Epoch 3, Batch 104, Loss: 944.474609
Gradient norm: 1075.5609130859375
Rank 2, Epoch 3, Batch 105, Loss: 527.765137
Gradient norm: 1170.885009765625
Gradient norm: 1220.5482177734375
Rank 0, Epoch 3, Batch 106, Loss: 595.310181
Gradient norm: 1028.014892578125
Rank 0, Epoch 3, Batch 107, Loss: 523.650818
Gradient norm: 1169.56103515625
Rank 0, Epoch 3, Batch 108, Loss: 432.215607
Gradient norm: 1122.0819091796875
Rank 0, Epoch 3, Batch 109, Loss: 607.036621
Gradient norm: 1257.431396484375
Rank 0, Epoch 3, Batch 110, Loss: 703.713745
Gradient norm: 1249.79248046875
Rank 0, Epoch 3, Batch 111, Loss: 863.529907
Gradient norm: 1082.5672607421875
Rank 0, Epoch 3, Batch 112, Loss: 694.029236
Gradient norm: 1222.696533203125
Rank 0, Epoch 3, Batch 113, Loss: 803.017273
Gradient norm: 1232.8826904296875
Rank 0, Epoch 3, Batch 114, Loss: 611.894165
Gradient norm: 1153.4796142578125
Rank 0, Epoch 3, Batch 115, Loss: 899.794189
Gradient norm: 1124.464599609375
Rank 0, Epoch 3, Batch 116, Loss: 806.719971
Gradient norm: 1270.03076171875
Rank 0, Epoch 3, Batch 117, Loss: 340.937988
Gradient norm: 1160.410888671875
Rank 0, Epoch 3, Batch 118, Loss: 478.340149
Gradient norm: 1296.2470703125
Rank 0, Epoch 3, Batch 119, Loss: 515.294617
Gradient norm: 1298.5113525390625
Rank 0, Epoch 3, Batch 120, Loss: 665.363770
Gradient norm: 1249.0430908203125
Rank 0, Epoch 3, Batch 121, Loss: 903.325195
Gradient norm: 1101.3741455078125
Rank 0, Epoch 3, Batch 122, Loss: 727.033142
Gradient norm: 1272.6016845703125
Rank 0, Epoch 3, Batch 123, Loss: 1002.034729
Gradient norm: 1163.30126953125
Rank 0, Epoch 3, Batch 124, Loss: 807.507202
Gradient norm: 1250.0655517578125
Rank 0, Epoch 3, Batch 125, Loss: 989.921204
Gradient norm: 1318.2159423828125
Rank 0, Epoch 3, Batch 126, Loss: 817.081787
Gradient norm: 1198.8948974609375
Rank 0, Epoch 3, Batch 127, Loss: 470.229248
Gradient norm: 1231.3529052734375
Rank 0, Epoch 3, Batch 128, Loss: 653.234375
Gradient norm: 1261.05615234375
Rank 0, Epoch 3, Batch 129, Loss: 671.701416
Gradient norm: 1247.5247802734375
Rank 0, Epoch 3, Batch 130, Loss: 775.570312
Gradient norm: 1216.3154296875
Rank 0, Epoch 3, Batch 131, Loss: 745.954895
Gradient norm: 1193.851806640625
Rank 0, Epoch 3, Batch 132, Loss: 746.062256
Gradient norm: 1244.8101806640625
Rank 0, Epoch 3, Batch 133, Loss: 782.154297
Gradient norm: 1275.2705078125
Rank 0, Epoch 3, Batch 134, Loss: 853.061646
Gradient norm: 1346.890625
Rank 0, Epoch 3, Batch 135, Loss: 931.047607
Gradient norm: 1249.533203125
Rank 0, Epoch 3, Batch 136, Loss: 690.582275
Gradient norm: 1167.74072265625
Rank 0, Epoch 3, Batch 137, Loss: 660.177124
Gradient norm: 1241.6397705078125
Rank 0, Epoch 3, Batch 138, Loss: 682.243103
Gradient norm: 1277.1328125
Rank 0, Epoch 3, Batch 139, Loss: 900.988342
Gradient norm: 1225.87109375
Rank 0, Epoch 3, Batch 140, Loss: 711.172852
Gradient norm: 1238.9945068359375
Rank 0, Epoch 3, Batch 141, Loss: 881.021179
Gradient norm: 1327.7396240234375
Rank 0, Epoch 3, Batch 142, Loss: 1076.963989
Gradient norm: 1218.0045166015625
Rank 0, Epoch 3, Batch 143, Loss: 573.124878
Gradient norm: 1316.078125
Rank 0, Epoch 3, Batch 144, Loss: 742.169495
Gradient norm: 1385.0181884765625
Rank 0, Epoch 3, Batch 145, Loss: 556.519897
Gradient norm: 1414.327880859375
Rank 0, Epoch 3, Batch 146, Loss: 720.351929
Gradient norm: 1333.3970947265625
Rank 0, Epoch 3, Batch 147, Loss: 833.418030
Gradient norm: 1321.2208251953125
Rank 0, Epoch 3, Batch 148, Loss: 767.555420
Gradient norm: 1092.5286865234375
Rank 0, Epoch 3, Batch 149, Loss: 1019.462402
Gradient norm: 1357.4940185546875
Rank 0, Epoch 3, Batch 150, Loss: 947.788208
Gradient norm: 1432.406005859375
Rank 0, Epoch 3, Batch 151, Loss: 1102.731934
Gradient norm: 1334.149658203125
Rank 0, Epoch 3, Batch 152, Loss: 743.200439
Gradient norm: 1100.2783203125
Rank 0, Epoch 3, Batch 153, Loss: 565.399170
Gradient norm: 1282.8863525390625
Rank 0, Epoch 3, Batch 154, Loss: 935.974487
Gradient norm: 1399.5303955078125
Rank 0, Epoch 3, Batch 155, Loss: 1252.557617
Gradient norm: 1145.06689453125
Rank 0, Epoch 3, Batch 156, Loss: 692.775452
Gradient norm: 1337.26806640625
Rank 0, Epoch 3, Batch 157, Loss: 716.734253
Gradient norm: 1376.1727294921875
Rank 0, Epoch 3, Batch 158, Loss: 875.444031
Gradient norm: 1404.724365234375
Rank 0, Epoch 3, Batch 159, Loss: 438.449280
Gradient norm: 1327.5206298828125
Rank 0, Epoch 3, Batch 160, Loss: 761.327148
Gradient norm: 1109.4083251953125
Rank 0, Epoch 3, Batch 161, Loss: 921.750854
Gradient norm: 1448.3983154296875
Rank 0, Epoch 3, Batch 162, Loss: 1018.051941
Gradient norm: 1385.680908203125
Rank 0, Epoch 3, Batch 163, Loss: 1084.445923
Gradient norm: 1459.735595703125
Rank 0, Epoch 3, Batch 164, Loss: 904.646973
Gradient norm: 1374.430419921875
Rank 0, Epoch 3, Batch 165, Loss: 748.497192
Gradient norm: 1385.161865234375
Rank 0, Epoch 3, Batch 166, Loss: 845.201782
Gradient norm: 1333.595458984375
Rank 0, Epoch 3, Batch 167, Loss: 1090.900146
Gradient norm: 1288.6239013671875
Rank 0, Epoch 3, Batch 168, Loss: 666.762329
Gradient norm: 1384.928466796875
Rank 0, Epoch 3, Batch 169, Loss: 1257.530273
Gradient norm: 1468.8287353515625
Rank 0, Epoch 3, Batch 170, Loss: 1256.572632
Gradient norm: 1415.880126953125
Rank 0, Epoch 3, Batch 171, Loss: 649.536316
Gradient norm: 1461.237548828125
Rank 0, Epoch 3, Batch 172, Loss: 812.138611
Gradient norm: 1328.8448486328125
Rank 0, Epoch 3, Batch 173, Loss: 730.606201
Gradient norm: 1276.0743408203125
Rank 0, Epoch 3, Batch 174, Loss: 976.429688
Gradient norm: 1368.5372314453125
Rank 0, Epoch 3, Batch 175, Loss: 1186.188965
Gradient norm: 1513.391845703125
Rank 0, Epoch 3, Batch 176, Loss: 1331.798828
Gradient norm: 1448.0511474609375
Rank 0, Epoch 3, Batch 177, Loss: 978.088623
Gradient norm: 1410.1070556640625
Rank 0, Epoch 3, Batch 178, Loss: 837.358154
Gradient norm: 1330.4500732421875
Rank 0, Epoch 3, Batch 179, Loss: 496.794434
Gradient norm: 1312.0230712890625
Rank 0, Epoch 3, Batch 180, Loss: 594.621948
Gradient norm: 1257.5489501953125
Rank 0, Epoch 3, Batch 181, Loss: 939.593872
Gradient norm: 1504.920654296875
Rank 0, Epoch 3, Batch 182, Loss: 966.802612
Gradient norm: 1432.99658203125
Rank 0, Epoch 3, Batch 183, Loss: 1007.381836
Gradient norm: 1473.2742919921875
Rank 0, Epoch 3, Batch 184, Loss: 1339.270752
Gradient norm: 1472.9013671875
Rank 0, Epoch 3, Batch 185, Loss: 1263.043945
Gradient norm: 1358.4796142578125
Rank 0, Epoch 3, Batch 186, Loss: 975.372375
Gradient norm: 1460.5494384765625
Rank 0, Epoch 3, Batch 187, Loss: 826.110840
Gradient norm: 1395.1820068359375
Rank 0, Epoch 3, Batch 188, Loss: 983.103088
Gradient norm: 1422.7730712890625
Rank 0, Epoch 3, Batch 189, Loss: 1005.137329
Gradient norm: 1531.2662353515625
Rank 0, Epoch 3, Batch 190, Loss: 1123.116211
Gradient norm: 1461.914306640625
Rank 0, Epoch 3, Batch 191, Loss: 843.339844
Gradient norm: 1546.234130859375
Rank 0, Epoch 3, Batch 192, Loss: 1035.961426
Gradient norm: 1489.408447265625
Rank 0, Epoch 3, Batch 193, Loss: 860.474731
Gradient norm: 1453.8302001953125
Rank 0, Epoch 3, Batch 194, Loss: 810.623413
Gradient norm: 1457.0732421875
Rank 0, Epoch 3, Batch 195, Loss: 965.524292
Gradient norm: 1531.5078125
Rank 0, Epoch 3, Batch 196, Loss: 1449.246216
Gradient norm: 1451.21142578125
Rank 0, Epoch 3, Batch 197, Loss: 1532.457642
Gradient norm: 1532.440673828125
Rank 0, Epoch 3, Batch 198, Loss: 1268.347046
Gradient norm: 1528.710693359375
Rank 0, Epoch 3, Batch 199, Loss: 1290.004395
Gradient norm: 1414.7462158203125
Rank 0, Epoch 3, Batch 200, Loss: 915.453491
Gradient norm: 1502.275146484375
Rank 0, Epoch 3, Batch 201, Loss: 1214.162842
Gradient norm: 1616.0589599609375
Rank 0, Epoch 3, Batch 202, Loss: 835.841309
Gradient norm: 1512.861328125
Rank 0, Epoch 3, Batch 203, Loss: 671.217896
Gradient norm: 1458.048828125
Rank 0, Epoch 3, Batch 204, Loss: 855.601074
Gradient norm: 1529.1785888671875
Rank 0, Epoch 3, Batch 205, Loss: 1122.694092
Gradient norm: 1192.5994873046875
Rank 0, Epoch 3, Batch 206, Loss: 739.835510
Gradient norm: 1615.583984375
Rank 0, Epoch 3, Batch 207, Loss: 1482.146851
Gradient norm: 1554.46826171875
Rank 0, Epoch 3, Batch 208, Loss: 1584.181030
Gradient norm: 1529.13720703125
Rank 0, Epoch 3, Batch 209, Loss: 1227.713623
Gradient norm: 1646.7510986328125
Gradient norm: 2291.00732421875
Rank 1, Epoch 3, Batch 105, Loss: 1798.269287
Gradient norm: 2268.016845703125
Rank 1, Epoch 3, Batch 106, Loss: 3339.240234
Gradient norm: 2133.031982421875
Rank 1, Epoch 3, Batch 107, Loss: 1847.719482
Gradient norm: 1689.9742431640625
Rank 1, Epoch 3, Batch 108, Loss: 1182.078125
Gradient norm: 2193.970947265625
Rank 1, Epoch 3, Batch 109, Loss: 2226.276367
Gradient norm: 2066.822021484375
Rank 1, Epoch 3, Batch 110, Loss: 2209.597656
Gradient norm: 2390.271240234375
Rank 1, Epoch 3, Batch 111, Loss: 2601.968262
Gradient norm: 2252.46875
Rank 1, Epoch 3, Batch 112, Loss: 2517.415039
Gradient norm: 2245.9072265625
Rank 1, Epoch 3, Batch 113, Loss: 2622.616211
Gradient norm: 2386.2470703125
Rank 1, Epoch 3, Batch 114, Loss: 1844.363281
Gradient norm: 2315.341796875
Rank 1, Epoch 3, Batch 115, Loss: 2560.548096
Gradient norm: 2411.3251953125
Rank 1, Epoch 3, Batch 116, Loss: 2186.088379
Gradient norm: 2013.2073974609375
Rank 1, Epoch 3, Batch 117, Loss: 3013.566650
Gradient norm: 2220.880859375
Rank 1, Epoch 3, Batch 118, Loss: 3220.021729
Gradient norm: 2029.371337890625
Rank 1, Epoch 3, Batch 119, Loss: 2282.915039
Gradient norm: 2276.47216796875
Rank 1, Epoch 3, Batch 120, Loss: 2031.467163
Gradient norm: 2170.66015625
Rank 1, Epoch 3, Batch 121, Loss: 1765.407227
Gradient norm: 2456.36376953125
Rank 1, Epoch 3, Batch 122, Loss: 2170.372559
Gradient norm: 2268.17919921875
Rank 1, Epoch 3, Batch 123, Loss: 2641.527832
Gradient norm: 2055.6669921875
Rank 1, Epoch 3, Batch 124, Loss: 2151.777832
Gradient norm: 2209.993408203125
Rank 1, Epoch 3, Batch 125, Loss: 2648.637939
Gradient norm: 2372.033447265625
Rank 1, Epoch 3, Batch 126, Loss: 2494.068359
Gradient norm: 2179.341064453125
Rank 1, Epoch 3, Batch 127, Loss: 3128.367432
Gradient norm: 2237.66552734375
Rank 1, Epoch 3, Batch 128, Loss: 2878.397949
Gradient norm: 2395.760009765625
Rank 1, Epoch 3, Batch 129, Loss: 2340.414062
Gradient norm: 2029.732421875
Rank 1, Epoch 3, Batch 130, Loss: 2057.227051
Gradient norm: 2370.765625
Rank 1, Epoch 3, Batch 131, Loss: 2258.465576
Gradient norm: 2282.240966796875
Rank 1, Epoch 3, Batch 132, Loss: 2876.905762
Gradient norm: 2250.55322265625
Rank 1, Epoch 3, Batch 133, Loss: 2522.979248
Gradient norm: 2541.2138671875
Rank 1, Epoch 3, Batch 134, Loss: 2819.166016
Gradient norm: 2296.166259765625
Rank 1, Epoch 3, Batch 135, Loss: 2193.618652
Gradient norm: 2393.906005859375
Rank 1, Epoch 3, Batch 136, Loss: 2609.798584
Gradient norm: 2418.490234375
Rank 1, Epoch 3, Batch 137, Loss: 2919.826660
Gradient norm: 2057.243408203125
Rank 1, Epoch 3, Batch 138, Loss: 2639.895508
Gradient norm: 2270.08447265625
Rank 1, Epoch 3, Batch 139, Loss: 3292.125977
Gradient norm: 2270.852294921875
Rank 1, Epoch 3, Batch 140, Loss: 2747.688477
Gradient norm: 2555.306640625
Rank 1, Epoch 3, Batch 141, Loss: 2477.754150
Gradient norm: 2351.91064453125
Rank 1, Epoch 3, Batch 142, Loss: 2402.278809
Gradient norm: 2339.23828125
Rank 1, Epoch 3, Batch 143, Loss: 2061.365234
Gradient norm: 2424.674560546875
Rank 1, Epoch 3, Batch 144, Loss: 2233.034424
Gradient norm: 2208.712158203125
Rank 1, Epoch 3, Batch 145, Loss: 2299.138672
Gradient norm: 2559.4462890625
Rank 1, Epoch 3, Batch 146, Loss: 2598.983154
Gradient norm: 2062.031494140625
Rank 1, Epoch 3, Batch 147, Loss: 2751.494873
Gradient norm: 2228.380859375
Rank 1, Epoch 3, Batch 148, Loss: 3029.309570
Gradient norm: 2510.974609375
Rank 1, Epoch 3, Batch 149, Loss: 4617.223145
Gradient norm: 2414.955078125
Rank 1, Epoch 3, Batch 150, Loss: 2378.621338
Gradient norm: 2494.061767578125
Rank 1, Epoch 3, Batch 151, Loss: 2342.474121
Gradient norm: 2333.154296875
Rank 1, Epoch 3, Batch 152, Loss: 1808.251221
Gradient norm: 2623.129150390625
Rank 1, Epoch 3, Batch 153, Loss: 3133.042480
Gradient norm: 2303.379638671875
Rank 1, Epoch 3, Batch 154, Loss: 3352.877686
Gradient norm: 2363.8203125
Rank 1, Epoch 3, Batch 155, Loss: 3365.799072
Gradient norm: 2448.353515625
Rank 1, Epoch 3, Batch 156, Loss: 2703.794434
Gradient norm: 2520.609130859375
Rank 1, Epoch 3, Batch 157, Loss: 2883.365723
Gradient norm: 2461.744384765625
Rank 1, Epoch 3, Batch 158, Loss: 2640.129883
Gradient norm: 2411.792724609375
Rank 1, Epoch 3, Batch 159, Loss: 3286.815186
Gradient norm: 2251.533203125
Rank 1, Epoch 3, Batch 160, Loss: 2095.854492
Gradient norm: 2501.218994140625
Rank 1, Epoch 3, Batch 161, Loss: 2913.731934
Gradient norm: 2477.15625
Rank 1, Epoch 3, Batch 162, Loss: 3166.649414
Gradient norm: 2593.871337890625
Rank 1, Epoch 3, Batch 163, Loss: 3034.213135
Gradient norm: 2585.51904296875
Rank 1, Epoch 3, Batch 164, Loss: 3136.642822
Gradient norm: 2295.8359375
Rank 1, Epoch 3, Batch 165, Loss: 3034.786133
Gradient norm: 2626.775390625
Rank 1, Epoch 3, Batch 166, Loss: 3832.992920
Gradient norm: 2637.266357421875
Rank 1, Epoch 3, Batch 167, Loss: 2677.012695
Gradient norm: 2249.429443359375
Rank 1, Epoch 3, Batch 168, Loss: 2815.393066
Gradient norm: 2484.431884765625
Rank 1, Epoch 3, Batch 169, Loss: 3248.808838
Gradient norm: 2455.66357421875
Rank 1, Epoch 3, Batch 170, Loss: 2504.941162
Gradient norm: 2543.72509765625
Rank 1, Epoch 3, Batch 171, Loss: 3478.940674
Gradient norm: 2563.85009765625
Rank 1, Epoch 3, Batch 172, Loss: 3863.842285
Gradient norm: 2422.02099609375
Rank 1, Epoch 3, Batch 173, Loss: 1780.107422
Gradient norm: 2655.1484375
Rank 1, Epoch 3, Batch 174, Loss: 3039.595703
Gradient norm: 2618.577392578125
Rank 1, Epoch 3, Batch 175, Loss: 3469.768066
Gradient norm: 2526.890869140625
Rank 1, Epoch 3, Batch 176, Loss: 3614.852051
Gradient norm: 2528.533447265625
Rank 1, Epoch 3, Batch 177, Loss: 2842.714844
Gradient norm: 2404.872314453125
Rank 1, Epoch 3, Batch 178, Loss: 3265.312256
Gradient norm: 2603.893798828125
Rank 1, Epoch 3, Batch 179, Loss: 3628.057617
Gradient norm: 2652.30322265625
Rank 1, Epoch 3, Batch 180, Loss: 2736.473877
Gradient norm: 2587.87841796875
Rank 1, Epoch 3, Batch 181, Loss: 1922.221191
Gradient norm: 2604.94091796875
Rank 1, Epoch 3, Batch 182, Loss: 3477.121094
Gradient norm: 2364.7587890625
Rank 1, Epoch 3, Batch 183, Loss: 3076.908691
Gradient norm: 2743.171630859375
Rank 1, Epoch 3, Batch 184, Loss: 3806.797607
Gradient norm: 2652.347412109375
Rank 1, Epoch 3, Batch 185, Loss: 2883.361084
Gradient norm: 2730.3935546875
Rank 1, Epoch 3, Batch 186, Loss: 3607.365234
Gradient norm: 2810.69384765625
Rank 1, Epoch 3, Batch 187, Loss: 3316.078613
Gradient norm: 2594.340576171875
Rank 1, Epoch 3, Batch 188, Loss: 4589.316895
Gradient norm: 2563.45947265625
Rank 1, Epoch 3, Batch 189, Loss: 4183.565918
Gradient norm: 2712.089111328125
Rank 1, Epoch 3, Batch 190, Loss: 3588.343750
Gradient norm: 2124.702880859375
Rank 1, Epoch 3, Batch 191, Loss: 2146.146729
Gradient norm: 2678.880615234375
Rank 1, Epoch 3, Batch 192, Loss: 2998.702881
Gradient norm: 2380.640625
Rank 1, Epoch 3, Batch 193, Loss: 3687.623779
Gradient norm: 2692.674560546875
Rank 1, Epoch 3, Batch 194, Loss: 2452.364258
Gradient norm: 2872.45263671875
Rank 1, Epoch 3, Batch 195, Loss: 2669.062988
Gradient norm: 2464.860107421875
Rank 1, Epoch 3, Batch 196, Loss: 3155.235596
Gradient norm: 2436.1259765625
Rank 1, Epoch 3, Batch 197, Loss: 2392.926025
Gradient norm: 2577.671142578125
Rank 1, Epoch 3, Batch 198, Loss: 4289.991699
Gradient norm: 2768.17724609375
Rank 1, Epoch 3, Batch 199, Loss: 4813.141602
Gradient norm: 2704.415771484375
Rank 1, Epoch 3, Batch 200, Loss: 2831.284180
Gradient norm: 2930.919189453125
Rank 1, Epoch 3, Batch 201, Loss: 3669.299072
Gradient norm: 2923.650390625
Rank 1, Epoch 3, Batch 202, Loss: 3656.709961
Gradient norm: 2688.485595703125
Rank 1, Epoch 3, Batch 203, Loss: 3271.862793
Gradient norm: 2268.909912109375
Rank 1, Epoch 3, Batch 204, Loss: 2986.471191
Gradient norm: 2705.89892578125
Rank 1, Epoch 3, Batch 205, Loss: 3686.996582
Gradient norm: 2352.399169921875
Rank 1, Epoch 3, Batch 206, Loss: 3274.546875
Gradient norm: 2797.254638671875
Rank 1, Epoch 3, Batch 207, Loss: 4527.623047
Gradient norm: 2674.65576171875
Rank 1, Epoch 3, Batch 208, Loss: 4231.698730
Gradient norm: 2708.65673828125
Rank 1, Epoch 3, Batch 209, Loss: 3461.846436
Rank 2, Epoch 3, Batch 106, Loss: 547.448730
Gradient norm: 1127.9564208984375
Rank 2, Epoch 3, Batch 107, Loss: 388.196136
Gradient norm: 1165.555419921875
Rank 2, Epoch 3, Batch 108, Loss: 659.899414
Gradient norm: 1214.30859375
Rank 2, Epoch 3, Batch 109, Loss: 731.520508
Gradient norm: 1223.8558349609375
Rank 2, Epoch 3, Batch 110, Loss: 670.179077
Gradient norm: 1224.0928955078125
Rank 2, Epoch 3, Batch 111, Loss: 637.063721
Gradient norm: 1251.08837890625
Rank 2, Epoch 3, Batch 112, Loss: 855.157837
Gradient norm: 1217.13623046875
Rank 2, Epoch 3, Batch 113, Loss: 767.267212
Gradient norm: 1164.620849609375
Rank 2, Epoch 3, Batch 114, Loss: 726.533813
Gradient norm: 1253.98095703125
Rank 2, Epoch 3, Batch 115, Loss: 780.432983
Gradient norm: 1301.69189453125
Rank 2, Epoch 3, Batch 116, Loss: 825.982300
Gradient norm: 1147.494873046875
Rank 2, Epoch 3, Batch 117, Loss: 525.591431
Gradient norm: 1219.3955078125
Rank 2, Epoch 3, Batch 118, Loss: 552.910645
Gradient norm: 1249.5035400390625
Rank 2, Epoch 3, Batch 119, Loss: 598.635254
Gradient norm: 1232.99951171875
Rank 2, Epoch 3, Batch 120, Loss: 534.566040
Gradient norm: 1073.27099609375
Rank 2, Epoch 3, Batch 121, Loss: 812.933472
Gradient norm: 1235.2315673828125
Rank 2, Epoch 3, Batch 122, Loss: 1015.278625
Gradient norm: 1249.3297119140625
Rank 2, Epoch 3, Batch 123, Loss: 916.936768
Gradient norm: 1263.216796875
Rank 2, Epoch 3, Batch 124, Loss: 783.511292
Gradient norm: 1199.9962158203125
Rank 2, Epoch 3, Batch 125, Loss: 747.887695
Gradient norm: 1027.7685546875
Rank 2, Epoch 3, Batch 126, Loss: 594.293762
Gradient norm: 1300.4061279296875
Rank 2, Epoch 3, Batch 127, Loss: 472.932190
Gradient norm: 1270.6883544921875
Rank 2, Epoch 3, Batch 128, Loss: 777.054443
Gradient norm: 1190.252197265625
Rank 2, Epoch 3, Batch 129, Loss: 724.603027
Gradient norm: 1282.58203125
Rank 2, Epoch 3, Batch 130, Loss: 571.529236
Gradient norm: 1286.644775390625
Rank 2, Epoch 3, Batch 131, Loss: 602.394287
Gradient norm: 1281.97509765625
Rank 2, Epoch 3, Batch 132, Loss: 742.398315
Gradient norm: 1319.38232421875
Rank 2, Epoch 3, Batch 133, Loss: 718.894287
Gradient norm: 1265.4859619140625
Rank 2, Epoch 3, Batch 134, Loss: 1015.918213
Gradient norm: 1255.823974609375
Rank 2, Epoch 3, Batch 135, Loss: 1091.449463
Gradient norm: 1237.1754150390625
Rank 2, Epoch 3, Batch 136, Loss: 1128.042236
Gradient norm: 1169.27099609375
Rank 2, Epoch 3, Batch 137, Loss: 744.922607
Gradient norm: 1255.5010986328125
Rank 2, Epoch 3, Batch 138, Loss: 534.703613
Gradient norm: 1372.07421875
Rank 2, Epoch 3, Batch 139, Loss: 629.421509
Gradient norm: 1263.942626953125
Rank 2, Epoch 3, Batch 140, Loss: 500.146484
Gradient norm: 1131.819091796875
Rank 2, Epoch 3, Batch 141, Loss: 611.460510
Gradient norm: 1237.22900390625
Rank 2, Epoch 3, Batch 142, Loss: 789.180542
Gradient norm: 1208.3330078125
Rank 2, Epoch 3, Batch 143, Loss: 896.377014
Gradient norm: 1252.40625
Rank 2, Epoch 3, Batch 144, Loss: 968.681274
Gradient norm: 1337.0091552734375
Rank 2, Epoch 3, Batch 145, Loss: 839.596008
Gradient norm: 1290.1016845703125
Rank 2, Epoch 3, Batch 146, Loss: 837.669189
Gradient norm: 1336.6112060546875
Rank 2, Epoch 3, Batch 147, Loss: 1078.427979
Gradient norm: 1409.9190673828125
Rank 2, Epoch 3, Batch 148, Loss: 879.516724
Gradient norm: 1316.804931640625
Rank 2, Epoch 3, Batch 149, Loss: 526.038696
Gradient norm: 1252.477783203125
Rank 2, Epoch 3, Batch 150, Loss: 535.396912
Gradient norm: 1427.702392578125
Rank 2, Epoch 3, Batch 151, Loss: 1060.663330
Gradient norm: 1201.6248779296875
Rank 2, Epoch 3, Batch 152, Loss: 767.212463
Gradient norm: 1281.4654541015625
Rank 2, Epoch 3, Batch 153, Loss: 1010.581909
Gradient norm: 1376.64453125
Rank 2, Epoch 3, Batch 154, Loss: 1018.023438
Gradient norm: 1283.12255859375
Rank 2, Epoch 3, Batch 155, Loss: 818.895508
Gradient norm: 1258.9464111328125
Rank 2, Epoch 3, Batch 156, Loss: 956.002625
Gradient norm: 1339.40185546875
Rank 2, Epoch 3, Batch 157, Loss: 729.828857
Gradient norm: 1303.6568603515625
Rank 2, Epoch 3, Batch 158, Loss: 805.682617
Gradient norm: 1276.8306884765625
Rank 2, Epoch 3, Batch 159, Loss: 577.097656
Gradient norm: 1261.8067626953125
Rank 2, Epoch 3, Batch 160, Loss: 791.220947
Gradient norm: 1414.5382080078125
Rank 2, Epoch 3, Batch 161, Loss: 938.409302
Gradient norm: 1399.6796875
Rank 2, Epoch 3, Batch 162, Loss: 892.861328
Gradient norm: 1318.241943359375
Rank 2, Epoch 3, Batch 163, Loss: 794.978516
Gradient norm: 1425.9947509765625
Rank 2, Epoch 3, Batch 164, Loss: 1209.843018
Gradient norm: 1452.98291015625
Rank 2, Epoch 3, Batch 165, Loss: 1071.279053
Gradient norm: 1290.0496826171875
Rank 2, Epoch 3, Batch 166, Loss: 1037.580444
Gradient norm: 1202.9661865234375
Rank 2, Epoch 3, Batch 167, Loss: 724.339844
Gradient norm: 1274.6806640625
Rank 2, Epoch 3, Batch 168, Loss: 757.945923
Gradient norm: 1461.281494140625
Rank 2, Epoch 3, Batch 169, Loss: 584.473511
Gradient norm: 1386.7769775390625
Rank 2, Epoch 3, Batch 170, Loss: 997.954834
Gradient norm: 1442.0782470703125
Rank 2, Epoch 3, Batch 171, Loss: 800.781860
Gradient norm: 1427.5194091796875
Rank 2, Epoch 3, Batch 172, Loss: 1343.151611
Gradient norm: 1410.4593505859375
Rank 2, Epoch 3, Batch 173, Loss: 976.187012
Gradient norm: 1338.8416748046875
Rank 2, Epoch 3, Batch 174, Loss: 905.908020
Gradient norm: 1331.7373046875
Rank 2, Epoch 3, Batch 175, Loss: 874.975586
Gradient norm: 1381.2978515625
Rank 2, Epoch 3, Batch 176, Loss: 970.091553
Gradient norm: 1484.1871337890625
Rank 2, Epoch 3, Batch 177, Loss: 1190.982666
Gradient norm: 1431.353759765625
Rank 2, Epoch 3, Batch 178, Loss: 811.286438
Gradient norm: 1384.661865234375
Rank 2, Epoch 3, Batch 179, Loss: 662.316467
Gradient norm: 1445.193603515625
Rank 2, Epoch 3, Batch 180, Loss: 820.303223
Gradient norm: 1400.6646728515625
Rank 2, Epoch 3, Batch 181, Loss: 980.745483
Gradient norm: 1313.5621337890625
Rank 2, Epoch 3, Batch 182, Loss: 887.538574
Gradient norm: 1310.333984375
Rank 2, Epoch 3, Batch 183, Loss: 1186.100342
Gradient norm: 1422.7899169921875
Rank 2, Epoch 3, Batch 184, Loss: 1093.706055
Gradient norm: 1448.6656494140625
Rank 2, Epoch 3, Batch 185, Loss: 1426.578491
Gradient norm: 1340.5677490234375
Rank 2, Epoch 3, Batch 186, Loss: 768.095093
Gradient norm: 1481.032958984375
Rank 2, Epoch 3, Batch 187, Loss: 496.347412
Gradient norm: 1319.291015625
Rank 2, Epoch 3, Batch 188, Loss: 662.302002
Gradient norm: 1419.276611328125
Rank 2, Epoch 3, Batch 189, Loss: 718.789673
Gradient norm: 1238.3780517578125
Rank 2, Epoch 3, Batch 190, Loss: 566.836121
Gradient norm: 1517.2235107421875
Rank 2, Epoch 3, Batch 191, Loss: 840.102661
Gradient norm: 1374.3338623046875
Rank 2, Epoch 3, Batch 192, Loss: 1260.989624
Gradient norm: 1426.757568359375
Rank 2, Epoch 3, Batch 193, Loss: 1260.550537
Gradient norm: 1376.403564453125
Rank 2, Epoch 3, Batch 194, Loss: 1377.171143
Gradient norm: 1441.130859375
Rank 2, Epoch 3, Batch 195, Loss: 1431.781128
Gradient norm: 1521.49658203125
Rank 2, Epoch 3, Batch 196, Loss: 699.050415
Gradient norm: 1579.073486328125
Rank 2, Epoch 3, Batch 197, Loss: 1073.089355
Gradient norm: 1413.242431640625
Rank 2, Epoch 3, Batch 198, Loss: 757.287109
Gradient norm: 1379.4456787109375
Rank 2, Epoch 3, Batch 199, Loss: 1027.687744
Gradient norm: 1501.2703857421875
Rank 2, Epoch 3, Batch 200, Loss: 984.890991
Gradient norm: 1508.4876708984375
Rank 2, Epoch 3, Batch 201, Loss: 1064.970947
Gradient norm: 1606.0489501953125
Rank 2, Epoch 3, Batch 202, Loss: 1124.703491
Gradient norm: 1417.2374267578125
Rank 2, Epoch 3, Batch 203, Loss: 1006.593628
Gradient norm: 1415.039306640625
Rank 2, Epoch 3, Batch 204, Loss: 1175.348511
Gradient norm: 1427.969970703125
Rank 2, Epoch 3, Batch 205, Loss: 1130.685303
Gradient norm: 1565.77734375
Rank 2, Epoch 3, Batch 206, Loss: 1467.396606
Gradient norm: 1297.647705078125
Rank 2, Epoch 3, Batch 207, Loss: 1089.908325
Gradient norm: 1608.6331787109375
Rank 2, Epoch 3, Batch 208, Loss: 936.640381
Gradient norm: 1566.71630859375
Rank 2, Epoch 3, Batch 209, Loss: 751.678284
Gradient norm: 1538.6497802734375
Rank 2, Epoch 3, Batch 210, Loss: 645.248047
Gradient norm: 1594.5997314453125
Rank 2, Epoch 3, Batch 211, Loss: 1155.859131
Gradient norm: 1501.893798828125
Rank 2, Epoch 3, Batch 212, Loss: 1129.577393
Gradient norm: 1466.1776123046875
Rank 2, Epoch 3, Batch 213, Loss: 1220.409912
Gradient norm: 1537.830810546875
Rank 2, Epoch 3, Batch 214, Loss: 1569.179932
Gradient norm: 1549.853759765625
Rank 2, Epoch 3, Batch 215, Loss: 1693.343994
Gradient norm: 1653.1412353515625
Rank 2, Epoch 3, Batch 216, Loss: 1329.519043
Gradient norm: 1565.1494140625
Rank 2, Epoch 3, Batch 217, Loss: 764.183838
Gradient norm: 1474.2403564453125
Rank 2, Epoch 3, Batch 218, Loss: 925.149414
Gradient norm: 1346.9281005859375
Rank 2, Epoch 3, Batch 219, Loss: 884.177124
Gradient norm: 1581.79736328125
Rank 2, Epoch 3, Batch 220, Loss: 1076.993652
Gradient norm: 1457.496337890625
Rank 2, Epoch 3, Batch 221, Loss: 1113.685059
Gradient norm: 1556.3565673828125
Rank 2, Epoch 3, Batch 222, Loss: 1088.364380
Gradient norm: 1556.3516845703125
Rank 2, Epoch 3, Batch 223, Loss: 1264.927612
Gradient norm: 1600.464111328125
Rank 2, Epoch 3, Batch 224, Loss: 1541.685547
Gradient norm: 1618.259521484375
Rank 2, Epoch 3, Batch 225, Loss: 1381.627930
Gradient norm: 1566.626953125
Rank 2, Epoch 3, Batch 226, Loss: 1134.060181
Gradient norm: 1542.19677734375
Rank 2, Epoch 3, Batch 227, Loss: 795.466309
Gradient norm: 1578.2265625
Rank 2, Epoch 3, Batch 228, Loss: 1230.221313
Gradient norm: 1385.1173095703125
Rank 2, Epoch 3, Batch 229, Loss: 965.370483
Gradient norm: 1608.091064453125
Rank 2, Epoch 3, Batch 230, Loss: 1241.188599
Gradient norm: 1635.8397216796875
Rank 2, Epoch 3, Batch 231, Loss: 1333.538330
Gradient norm: 1578.3707275390625
Rank 2, Epoch 3, Batch 232, Loss: 1162.409424
Gradient norm: 1562.71826171875
Rank 2, Epoch 3, Batch 233, Loss: 948.190857
Gradient norm: 1549.687744140625
Rank 2, Epoch 3, Batch 234, Loss: 1177.295654
Gradient norm: 1699.82080078125
Rank 2, Epoch 3, Batch 235, Loss: 1607.373535
Gradient norm: 1478.2255859375
Rank 2, Epoch 3, Batch 236, Loss: 1119.486938
Gradient norm: 1741.45556640625
Rank 2, Epoch 3, Batch 237, Loss: 1670.323730
Gradient norm: 1791.1920166015625
Rank 2, Epoch 3, Batch 238, Loss: 1218.829590
Gradient norm: 1671.4066162109375
Rank 2, Epoch 3, Batch 239, Loss: 1031.920532
Gradient norm: 1686.9481201171875
Rank 2, Epoch 3, Batch 240, Loss: 1155.588257
Gradient norm: 1569.7618408203125
Rank 2, Epoch 3, Batch 241, Loss: 1138.193115
Gradient norm: 1646.1787109375
Rank 2, Epoch 3, Batch 242, Loss: 1393.212524
Gradient norm: 1670.8349609375
Rank 2, Epoch 3, Batch 243, Loss: 1380.655518
Gradient norm: 1557.4520263671875
Rank 2, Epoch 3, Batch 244, Loss: 1576.646484
Gradient norm: 1549.4571533203125
Rank 2, Epoch 3, Batch 245, Loss: 1915.556641
Gradient norm: 1611.906494140625
Rank 2, Epoch 3, Batch 246, Loss: 1402.377075
Gradient norm: 1809.4383544921875
Rank 2, Epoch 3, Batch 247, Loss: 1309.665405
Gradient norm: 1788.476806640625
Rank 2, Epoch 3, Batch 248, Loss: 779.534180
Gradient norm: 1630.865234375
Rank 2, Epoch 3, Batch 249, Loss: 1152.697021
Gradient norm: 1602.82568359375
Rank 2, Epoch 3, Batch 250, Loss: 890.030273
Gradient norm: 1581.156005859375
Rank 2, Epoch 3, Batch 251, Loss: 1278.630493
Gradient norm: 1627.0648193359375
Rank 2, Epoch 3, Batch 252, Loss: 1226.422241
Gradient norm: 1545.4482421875
Rank 2, Epoch 3, Batch 253, Loss: 1635.082031
Gradient norm: 1697.3638916015625
Rank 2, Epoch 3, Batch 254, Loss: 1938.038086
Gradient norm: 1691.9744873046875
Rank 2, Epoch 3, Batch 255, Loss: 1482.512939
Gradient norm: 1678.2117919921875
Rank 2, Epoch 3, Batch 256, Loss: 1301.101685
Gradient norm: 1735.0987548828125
Rank 2, Epoch 3, Batch 257, Loss: 1413.225830
Gradient norm: 1740.3399658203125
Rank 2, Epoch 3, Batch 258, Loss: 1181.547241
Gradient norm: 1750.509033203125
Rank 2, Epoch 3, Batch 259, Loss: 1126.975464
Gradient norm: 1549.35791015625
Rank 2, Epoch 3, Batch 260, Loss: 689.891479
Rank 2, Epoch 3, Val Loss: 329.2522, Val Acc: 0.1000, Time: 208.36s
Gradient norm: 2621.84375
Rank 1, Epoch 3, Batch 210, Loss: 1538.788574
Gradient norm: 2735.4658203125
Rank 1, Epoch 3, Batch 211, Loss: 2659.922852
Gradient norm: 2448.331787109375
Rank 1, Epoch 3, Batch 212, Loss: 2193.599854
Gradient norm: 2539.44189453125
Rank 1, Epoch 3, Batch 213, Loss: 3018.951172
Gradient norm: 2692.373779296875
Rank 1, Epoch 3, Batch 214, Loss: 4335.028809
Gradient norm: 2765.39208984375
Rank 1, Epoch 3, Batch 215, Loss: 4888.178711
Gradient norm: 2940.78125
Rank 1, Epoch 3, Batch 216, Loss: 4802.517578
Gradient norm: 2915.6728515625
Rank 1, Epoch 3, Batch 217, Loss: 4424.070312
Gradient norm: 2891.982421875
Rank 1, Epoch 3, Batch 218, Loss: 3741.730469
Gradient norm: 2759.1591796875
Rank 1, Epoch 3, Batch 219, Loss: 3270.148926
Gradient norm: 2579.422119140625
Rank 1, Epoch 3, Batch 220, Loss: 2378.680420
Gradient norm: 2350.889892578125
Rank 1, Epoch 3, Batch 221, Loss: 3515.011719
Gradient norm: 2934.9248046875
Rank 1, Epoch 3, Batch 222, Loss: 3633.520508
Gradient norm: 2777.64306640625
Rank 1, Epoch 3, Batch 223, Loss: 3138.340332
Gradient norm: 2887.4990234375
Rank 1, Epoch 3, Batch 224, Loss: 3895.779297
Gradient norm: 2797.81494140625
Rank 1, Epoch 3, Batch 225, Loss: 3632.907959
Gradient norm: 2782.078369140625
Rank 1, Epoch 3, Batch 226, Loss: 3331.552246
Gradient norm: 2991.650390625
Rank 1, Epoch 3, Batch 227, Loss: 5243.644043
Gradient norm: 3058.006591796875
Rank 1, Epoch 3, Batch 228, Loss: 6719.715332
Gradient norm: 2733.763916015625
Rank 1, Epoch 3, Batch 229, Loss: 4031.335205
Gradient norm: 2695.442138671875
Rank 1, Epoch 3, Batch 230, Loss: 2741.066895
Gradient norm: 2832.8544921875
Rank 1, Epoch 3, Batch 231, Loss: 2567.822266
Gradient norm: 2908.81396484375
Rank 1, Epoch 3, Batch 232, Loss: 3256.422852
Gradient norm: 3123.796630859375
Rank 1, Epoch 3, Batch 233, Loss: 4113.299316
Gradient norm: 2804.27978515625
Rank 1, Epoch 3, Batch 234, Loss: 3522.900391
Gradient norm: 2867.504638671875
Rank 1, Epoch 3, Batch 235, Loss: 4523.964355
Gradient norm: 3050.7841796875
Rank 1, Epoch 3, Batch 236, Loss: 3935.244141
Gradient norm: 2819.608642578125
Rank 1, Epoch 3, Batch 237, Loss: 4464.070312
Gradient norm: 2795.950927734375
Rank 1, Epoch 3, Batch 238, Loss: 5587.813477
Gradient norm: 2936.63623046875
Rank 1, Epoch 3, Batch 239, Loss: 5524.896484
Gradient norm: 2726.24072265625
Rank 1, Epoch 3, Batch 240, Loss: 3843.993652
Gradient norm: 2824.42236328125
Rank 1, Epoch 3, Batch 241, Loss: 2627.289551
Gradient norm: 2948.89794921875
Rank 1, Epoch 3, Batch 242, Loss: 2608.566406
Gradient norm: 2892.2783203125
Rank 1, Epoch 3, Batch 243, Loss: 2776.997070
Gradient norm: 2808.654296875
Rank 1, Epoch 3, Batch 244, Loss: 4467.041016
Gradient norm: 2903.599853515625
Rank 1, Epoch 3, Batch 245, Loss: 5320.442383
Gradient norm: 2977.2314453125
Rank 1, Epoch 3, Batch 246, Loss: 5076.016602
Gradient norm: 3165.3251953125
Rank 1, Epoch 3, Batch 247, Loss: 2933.784912
Gradient norm: 3089.12158203125
Rank 1, Epoch 3, Batch 248, Loss: 4675.375977
Gradient norm: 2969.033935546875
Rank 1, Epoch 3, Batch 249, Loss: 3204.214844
Gradient norm: 2195.812744140625
Rank 1, Epoch 3, Batch 250, Loss: 2646.207031
Gradient norm: 2781.844970703125
Rank 1, Epoch 3, Batch 251, Loss: 4415.937500
Gradient norm: 2870.593505859375
Rank 1, Epoch 3, Batch 252, Loss: 5791.799805
Gradient norm: 3073.99169921875
Rank 1, Epoch 3, Batch 253, Loss: 3872.228760
Gradient norm: 3231.593505859375
Rank 1, Epoch 3, Batch 254, Loss: 5249.043945
Gradient norm: 3021.282958984375
Rank 1, Epoch 3, Batch 255, Loss: 3677.941162
Gradient norm: 3060.293212890625
Rank 1, Epoch 3, Batch 256, Loss: 4083.813965
Gradient norm: 2920.678466796875
Rank 1, Epoch 3, Batch 257, Loss: 4310.726562
Gradient norm: 3016.234619140625
Rank 1, Epoch 3, Batch 258, Loss: 4948.298828
Gradient norm: 2757.318359375
Rank 1, Epoch 3, Batch 259, Loss: 3751.733887
Gradient norm: 3246.20263671875
Rank 1, Epoch 3, Batch 260, Loss: 5147.708496
Rank 1, Epoch 3, Val Loss: 63278.0546, Val Acc: 0.1000, Time: 214.11s
Rank 0, Epoch 3, Batch 210, Loss: 1103.374512
Gradient norm: 1637.6221923828125
Rank 0, Epoch 3, Batch 211, Loss: 1124.566162
Gradient norm: 1497.6270751953125
Rank 0, Epoch 3, Batch 212, Loss: 1178.569580
Gradient norm: 1318.5010986328125
Rank 0, Epoch 3, Batch 213, Loss: 1067.139404
Gradient norm: 1368.6973876953125
Rank 0, Epoch 3, Batch 214, Loss: 744.405396
Gradient norm: 1602.431396484375
Rank 0, Epoch 3, Batch 215, Loss: 964.640137
Gradient norm: 1350.238037109375
Rank 0, Epoch 3, Batch 216, Loss: 942.650818
Gradient norm: 1500.2706298828125
Rank 0, Epoch 3, Batch 217, Loss: 1352.925781
Gradient norm: 1310.7747802734375
Rank 0, Epoch 3, Batch 218, Loss: 979.590271
Gradient norm: 1512.714111328125
Rank 0, Epoch 3, Batch 219, Loss: 1271.273804
Gradient norm: 1495.59228515625
Rank 0, Epoch 3, Batch 220, Loss: 1081.616455
Gradient norm: 1458.328125
Rank 0, Epoch 3, Batch 221, Loss: 864.243896
Gradient norm: 1570.68603515625
Rank 0, Epoch 3, Batch 222, Loss: 1332.121582
Gradient norm: 1508.3375244140625
Rank 0, Epoch 3, Batch 223, Loss: 1035.652100
Gradient norm: 1655.795166015625
Rank 0, Epoch 3, Batch 224, Loss: 984.343262
Gradient norm: 1557.273681640625
Rank 0, Epoch 3, Batch 225, Loss: 1005.116577
Gradient norm: 1528.80419921875
Rank 0, Epoch 3, Batch 226, Loss: 1260.367188
Gradient norm: 1536.777099609375
Rank 0, Epoch 3, Batch 227, Loss: 943.245605
Gradient norm: 1556.22900390625
Rank 0, Epoch 3, Batch 228, Loss: 1123.677612
Gradient norm: 1455.02197265625
Rank 0, Epoch 3, Batch 229, Loss: 1203.140625
Gradient norm: 1304.8907470703125
Rank 0, Epoch 3, Batch 230, Loss: 1192.177124
Gradient norm: 1530.17578125
Rank 0, Epoch 3, Batch 231, Loss: 1484.448120
Gradient norm: 1602.7652587890625
Rank 0, Epoch 3, Batch 232, Loss: 1187.381348
Gradient norm: 1516.2415771484375
Rank 0, Epoch 3, Batch 233, Loss: 1234.098633
Gradient norm: 1401.1533203125
Rank 0, Epoch 3, Batch 234, Loss: 789.443726
Gradient norm: 1661.281005859375
Rank 0, Epoch 3, Batch 235, Loss: 727.038696
Gradient norm: 1464.3897705078125
Rank 0, Epoch 3, Batch 236, Loss: 878.095886
Gradient norm: 1725.515380859375
Rank 0, Epoch 3, Batch 237, Loss: 1143.554565
Gradient norm: 1568.713623046875
Rank 0, Epoch 3, Batch 238, Loss: 1340.011230
Gradient norm: 1677.5528564453125
Rank 0, Epoch 3, Batch 239, Loss: 1436.549561
Gradient norm: 1721.71044921875
Rank 0, Epoch 3, Batch 240, Loss: 1884.951782
Gradient norm: 1575.429443359375
Rank 0, Epoch 3, Batch 241, Loss: 1338.376709
Gradient norm: 1671.9697265625
Rank 0, Epoch 3, Batch 242, Loss: 1575.631958
Gradient norm: 1692.7947998046875
Rank 0, Epoch 3, Batch 243, Loss: 1157.725586
Gradient norm: 1701.2625732421875
Rank 0, Epoch 3, Batch 244, Loss: 1211.797241
Gradient norm: 1543.6490478515625
Rank 0, Epoch 3, Batch 245, Loss: 1128.863525
Gradient norm: 1683.4176025390625
Rank 0, Epoch 3, Batch 246, Loss: 1129.929932
Gradient norm: 1565.33740234375
Rank 0, Epoch 3, Batch 247, Loss: 1192.230469
Gradient norm: 1563.8984375
Rank 0, Epoch 3, Batch 248, Loss: 1228.046387
Gradient norm: 1621.408203125
Rank 0, Epoch 3, Batch 249, Loss: 1428.944824
Gradient norm: 1424.102783203125
Rank 0, Epoch 3, Batch 250, Loss: 1847.600952
Gradient norm: 1486.2886962890625
Rank 0, Epoch 3, Batch 251, Loss: 1203.426514
Gradient norm: 1649.0994873046875
Rank 0, Epoch 3, Batch 252, Loss: 584.192627
Gradient norm: 1582.8759765625
Rank 0, Epoch 3, Batch 253, Loss: 676.110535
Gradient norm: 1731.701171875
Rank 0, Epoch 3, Batch 254, Loss: 682.690430
Gradient norm: 1518.228271484375
Rank 0, Epoch 3, Batch 255, Loss: 1404.464111
Gradient norm: 1730.2100830078125
Rank 0, Epoch 3, Batch 256, Loss: 1562.747070
Gradient norm: 1684.095947265625
Rank 0, Epoch 3, Batch 257, Loss: 1549.458740
Gradient norm: 1611.01416015625
Rank 0, Epoch 3, Batch 258, Loss: 1839.157715
Gradient norm: 1678.424072265625
Rank 0, Epoch 3, Batch 259, Loss: 1286.223877
Gradient norm: 1606.6458740234375
Rank 0, Epoch 3, Batch 260, Loss: 1412.461792
Rank 0, Epoch 3, Val Loss: 444.8557, Val Acc: 0.1000, Time: 211.43s
Gradient norm: 1667.0130615234375
Rank 2, Epoch 4, Batch 0, Loss: 1138.289062
Gradient norm: 1591.309814453125
Rank 2, Epoch 4, Batch 1, Loss: 1503.827881
Gradient norm: 1699.112548828125
Rank 2, Epoch 4, Batch 2, Loss: 2068.211182
Gradient norm: 1683.868408203125
Rank 2, Epoch 4, Batch 3, Loss: 1450.172119
Gradient norm: 1653.3907470703125
Rank 2, Epoch 4, Batch 4, Loss: 2028.466431
Gradient norm: 1688.099365234375
Rank 2, Epoch 4, Batch 5, Loss: 1522.479980
Gradient norm: 1684.939453125
Rank 2, Epoch 4, Batch 6, Loss: 710.868347
Gradient norm: 1625.8817138671875
Rank 2, Epoch 4, Batch 7, Loss: 914.609436
Gradient norm: 1698.638671875
Rank 2, Epoch 4, Batch 8, Loss: 1338.485840
Gradient norm: 1683.399658203125
Rank 2, Epoch 4, Batch 9, Loss: 1068.570068
Gradient norm: 1750.4371337890625
Rank 2, Epoch 4, Batch 10, Loss: 1959.769531
Gradient norm: 1871.75048828125
Rank 2, Epoch 4, Batch 11, Loss: 2044.852905
Gradient norm: 1326.0574951171875
Rank 2, Epoch 4, Batch 12, Loss: 1155.698486
Gradient norm: 1858.161376953125
Rank 2, Epoch 4, Batch 13, Loss: 1260.238037
Gradient norm: 1618.81689453125
Rank 2, Epoch 4, Batch 14, Loss: 968.846436
Gradient norm: 1699.6549072265625
Rank 2, Epoch 4, Batch 15, Loss: 1865.646973
Gradient norm: 1780.947021484375
Rank 2, Epoch 4, Batch 16, Loss: 1048.209717
Gradient norm: 1701.3516845703125
Rank 2, Epoch 4, Batch 17, Loss: 1229.947266
Gradient norm: 1717.9139404296875
Rank 2, Epoch 4, Batch 18, Loss: 1737.939331
Gradient norm: 1712.4130859375
Rank 2, Epoch 4, Batch 19, Loss: 1612.112671
Gradient norm: 1897.589599609375
Rank 2, Epoch 4, Batch 20, Loss: 1478.885986
Gradient norm: 1744.253173828125
Rank 2, Epoch 4, Batch 21, Loss: 1121.145630
Gradient norm: 1783.54541015625
Rank 2, Epoch 4, Batch 22, Loss: 1325.826538
Gradient norm: 1651.9796142578125
Rank 2, Epoch 4, Batch 23, Loss: 1433.890625
Gradient norm: 1756.156005859375
Rank 2, Epoch 4, Batch 24, Loss: 1964.106079
Gradient norm: 1888.9239501953125
Rank 2, Epoch 4, Batch 25, Loss: 2174.891602
Gradient norm: 1736.804931640625
Rank 2, Epoch 4, Batch 26, Loss: 1321.270020
Gradient norm: 1725.7725830078125
Rank 2, Epoch 4, Batch 27, Loss: 1562.619385
Gradient norm: 1838.653076171875
Rank 2, Epoch 4, Batch 28, Loss: 1381.562988
Gradient norm: 1619.045654296875
Rank 2, Epoch 4, Batch 29, Loss: 1142.235840
Gradient norm: 1760.509033203125
Rank 2, Epoch 4, Batch 30, Loss: 1631.593018
Gradient norm: 1954.004150390625
Rank 2, Epoch 4, Batch 31, Loss: 1268.030151
Gradient norm: 1721.6695556640625
Rank 2, Epoch 4, Batch 32, Loss: 1150.905273
Gradient norm: 1689.727294921875
Rank 2, Epoch 4, Batch 33, Loss: 1126.399170
Gradient norm: 1900.7049560546875
Rank 2, Epoch 4, Batch 34, Loss: 1404.550293
Gradient norm: 1781.9029541015625
Rank 2, Epoch 4, Batch 35, Loss: 2086.439453
Gradient norm: 1663.0760498046875
Rank 2, Epoch 4, Batch 36, Loss: 1510.683350
Gradient norm: 1866.9407958984375
Rank 2, Epoch 4, Batch 37, Loss: 2652.015625
Gradient norm: 1911.7615966796875
Rank 2, Epoch 4, Batch 38, Loss: 2319.382324
Gradient norm: 1883.9287109375
Rank 2, Epoch 4, Batch 39, Loss: 948.807617
Gradient norm: 1857.031494140625
Rank 2, Epoch 4, Batch 40, Loss: 1640.352661
Gradient norm: 1819.219970703125
Rank 2, Epoch 4, Batch 41, Loss: 1274.414795
Gradient norm: 1821.294921875
Rank 2, Epoch 4, Batch 42, Loss: 909.063904
Gradient norm: 1688.28466796875
Rank 2, Epoch 4, Batch 43, Loss: 1561.984131
Gradient norm: 1856.7615966796875
Rank 2, Epoch 4, Batch 44, Loss: 2665.057617
Gradient norm: 1863.3101806640625
Rank 2, Epoch 4, Batch 45, Loss: 1559.829346
Gradient norm: 1798.890380859375
Rank 2, Epoch 4, Batch 46, Loss: 1645.149170
Gradient norm: 1772.389892578125
Rank 2, Epoch 4, Batch 47, Loss: 1698.054688
Gradient norm: 1781.2672119140625
Rank 2, Epoch 4, Batch 48, Loss: 1460.884521
Gradient norm: 1867.876220703125
Rank 2, Epoch 4, Batch 49, Loss: 1347.660034
Gradient norm: 1693.599609375
Rank 2, Epoch 4, Batch 50, Loss: 1006.565308
Gradient norm: 1642.802001953125
Rank 2, Epoch 4, Batch 51, Loss: 1290.808594
Gradient norm: 1936.7862548828125
Rank 2, Epoch 4, Batch 52, Loss: 1807.691406
Gradient norm: 1786.1246337890625
Rank 2, Epoch 4, Batch 53, Loss: 1732.570312
Gradient norm: 1788.819580078125
Rank 2, Epoch 4, Batch 54, Loss: 2098.981689
Gradient norm: 1714.3529052734375
Rank 2, Epoch 4, Batch 55, Loss: 1885.572510
Gradient norm: 1646.7744140625
Rank 2, Epoch 4, Batch 56, Loss: 930.347168
Gradient norm: 1873.638916015625
Rank 2, Epoch 4, Batch 57, Loss: 1579.799561
Gradient norm: 1809.9310302734375
Rank 2, Epoch 4, Batch 58, Loss: 1373.533447
Gradient norm: 2015.4739990234375
Rank 2, Epoch 4, Batch 59, Loss: 1831.582520
Gradient norm: 1954.2637939453125
Rank 2, Epoch 4, Batch 60, Loss: 1992.573730
Gradient norm: 1905.2900390625
Rank 2, Epoch 4, Batch 61, Loss: 1655.539917
Gradient norm: 1996.0091552734375
Rank 2, Epoch 4, Batch 62, Loss: 1531.015137
Gradient norm: 1973.5450439453125
Rank 2, Epoch 4, Batch 63, Loss: 1298.795654
Gradient norm: 1650.615234375
Rank 2, Epoch 4, Batch 64, Loss: 1546.691162
Gradient norm: 2005.676513671875
Rank 2, Epoch 4, Batch 65, Loss: 2167.387207
Gradient norm: 1976.26171875
Rank 2, Epoch 4, Batch 66, Loss: 1704.273193
Gradient norm: 1880.1014404296875
Rank 2, Epoch 4, Batch 67, Loss: 1387.748169
Gradient norm: 1885.2618408203125
Rank 2, Epoch 4, Batch 68, Loss: 2314.466064
Gradient norm: 2078.376220703125
Rank 2, Epoch 4, Batch 69, Loss: 3102.019775
Gradient norm: 1874.6573486328125
Rank 2, Epoch 4, Batch 70, Loss: 1569.623535
Gradient norm: 2055.040283203125
Rank 2, Epoch 4, Batch 71, Loss: 1820.952637
Gradient norm: 1944.1339111328125
Rank 2, Epoch 4, Batch 72, Loss: 1012.169434
Gradient norm: 1937.9644775390625
Rank 2, Epoch 4, Batch 73, Loss: 1652.655884
Gradient norm: 1890.171875
Rank 2, Epoch 4, Batch 74, Loss: 1788.072876
Gradient norm: 2111.287109375
Rank 2, Epoch 4, Batch 75, Loss: 2674.089844
Gradient norm: 1870.2633056640625
Rank 2, Epoch 4, Batch 76, Loss: 2224.955566
Gradient norm: 1937.9154052734375
Rank 2, Epoch 4, Batch 77, Loss: 1986.734497
Gradient norm: 2026.1859130859375
Rank 2, Epoch 4, Batch 78, Loss: 1288.830811
Gradient norm: 1955.26171875
Rank 2, Epoch 4, Batch 79, Loss: 1292.456909
Gradient norm: 1992.821044921875
Rank 2, Epoch 4, Batch 80, Loss: 1721.501587
Gradient norm: 1840.7252197265625
Rank 2, Epoch 4, Batch 81, Loss: 1973.124512
Gradient norm: 1785.0595703125
Rank 2, Epoch 4, Batch 82, Loss: 1866.141113
Gradient norm: 2102.086181640625
Rank 2, Epoch 4, Batch 83, Loss: 2643.310059
Gradient norm: 2055.510009765625
Rank 2, Epoch 4, Batch 84, Loss: 1979.315918
Gradient norm: 2029.6593017578125
Rank 2, Epoch 4, Batch 85, Loss: 1476.066895
Gradient norm: 2006.6158447265625
Rank 2, Epoch 4, Batch 86, Loss: 1525.754639
Gradient norm: 1934.5968017578125
Rank 2, Epoch 4, Batch 87, Loss: 1767.906616
Gradient norm: 2050.203369140625
Rank 2, Epoch 4, Batch 88, Loss: 2609.772705
Gradient norm: 2024.1436767578125
Rank 2, Epoch 4, Batch 89, Loss: 1935.980469
Gradient norm: 2035.0546875
Rank 2, Epoch 4, Batch 90, Loss: 2270.989746
Gradient norm: 1864.6209716796875
Rank 2, Epoch 4, Batch 91, Loss: 1633.752441
Gradient norm: 1989.803955078125
Rank 2, Epoch 4, Batch 92, Loss: 1590.850586
Gradient norm: 1899.4510498046875
Rank 2, Epoch 4, Batch 93, Loss: 2320.668945
Gradient norm: 1981.310546875
Rank 2, Epoch 4, Batch 94, Loss: 1708.067627
Gradient norm: 2132.232421875
Rank 2, Epoch 4, Batch 95, Loss: 1903.836304
Gradient norm: 1932.9647216796875
Rank 2, Epoch 4, Batch 96, Loss: 2198.011719
Gradient norm: 2040.9635009765625
Rank 2, Epoch 4, Batch 97, Loss: 1988.356079
Gradient norm: 2195.507080078125
Rank 2, Epoch 4, Batch 98, Loss: 2039.335449
Gradient norm: 2065.917236328125
Rank 2, Epoch 4, Batch 99, Loss: 2074.541260
Gradient norm: 2118.09130859375
Rank 2, Epoch 4, Batch 100, Loss: 2700.198242
Gradient norm: 2129.19775390625
Rank 2, Epoch 4, Batch 101, Loss: 1674.309204
Gradient norm: 2016.3448486328125
Rank 2, Epoch 4, Batch 102, Loss: 1713.842651
Gradient norm: 2122.998046875
Rank 2, Epoch 4, Batch 103, Loss: 1714.728638
Gradient norm: 2178.9384765625
Rank 2, Epoch 4, Batch 104, Loss: 2385.250977
Gradient norm: 2068.91650390625Gradient norm: 1586.2164306640625
Rank 0, Epoch 4, Batch 0, Loss: 1180.156128
Gradient norm: 1608.8426513671875
Rank 0, Epoch 4, Batch 1, Loss: 1369.396973
Gradient norm: 1629.5811767578125
Rank 0, Epoch 4, Batch 2, Loss: 1480.391968
Gradient norm: 1725.05859375
Rank 0, Epoch 4, Batch 3, Loss: 1400.134644
Gradient norm: 1690.944580078125
Rank 0, Epoch 4, Batch 4, Loss: 1240.097412
Gradient norm: 1645.380615234375
Rank 0, Epoch 4, Batch 5, Loss: 1195.170166
Gradient norm: 1808.1434326171875
Rank 0, Epoch 4, Batch 6, Loss: 1210.185547
Gradient norm: 1733.8106689453125
Rank 0, Epoch 4, Batch 7, Loss: 905.524170
Gradient norm: 1528.529296875
Rank 0, Epoch 4, Batch 8, Loss: 1729.488281
Gradient norm: 1829.005615234375
Rank 0, Epoch 4, Batch 9, Loss: 1864.599365
Gradient norm: 1823.1817626953125
Rank 0, Epoch 4, Batch 10, Loss: 1788.469727
Gradient norm: 1632.2840576171875
Rank 0, Epoch 4, Batch 11, Loss: 1575.324219
Gradient norm: 1752.2398681640625
Rank 0, Epoch 4, Batch 12, Loss: 968.461792
Gradient norm: 1630.2633056640625
Rank 0, Epoch 4, Batch 13, Loss: 799.589355
Gradient norm: 1614.0465087890625
Rank 0, Epoch 4, Batch 14, Loss: 1494.845703
Gradient norm: 1804.12451171875
Rank 0, Epoch 4, Batch 15, Loss: 1558.370850
Gradient norm: 1613.696044921875
Rank 0, Epoch 4, Batch 16, Loss: 1500.578369
Gradient norm: 1569.2513427734375
Rank 0, Epoch 4, Batch 17, Loss: 1747.924805
Gradient norm: 1836.73974609375
Rank 0, Epoch 4, Batch 18, Loss: 1607.526001
Gradient norm: 1791.2840576171875
Rank 0, Epoch 4, Batch 19, Loss: 1769.305054
Gradient norm: 1653.941162109375
Rank 0, Epoch 4, Batch 20, Loss: 643.477905
Gradient norm: 1790.5716552734375
Rank 0, Epoch 4, Batch 21, Loss: 1004.246155
Gradient norm: 1786.0748291015625
Rank 0, Epoch 4, Batch 22, Loss: 859.793213
Gradient norm: 1771.3787841796875
Rank 0, Epoch 4, Batch 23, Loss: 1449.963013
Gradient norm: 1667.9947509765625
Rank 0, Epoch 4, Batch 24, Loss: 1877.391602
Gradient norm: 1757.793212890625
Rank 0, Epoch 4, Batch 25, Loss: 1854.838379
Gradient norm: 1847.903076171875
Rank 0, Epoch 4, Batch 26, Loss: 2204.926758
Gradient norm: 1779.2032470703125
Rank 0, Epoch 4, Batch 27, Loss: 2008.431152
Gradient norm: 1865.5648193359375
Rank 0, Epoch 4, Batch 28, Loss: 1189.328857
Gradient norm: 1797.58642578125
Rank 0, Epoch 4, Batch 29, Loss: 1239.495850
Gradient norm: 1775.0972900390625
Rank 0, Epoch 4, Batch 30, Loss: 1231.110107
Gradient norm: 1052.7537841796875
Rank 0, Epoch 4, Batch 31, Loss: 1537.178833
Gradient norm: 1626.6505126953125
Rank 0, Epoch 4, Batch 32, Loss: 812.398438
Gradient norm: 1806.328125
Rank 0, Epoch 4, Batch 33, Loss: 1430.714844
Gradient norm: 1864.1387939453125
Rank 0, Epoch 4, Batch 34, Loss: 1503.134277
Gradient norm: 1885.30224609375
Rank 0, Epoch 4, Batch 35, Loss: 1602.018555
Gradient norm: 1746.588134765625
Rank 0, Epoch 4, Batch 36, Loss: 1768.328491
Gradient norm: 1630.4468994140625
Rank 0, Epoch 4, Batch 37, Loss: 1450.299072
Gradient norm: 1791.125732421875
Rank 0, Epoch 4, Batch 38, Loss: 1469.984131
Gradient norm: 1840.8878173828125
Rank 0, Epoch 4, Batch 39, Loss: 1580.307373
Gradient norm: 1711.189697265625
Rank 0, Epoch 4, Batch 40, Loss: 1726.063721
Gradient norm: 1837.3389892578125
Rank 0, Epoch 4, Batch 41, Loss: 2004.296387
Gradient norm: 1821.96923828125
Rank 0, Epoch 4, Batch 42, Loss: 1649.724609
Gradient norm: 1515.7122802734375
Rank 0, Epoch 4, Batch 43, Loss: 1159.214111
Gradient norm: 1893.6007080078125
Rank 0, Epoch 4, Batch 44, Loss: 1601.097412
Gradient norm: 1758.8642578125
Rank 0, Epoch 4, Batch 45, Loss: 1515.536255
Gradient norm: 1995.4561767578125
Rank 0, Epoch 4, Batch 46, Loss: 1628.694092
Gradient norm: 1897.1376953125
Rank 0, Epoch 4, Batch 47, Loss: 1139.210083
Gradient norm: 1841.8389892578125
Rank 0, Epoch 4, Batch 48, Loss: 1236.707764
Gradient norm: 1755.718994140625
Rank 0, Epoch 4, Batch 49, Loss: 1308.002930
Gradient norm: 1850.0013427734375
Rank 0, Epoch 4, Batch 50, Loss: 1769.918823
Gradient norm: 1895.597900390625
Rank 0, Epoch 4, Batch 51, Loss: 2203.115723
Gradient norm: 1857.9649658203125
Rank 0, Epoch 4, Batch 52, Loss: 1687.346191
Gradient norm: 1903.55615234375
Rank 0, Epoch 4, Batch 53, Loss: 2583.509521
Gradient norm: 1790.9755859375
Rank 0, Epoch 4, Batch 54, Loss: 2286.699463
Gradient norm: 1899.8447265625
Rank 0, Epoch 4, Batch 55, Loss: 1647.193237
Gradient norm: 2009.2593994140625
Rank 0, Epoch 4, Batch 56, Loss: 808.771667
Gradient norm: 1890.0782470703125
Rank 0, Epoch 4, Batch 57, Loss: 811.903503
Gradient norm: 1797.3651123046875
Rank 0, Epoch 4, Batch 58, Loss: 1110.471924
Gradient norm: 1743.598876953125
Rank 0, Epoch 4, Batch 59, Loss: 1518.681885
Gradient norm: 1883.8172607421875
Rank 0, Epoch 4, Batch 60, Loss: 1863.078979
Gradient norm: 2063.40283203125
Rank 0, Epoch 4, Batch 61, Loss: 2337.502686
Gradient norm: 1916.488525390625
Rank 0, Epoch 4, Batch 62, Loss: 2200.912109
Gradient norm: 1995.7342529296875
Rank 0, Epoch 4, Batch 63, Loss: 2663.099609
Gradient norm: 1895.64111328125
Rank 0, Epoch 4, Batch 64, Loss: 2406.485107
Gradient norm: 2014.2783203125
Rank 0, Epoch 4, Batch 65, Loss: 1601.406616
Gradient norm: 1801.952392578125
Rank 0, Epoch 4, Batch 66, Loss: 1624.195801
Gradient norm: 1781.7808837890625
Rank 0, Epoch 4, Batch 67, Loss: 1588.983398
Gradient norm: 1909.51025390625
Rank 0, Epoch 4, Batch 68, Loss: 1442.590576
Gradient norm: 2034.1351318359375
Rank 0, Epoch 4, Batch 69, Loss: 1643.158936
Gradient norm: 2047.2921142578125
Rank 0, Epoch 4, Batch 70, Loss: 2193.561035
Gradient norm: 1887.744873046875
Rank 0, Epoch 4, Batch 71, Loss: 1468.893799
Gradient norm: 1934.217529296875
Rank 0, Epoch 4, Batch 72, Loss: 1742.800781
Gradient norm: 1790.9952392578125
Rank 0, Epoch 4, Batch 73, Loss: 1269.247070
Gradient norm: 1844.43603515625
Rank 0, Epoch 4, Batch 74, Loss: 1642.203247
Gradient norm: 1862.5147705078125
Rank 0, Epoch 4, Batch 75, Loss: 2533.371094
Gradient norm: 2074.925537109375
Rank 0, Epoch 4, Batch 76, Loss: 2391.813477
Gradient norm: 1944.90625
Rank 0, Epoch 4, Batch 77, Loss: 2043.435181
Gradient norm: 1900.2587890625
Rank 0, Epoch 4, Batch 78, Loss: 1201.607544
Gradient norm: 1433.8173828125
Rank 0, Epoch 4, Batch 79, Loss: 839.429016
Gradient norm: 1867.4212646484375
Rank 0, Epoch 4, Batch 80, Loss: 1447.830078
Gradient norm: 2019.651123046875
Rank 0, Epoch 4, Batch 81, Loss: 1631.632690
Gradient norm: 1761.4520263671875
Rank 0, Epoch 4, Batch 82, Loss: 1690.988159
Gradient norm: 2026.46533203125
Rank 0, Epoch 4, Batch 83, Loss: 1877.747559
Gradient norm: 2015.9373779296875
Rank 0, Epoch 4, Batch 84, Loss: 2351.123291
Gradient norm: 1934.5126953125
Rank 0, Epoch 4, Batch 85, Loss: 2454.030273
Gradient norm: 1913.498046875
Rank 0, Epoch 4, Batch 86, Loss: 1861.635498
Gradient norm: 1873.0184326171875
Rank 0, Epoch 4, Batch 87, Loss: 1276.781860
Gradient norm: 1838.8648681640625
Rank 0, Epoch 4, Batch 88, Loss: 1442.851929
Gradient norm: 1823.087158203125
Rank 0, Epoch 4, Batch 89, Loss: 1556.778076
Gradient norm: 1972.92529296875
Rank 0, Epoch 4, Batch 90, Loss: 1826.277100
Gradient norm: 1996.026123046875
Rank 0, Epoch 4, Batch 91, Loss: 2292.587402
Gradient norm: 1977.7738037109375
Rank 0, Epoch 4, Batch 92, Loss: 1772.817871
Gradient norm: 2167.552490234375
Rank 0, Epoch 4, Batch 93, Loss: 1785.634033
Gradient norm: 1940.333740234375
Rank 0, Epoch 4, Batch 94, Loss: 2189.744629
Gradient norm: 2059.222412109375
Rank 0, Epoch 4, Batch 95, Loss: 1785.309326
Gradient norm: 1630.123779296875
Rank 0, Epoch 4, Batch 96, Loss: 1346.576172
Gradient norm: 1863.1719970703125
Rank 0, Epoch 4, Batch 97, Loss: 1682.570068
Gradient norm: 2094.2744140625
Rank 0, Epoch 4, Batch 98, Loss: 1986.615112
Gradient norm: 1894.6932373046875
Rank 0, Epoch 4, Batch 99, Loss: 2703.727051
Gradient norm: 2040.80126953125
Rank 0, Epoch 4, Batch 100, Loss: 1760.485107
Gradient norm: 1843.565673828125
Rank 0, Epoch 4, Batch 101, Loss: 1379.585449
Gradient norm: 2018.2247314453125
Rank 0, Epoch 4, Batch 102, Loss: 1983.585693
Gradient norm: 2063.46826171875
Rank 0, Epoch 4, Batch 103, Loss: 1647.759033
Gradient norm: 2126.095703125
Rank 0, Epoch 4, Batch 104, Loss: 1601.411011
Gradient norm: 2953.02587890625
Rank 1, Epoch 4, Batch 0, Loss: 4543.915039
Gradient norm: 3244.62109375
Rank 1, Epoch 4, Batch 1, Loss: 4858.933105
Gradient norm: 3078.578125
Rank 1, Epoch 4, Batch 2, Loss: 3688.184570
Gradient norm: 2821.800537109375
Rank 1, Epoch 4, Batch 3, Loss: 3722.489746
Gradient norm: 3127.616455078125
Rank 1, Epoch 4, Batch 4, Loss: 4753.244141
Gradient norm: 2867.119140625
Rank 1, Epoch 4, Batch 5, Loss: 4606.559570
Gradient norm: 3117.989501953125
Rank 1, Epoch 4, Batch 6, Loss: 5186.954102
Gradient norm: 3071.41845703125
Rank 1, Epoch 4, Batch 7, Loss: 4298.060547
Gradient norm: 3169.983642578125
Rank 1, Epoch 4, Batch 8, Loss: 3811.893555
Gradient norm: 3252.5830078125
Rank 1, Epoch 4, Batch 9, Loss: 5322.600586
Gradient norm: 3163.666259765625
Rank 1, Epoch 4, Batch 10, Loss: 5669.236816
Gradient norm: 3301.161865234375
Rank 1, Epoch 4, Batch 11, Loss: 4939.920898
Gradient norm: 3345.0224609375
Rank 1, Epoch 4, Batch 12, Loss: 6131.670410
Gradient norm: 3233.076171875
Rank 1, Epoch 4, Batch 13, Loss: 4750.553711
Gradient norm: 2847.37939453125
Rank 1, Epoch 4, Batch 14, Loss: 4590.714355
Gradient norm: 3326.029052734375
Rank 1, Epoch 4, Batch 15, Loss: 5460.829102
Gradient norm: 3331.88818359375
Rank 1, Epoch 4, Batch 16, Loss: 4425.119141
Gradient norm: 2575.985107421875
Rank 1, Epoch 4, Batch 17, Loss: 4521.937988
Gradient norm: 3214.093505859375
Rank 1, Epoch 4, Batch 18, Loss: 3731.304443
Gradient norm: 2964.746826171875
Rank 1, Epoch 4, Batch 19, Loss: 4707.353516
Gradient norm: 3470.461669921875
Rank 1, Epoch 4, Batch 20, Loss: 4213.923828
Gradient norm: 2761.949462890625
Rank 1, Epoch 4, Batch 21, Loss: 5034.139648
Gradient norm: 2861.0185546875
Rank 1, Epoch 4, Batch 22, Loss: 3366.622070
Gradient norm: 3279.16748046875
Rank 1, Epoch 4, Batch 23, Loss: 5468.033691
Gradient norm: 3339.6220703125
Rank 1, Epoch 4, Batch 24, Loss: 6301.342773
Gradient norm: 3153.48681640625
Rank 1, Epoch 4, Batch 25, Loss: 6529.245117
Gradient norm: 3093.272216796875
Rank 1, Epoch 4, Batch 26, Loss: 2861.990723
Gradient norm: 2916.91162109375
Rank 1, Epoch 4, Batch 27, Loss: 2960.293457
Gradient norm: 3444.22900390625
Rank 1, Epoch 4, Batch 28, Loss: 5074.779785
Gradient norm: 3170.778076171875
Rank 1, Epoch 4, Batch 29, Loss: 4635.042480
Gradient norm: 3207.44482421875
Rank 1, Epoch 4, Batch 30, Loss: 5067.629883
Gradient norm: 3233.322998046875
Rank 1, Epoch 4, Batch 31, Loss: 5858.707520
Gradient norm: 3346.1669921875
Rank 1, Epoch 4, Batch 32, Loss: 6295.415039
Gradient norm: 3561.537353515625
Rank 1, Epoch 4, Batch 33, Loss: 6467.497559
Gradient norm: 2870.37060546875
Rank 1, Epoch 4, Batch 34, Loss: 4346.644043
Gradient norm: 2851.583251953125
Rank 1, Epoch 4, Batch 35, Loss: 4047.823730
Gradient norm: 3221.539306640625
Rank 1, Epoch 4, Batch 36, Loss: 5715.013672
Gradient norm: 3121.286865234375
Rank 1, Epoch 4, Batch 37, Loss: 4136.394043
Gradient norm: 3302.3203125
Rank 1, Epoch 4, Batch 38, Loss: 4589.302734
Gradient norm: 3300.501708984375
Rank 1, Epoch 4, Batch 39, Loss: 5006.618652
Gradient norm: 3087.805908203125
Rank 1, Epoch 4, Batch 40, Loss: 5365.279297
Gradient norm: 3366.671630859375
Rank 1, Epoch 4, Batch 41, Loss: 4950.038086
Gradient norm: 3477.7861328125
Rank 1, Epoch 4, Batch 42, Loss: 5568.174805
Gradient norm: 3397.77783203125
Rank 1, Epoch 4, Batch 43, Loss: 5575.548340
Gradient norm: 3296.869384765625
Rank 1, Epoch 4, Batch 44, Loss: 4203.919922
Gradient norm: 3328.0517578125
Rank 1, Epoch 4, Batch 45, Loss: 5424.764648
Gradient norm: 3529.130859375
Rank 1, Epoch 4, Batch 46, Loss: 4910.187988
Gradient norm: 3437.308349609375
Rank 1, Epoch 4, Batch 47, Loss: 4023.002930
Gradient norm: 3198.3271484375
Rank 1, Epoch 4, Batch 48, Loss: 7932.184570
Gradient norm: 2900.8330078125
Rank 1, Epoch 4, Batch 49, Loss: 4959.072266
Gradient norm: 3374.37353515625
Rank 1, Epoch 4, Batch 50, Loss: 6836.581543
Gradient norm: 3491.44970703125
Rank 1, Epoch 4, Batch 51, Loss: 6213.037109
Gradient norm: 3310.293701171875
Rank 1, Epoch 4, Batch 52, Loss: 4914.372070
Gradient norm: 3002.2314453125
Rank 1, Epoch 4, Batch 53, Loss: 3855.748047
Gradient norm: 3358.20849609375
Rank 1, Epoch 4, Batch 54, Loss: 4689.982910
Gradient norm: 3568.837890625
Rank 1, Epoch 4, Batch 55, Loss: 5134.809570
Gradient norm: 3529.405517578125
Rank 1, Epoch 4, Batch 56, Loss: 5057.461426
Gradient norm: 3536.000732421875
Rank 1, Epoch 4, Batch 57, Loss: 5627.404297
Gradient norm: 3313.979248046875
Rank 1, Epoch 4, Batch 58, Loss: 5782.832031
Gradient norm: 3412.066650390625
Rank 1, Epoch 4, Batch 59, Loss: 6142.054688
Gradient norm: 3324.699462890625
Rank 1, Epoch 4, Batch 60, Loss: 6803.676270
Gradient norm: 3684.389892578125
Rank 1, Epoch 4, Batch 61, Loss: 6066.572754
Gradient norm: 3545.803955078125
Rank 1, Epoch 4, Batch 62, Loss: 6539.940918
Gradient norm: 3264.45166015625
Rank 1, Epoch 4, Batch 63, Loss: 6889.014648
Gradient norm: 3099.83056640625
Rank 1, Epoch 4, Batch 64, Loss: 4254.582031
Gradient norm: 3225.0048828125
Rank 1, Epoch 4, Batch 65, Loss: 3701.217285
Gradient norm: 3661.781005859375
Rank 1, Epoch 4, Batch 66, Loss: 4804.210449
Gradient norm: 3530.156494140625
Rank 1, Epoch 4, Batch 67, Loss: 6068.028809
Gradient norm: 3514.552001953125
Rank 1, Epoch 4, Batch 68, Loss: 6954.182617
Gradient norm: 3613.8056640625
Rank 1, Epoch 4, Batch 69, Loss: 7173.949219
Gradient norm: 3661.79345703125
Rank 1, Epoch 4, Batch 70, Loss: 7921.557129
Gradient norm: 3330.5830078125
Rank 1, Epoch 4, Batch 71, Loss: 5236.234375
Gradient norm: 3518.160400390625
Rank 1, Epoch 4, Batch 72, Loss: 6125.184570
Gradient norm: 3531.961669921875
Rank 1, Epoch 4, Batch 73, Loss: 3753.180176
Gradient norm: 2870.9580078125
Rank 1, Epoch 4, Batch 74, Loss: 5380.522949
Gradient norm: 3662.3974609375
Rank 1, Epoch 4, Batch 75, Loss: 5394.462891
Gradient norm: 3750.6142578125
Rank 1, Epoch 4, Batch 76, Loss: 5677.772461
Gradient norm: 3418.50634765625
Rank 1, Epoch 4, Batch 77, Loss: 5078.443359
Gradient norm: 3083.662109375
Rank 1, Epoch 4, Batch 78, Loss: 5926.853516
Gradient norm: 3397.614013671875
Rank 1, Epoch 4, Batch 79, Loss: 6830.942383
Gradient norm: 2795.711181640625
Rank 1, Epoch 4, Batch 80, Loss: 4668.615723
Gradient norm: 3625.29638671875
Rank 1, Epoch 4, Batch 81, Loss: 7093.105469
Gradient norm: 3644.88525390625
Rank 1, Epoch 4, Batch 82, Loss: 6365.385254
Gradient norm: 3278.779541015625
Rank 1, Epoch 4, Batch 83, Loss: 5260.578125
Gradient norm: 3451.227783203125
Rank 1, Epoch 4, Batch 84, Loss: 4266.951172
Gradient norm: 3483.361572265625
Rank 1, Epoch 4, Batch 85, Loss: 4373.909180
Gradient norm: 3691.6806640625
Rank 1, Epoch 4, Batch 86, Loss: 6745.380371
Gradient norm: 3226.753662109375
Rank 1, Epoch 4, Batch 87, Loss: 5976.558594
Gradient norm: 3613.68798828125
Rank 1, Epoch 4, Batch 88, Loss: 6214.178711
Gradient norm: 3586.990478515625
Rank 1, Epoch 4, Batch 89, Loss: 5722.886719
Gradient norm: 3604.294921875
Rank 1, Epoch 4, Batch 90, Loss: 6853.643555
Gradient norm: 3507.52880859375
Rank 1, Epoch 4, Batch 91, Loss: 6340.323730
Gradient norm: 3710.8310546875
Rank 1, Epoch 4, Batch 92, Loss: 3756.605469
Gradient norm: 3368.7939453125
Rank 1, Epoch 4, Batch 93, Loss: 6096.568359
Gradient norm: 3528.257568359375
Rank 1, Epoch 4, Batch 94, Loss: 6304.640137
Gradient norm: 3720.03759765625
Rank 1, Epoch 4, Batch 95, Loss: 8912.601562
Gradient norm: 3896.734619140625
Rank 1, Epoch 4, Batch 96, Loss: 8561.774414
Gradient norm: 3813.96044921875
Rank 1, Epoch 4, Batch 97, Loss: 7465.807617
Gradient norm: 3746.49560546875
Rank 1, Epoch 4, Batch 98, Loss: 4415.249512
Gradient norm: 3790.92724609375
Rank 1, Epoch 4, Batch 99, Loss: 5261.422363
Gradient norm: 3712.156982421875
Rank 1, Epoch 4, Batch 100, Loss: 7187.041016
Gradient norm: 3865.60791015625
Rank 1, Epoch 4, Batch 101, Loss: 5566.559082
Gradient norm: 3537.485595703125
Rank 1, Epoch 4, Batch 102, Loss: 5499.971191
Gradient norm: 4011.656005859375
Rank 1, Epoch 4, Batch 103, Loss: 9026.787109
Gradient norm: 3398.527099609375
Rank 1, Epoch 4, Batch 104, Loss: 7732.904297
Gradient norm: 3723.093017578125
Rank 1, Epoch 4, Batch 105, Loss: 8066.660156
Gradient norm: 1839.3841552734375
Rank 0, Epoch 4, Batch 105, Loss: 1836.468140
Gradient norm: 2026.043701171875
Rank 0, Epoch 4, Batch 106, Loss: 2365.403809
Gradient norm: 1902.138427734375
Rank 0, Epoch 4, Batch 107, Loss: 2560.728516
Gradient norm: 2159.453125
Rank 0, Epoch 4, Batch 108, Loss: 2181.617920
Gradient norm: 2059.7041015625
Rank 0, Epoch 4, Batch 109, Loss: 2267.130127
Gradient norm: 1873.171142578125
Rank 0, Epoch 4, Batch 110, Loss: 1658.085205
Gradient norm: 2158.210693359375
Rank 0, Epoch 4, Batch 111, Loss: 1549.129150
Gradient norm: 2213.891357421875
Rank 0, Epoch 4, Batch 112, Loss: 1757.482178
Gradient norm: 1994.19384765625
Rank 0, Epoch 4, Batch 113, Loss: 2281.231445
Gradient norm: 1965.4912109375
Rank 0, Epoch 4, Batch 114, Loss: 2340.192383
Gradient norm: 2090.487060546875
Rank 0, Epoch 4, Batch 115, Loss: 1760.684570
Gradient norm: 2184.95068359375
Rank 0, Epoch 4, Batch 116, Loss: 2050.424561
Gradient norm: 1901.7025146484375
Rank 0, Epoch 4, Batch 117, Loss: 1707.174072
Gradient norm: 2205.712158203125
Rank 0, Epoch 4, Batch 118, Loss: 2359.707275
Gradient norm: 2158.60693359375
Rank 0, Epoch 4, Batch 119, Loss: 2323.432129
Gradient norm: 1983.689208984375
Rank 0, Epoch 4, Batch 120, Loss: 2199.848633
Gradient norm: 2207.3544921875
Rank 0, Epoch 4, Batch 121, Loss: 2752.123779
Gradient norm: 2094.840087890625
Rank 0, Epoch 4, Batch 122, Loss: 2597.795410
Gradient norm: 2059.02392578125
Rank 0, Epoch 4, Batch 123, Loss: 2050.881104
Gradient norm: 1941.0281982421875
Rank 0, Epoch 4, Batch 124, Loss: 1760.553467
Gradient norm: 2085.890380859375
Rank 0, Epoch 4, Batch 125, Loss: 1643.472656
Gradient norm: 2124.31396484375
Rank 0, Epoch 4, Batch 126, Loss: 1727.954102
Gradient norm: 2242.7060546875
Rank 0, Epoch 4, Batch 127, Loss: 2001.966309
Gradient norm: 1931.9791259765625
Rank 0, Epoch 4, Batch 128, Loss: 1538.015259
Gradient norm: 2227.704345703125
Rank 0, Epoch 4, Batch 129, Loss: 2774.320312
Gradient norm: 2061.619873046875
Rank 0, Epoch 4, Batch 130, Loss: 3959.451172
Gradient norm: 2237.36962890625
Rank 0, Epoch 4, Batch 131, Loss: 1733.758301
Gradient norm: 2149.9052734375
Rank 0, Epoch 4, Batch 132, Loss: 1789.969727
Gradient norm: 2045.6708984375
Rank 0, Epoch 4, Batch 133, Loss: 1277.066406
Gradient norm: 2222.718017578125
Rank 0, Epoch 4, Batch 134, Loss: 1415.233032
Gradient norm: 2122.148193359375
Rank 0, Epoch 4, Batch 135, Loss: 1697.089844
Gradient norm: 2175.059326171875
Rank 0, Epoch 4, Batch 136, Loss: 2103.294434
Gradient norm: 2152.88427734375
Rank 0, Epoch 4, Batch 137, Loss: 3068.653809
Gradient norm: 2049.45068359375
Rank 0, Epoch 4, Batch 138, Loss: 2895.470703
Gradient norm: 2235.2041015625
Rank 0, Epoch 4, Batch 139, Loss: 3880.329102
Gradient norm: 2352.331298828125
Rank 0, Epoch 4, Batch 140, Loss: 3007.113770
Gradient norm: 2145.265869140625
Rank 0, Epoch 4, Batch 141, Loss: 1187.460693
Gradient norm: 2149.10546875
Rank 0, Epoch 4, Batch 142, Loss: 1815.469727
Gradient norm: 2077.711181640625
Rank 0, Epoch 4, Batch 143, Loss: 2196.768555
Gradient norm: 2181.89306640625
Rank 0, Epoch 4, Batch 144, Loss: 2058.429688
Gradient norm: 2237.0439453125
Rank 0, Epoch 4, Batch 145, Loss: 2370.765869
Gradient norm: 2316.1689453125
Rank 0, Epoch 4, Batch 146, Loss: 2177.323730
Gradient norm: 2177.932861328125
Rank 0, Epoch 4, Batch 147, Loss: 2198.729248
Gradient norm: 2232.34814453125
Rank 0, Epoch 4, Batch 148, Loss: 2559.231934
Gradient norm: 2189.645751953125
Rank 0, Epoch 4, Batch 149, Loss: 1827.218994
Gradient norm: 2223.9970703125
Rank 0, Epoch 4, Batch 150, Loss: 2883.303223
Gradient norm: 2013.3494873046875
Rank 0, Epoch 4, Batch 151, Loss: 3194.035645
Gradient norm: 2349.932861328125
Rank 0, Epoch 4, Batch 152, Loss: 3139.343750
Gradient norm: 2095.727783203125
Rank 0, Epoch 4, Batch 153, Loss: 1996.807373
Gradient norm: 2340.60791015625
Rank 0, Epoch 4, Batch 154, Loss: 1346.724121
Gradient norm: 2201.647216796875
Rank 0, Epoch 4, Batch 155, Loss: 1668.311401
Gradient norm: 2322.817138671875
Rank 0, Epoch 4, Batch 156, Loss: 2379.685547
Gradient norm: 2336.441650390625
Rank 0, Epoch 4, Batch 157, Loss: 1902.479126
Gradient norm: 2427.85595703125
Rank 0, Epoch 4, Batch 158, Loss: 2936.817383
Gradient norm: 2306.682373046875
Rank 0, Epoch 4, Batch 159, Loss: 3419.111816
Gradient norm: 2300.914794921875
Rank 0, Epoch 4, Batch 160, Loss: 3504.518799
Gradient norm: 2193.954345703125
Rank 0, Epoch 4, Batch 161, Loss: 3332.337646
Gradient norm: 2271.765380859375
Rank 0, Epoch 4, Batch 162, Loss: 2626.735352
Gradient norm: 2210.576171875
Rank 0, Epoch 4, Batch 163, Loss: 1116.286621
Gradient norm: 2392.67919921875
Rank 0, Epoch 4, Batch 164, Loss: 1986.487793
Gradient norm: 2353.11962890625
Rank 0, Epoch 4, Batch 165, Loss: 2456.567871
Gradient norm: 1996.167236328125
Rank 0, Epoch 4, Batch 166, Loss: 2266.958984
Gradient norm: 2298.628173828125
Rank 0, Epoch 4, Batch 167, Loss: 2950.981445
Gradient norm: 2234.941650390625
Rank 0, Epoch 4, Batch 168, Loss: 2617.453613
Gradient norm: 2202.515869140625
Rank 0, Epoch 4, Batch 169, Loss: 2417.260498
Gradient norm: 2166.354736328125
Rank 0, Epoch 4, Batch 170, Loss: 3045.021484
Gradient norm: 2422.13671875
Rank 0, Epoch 4, Batch 171, Loss: 2477.753418
Gradient norm: 2279.8125
Rank 0, Epoch 4, Batch 172, Loss: 2372.678223
Gradient norm: 2266.689453125
Rank 0, Epoch 4, Batch 173, Loss: 2015.409668
Gradient norm: 2313.32421875
Rank 0, Epoch 4, Batch 174, Loss: 2452.817383
Gradient norm: 2460.65380859375
Rank 0, Epoch 4, Batch 175, Loss: 2609.945312
Gradient norm: 2397.623291015625
Rank 0, Epoch 4, Batch 176, Loss: 2452.050293
Gradient norm: 2173.37548828125
Rank 0, Epoch 4, Batch 177, Loss: 2214.683350
Gradient norm: 2413.04638671875
Rank 0, Epoch 4, Batch 178, Loss: 2835.293457
Gradient norm: 2553.125732421875
Rank 0, Epoch 4, Batch 179, Loss: 4017.804199
Gradient norm: 2299.0908203125
Rank 0, Epoch 4, Batch 180, Loss: 3149.348145
Gradient norm: 2371.289306640625
Rank 0, Epoch 4, Batch 181, Loss: 3872.570312
Gradient norm: 2420.9970703125
Rank 0, Epoch 4, Batch 182, Loss: 1648.903442
Gradient norm: 2503.04296875
Rank 0, Epoch 4, Batch 183, Loss: 1299.963867
Gradient norm: 2383.0849609375
Rank 0, Epoch 4, Batch 184, Loss: 2240.092529
Gradient norm: 2420.969482421875
Rank 0, Epoch 4, Batch 185, Loss: 3210.997803
Gradient norm: 2372.431884765625
Rank 0, Epoch 4, Batch 186, Loss: 3238.217285
Gradient norm: 2421.377197265625
Rank 0, Epoch 4, Batch 187, Loss: 3408.327148
Gradient norm: 2339.94677734375
Rank 0, Epoch 4, Batch 188, Loss: 2363.041992
Gradient norm: 2332.634033203125
Rank 0, Epoch 4, Batch 189, Loss: 2625.070312
Gradient norm: 2444.68994140625
Rank 0, Epoch 4, Batch 190, Loss: 2983.166504
Gradient norm: 2283.638671875
Rank 0, Epoch 4, Batch 191, Loss: 3380.138672
Gradient norm: 1998.888671875
Rank 0, Epoch 4, Batch 192, Loss: 2800.905029
Gradient norm: 2378.463623046875
Rank 0, Epoch 4, Batch 193, Loss: 2331.176758
Gradient norm: 2345.265380859375
Rank 0, Epoch 4, Batch 194, Loss: 2253.140869
Gradient norm: 2409.608154296875
Rank 0, Epoch 4, Batch 195, Loss: 2604.557617
Gradient norm: 2513.020751953125
Rank 0, Epoch 4, Batch 196, Loss: 2856.195801
Gradient norm: 2383.23193359375
Rank 0, Epoch 4, Batch 197, Loss: 2057.670898
Gradient norm: 2395.853271484375
Rank 0, Epoch 4, Batch 198, Loss: 2268.724365
Gradient norm: 2484.87158203125
Rank 0, Epoch 4, Batch 199, Loss: 2774.510254
Gradient norm: 2289.336669921875
Rank 0, Epoch 4, Batch 200, Loss: 2954.807129
Gradient norm: 2304.2041015625
Rank 0, Epoch 4, Batch 201, Loss: 2627.145508
Gradient norm: 2299.57568359375
Rank 0, Epoch 4, Batch 202, Loss: 3178.679199
Gradient norm: 2420.325439453125
Rank 0, Epoch 4, Batch 203, Loss: 3484.407959
Gradient norm: 2230.893798828125
Rank 0, Epoch 4, Batch 204, Loss: 3203.842773
Gradient norm: 2489.111572265625
Rank 0, Epoch 4, Batch 205, Loss: 2603.227051
Gradient norm: 2588.015625
Rank 0, Epoch 4, Batch 206, Loss: 2972.339600
Gradient norm: 2263.156005859375
Rank 0, Epoch 4, Batch 207, Loss: 2906.909668
Gradient norm: 2356.312255859375
Rank 0, Epoch 4, Batch 208, Loss: 1846.767090
Gradient norm: 2571.879638671875

Rank 2, Epoch 4, Batch 105, Loss: 2619.083496
Gradient norm: 2011.9012451171875
Rank 2, Epoch 4, Batch 106, Loss: 1647.144287
Gradient norm: 1853.93310546875
Rank 2, Epoch 4, Batch 107, Loss: 1634.355957
Gradient norm: 2091.544921875
Rank 2, Epoch 4, Batch 108, Loss: 2768.207031
Gradient norm: 1936.46240234375
Rank 2, Epoch 4, Batch 109, Loss: 2768.928223
Gradient norm: 2063.439208984375
Rank 2, Epoch 4, Batch 110, Loss: 2462.508301
Gradient norm: 1952.7520751953125
Rank 2, Epoch 4, Batch 111, Loss: 987.847168
Gradient norm: 1962.36083984375
Rank 2, Epoch 4, Batch 112, Loss: 913.076782
Gradient norm: 1840.244873046875
Rank 2, Epoch 4, Batch 113, Loss: 1526.043457
Gradient norm: 2070.281982421875
Rank 2, Epoch 4, Batch 114, Loss: 1272.788940
Gradient norm: 2053.94091796875
Rank 2, Epoch 4, Batch 115, Loss: 2348.405518
Gradient norm: 2118.391357421875
Rank 2, Epoch 4, Batch 116, Loss: 2277.927734
Gradient norm: 2024.564208984375
Rank 2, Epoch 4, Batch 117, Loss: 2135.707764
Gradient norm: 2201.411376953125
Rank 2, Epoch 4, Batch 118, Loss: 3332.551025
Gradient norm: 1890.87548828125
Rank 2, Epoch 4, Batch 119, Loss: 2765.910156
Gradient norm: 2054.500244140625
Rank 2, Epoch 4, Batch 120, Loss: 1445.731812
Gradient norm: 2195.733154296875
Rank 2, Epoch 4, Batch 121, Loss: 1805.138794
Gradient norm: 2206.521728515625
Rank 2, Epoch 4, Batch 122, Loss: 2111.672852
Gradient norm: 1995.5142822265625
Rank 2, Epoch 4, Batch 123, Loss: 1879.716309
Gradient norm: 2106.490966796875
Rank 2, Epoch 4, Batch 124, Loss: 2089.978516
Gradient norm: 2098.6416015625
Rank 2, Epoch 4, Batch 125, Loss: 1982.102173
Gradient norm: 2068.82421875
Rank 2, Epoch 4, Batch 126, Loss: 2578.892578
Gradient norm: 2181.74560546875
Rank 2, Epoch 4, Batch 127, Loss: 2210.206055
Gradient norm: 1794.67626953125
Rank 2, Epoch 4, Batch 128, Loss: 2047.320557
Gradient norm: 2298.138427734375
Rank 2, Epoch 4, Batch 129, Loss: 2742.739014
Gradient norm: 2168.98291015625
Rank 2, Epoch 4, Batch 130, Loss: 1286.368164
Gradient norm: 2020.44873046875
Rank 2, Epoch 4, Batch 131, Loss: 1716.113281
Gradient norm: 2176.84716796875
Rank 2, Epoch 4, Batch 132, Loss: 2073.357910
Gradient norm: 1995.5048828125
Rank 2, Epoch 4, Batch 133, Loss: 2115.091797
Gradient norm: 2156.14990234375
Rank 2, Epoch 4, Batch 134, Loss: 2604.536621
Gradient norm: 2166.83837890625
Rank 2, Epoch 4, Batch 135, Loss: 2488.952148
Gradient norm: 1919.18798828125
Rank 2, Epoch 4, Batch 136, Loss: 1788.676147
Gradient norm: 2142.5224609375
Rank 2, Epoch 4, Batch 137, Loss: 2599.083008
Gradient norm: 2185.3818359375
Rank 2, Epoch 4, Batch 138, Loss: 2645.117188
Gradient norm: 2237.8564453125
Rank 2, Epoch 4, Batch 139, Loss: 2199.111084
Gradient norm: 2361.261962890625
Rank 2, Epoch 4, Batch 140, Loss: 2254.172119
Gradient norm: 2214.218017578125
Rank 2, Epoch 4, Batch 141, Loss: 2002.248291
Gradient norm: 2044.4864501953125
Rank 2, Epoch 4, Batch 142, Loss: 1788.840576
Gradient norm: 2064.248779296875
Rank 2, Epoch 4, Batch 143, Loss: 1581.028320
Gradient norm: 2194.20556640625
Rank 2, Epoch 4, Batch 144, Loss: 2752.725830
Gradient norm: 2075.9365234375
Rank 2, Epoch 4, Batch 145, Loss: 2285.364502
Gradient norm: 2217.9501953125
Rank 2, Epoch 4, Batch 146, Loss: 2456.885498
Gradient norm: 2277.513671875
Rank 2, Epoch 4, Batch 147, Loss: 3119.168213
Gradient norm: 2346.2578125
Rank 2, Epoch 4, Batch 148, Loss: 2698.022705
Gradient norm: 2201.884033203125
Rank 2, Epoch 4, Batch 149, Loss: 2564.023926
Gradient norm: 2038.2720947265625
Rank 2, Epoch 4, Batch 150, Loss: 1618.128906
Gradient norm: 2291.869873046875
Rank 2, Epoch 4, Batch 151, Loss: 1343.796143
Gradient norm: 2195.871826171875
Rank 2, Epoch 4, Batch 152, Loss: 2551.585205
Gradient norm: 2319.386962890625
Rank 2, Epoch 4, Batch 153, Loss: 2638.980469
Gradient norm: 2334.263671875
Rank 2, Epoch 4, Batch 154, Loss: 2681.140869
Gradient norm: 2295.742919921875
Rank 2, Epoch 4, Batch 155, Loss: 3446.018066
Gradient norm: 2334.727294921875
Rank 2, Epoch 4, Batch 156, Loss: 2537.550781
Gradient norm: 2048.419189453125
Rank 2, Epoch 4, Batch 157, Loss: 1888.721924
Gradient norm: 2273.68212890625
Rank 2, Epoch 4, Batch 158, Loss: 2880.023438
Gradient norm: 2150.572998046875
Rank 2, Epoch 4, Batch 159, Loss: 2895.063965
Gradient norm: 2164.2890625
Rank 2, Epoch 4, Batch 160, Loss: 2011.047119
Gradient norm: 2326.9404296875
Rank 2, Epoch 4, Batch 161, Loss: 2262.336914
Gradient norm: 2174.91064453125
Rank 2, Epoch 4, Batch 162, Loss: 2035.152344
Gradient norm: 2294.235595703125
Rank 2, Epoch 4, Batch 163, Loss: 1880.086914
Gradient norm: 2350.885498046875
Rank 2, Epoch 4, Batch 164, Loss: 2702.729004
Gradient norm: 2308.15625
Rank 2, Epoch 4, Batch 165, Loss: 2666.444336
Gradient norm: 2132.2734375
Rank 2, Epoch 4, Batch 166, Loss: 2242.691406
Gradient norm: 2198.213623046875
Rank 2, Epoch 4, Batch 167, Loss: 2699.695068
Gradient norm: 2255.084716796875
Rank 2, Epoch 4, Batch 168, Loss: 3425.482910
Gradient norm: 2224.6171875
Rank 2, Epoch 4, Batch 169, Loss: 2755.533691
Gradient norm: 2103.09326171875
Rank 2, Epoch 4, Batch 170, Loss: 1689.310791
Gradient norm: 2265.861572265625
Rank 2, Epoch 4, Batch 171, Loss: 2310.815918
Gradient norm: 2243.316650390625
Rank 2, Epoch 4, Batch 172, Loss: 2277.181641
Gradient norm: 2169.407958984375
Rank 2, Epoch 4, Batch 173, Loss: 1782.311523
Gradient norm: 2325.555419921875
Rank 2, Epoch 4, Batch 174, Loss: 2316.285645
Gradient norm: 2211.793212890625
Rank 2, Epoch 4, Batch 175, Loss: 2086.456055
Gradient norm: 2453.449951171875
Rank 2, Epoch 4, Batch 176, Loss: 2527.894775
Gradient norm: 2361.337646484375
Rank 2, Epoch 4, Batch 177, Loss: 1841.643555
Gradient norm: 2021.0814208984375
Rank 2, Epoch 4, Batch 178, Loss: 2789.567627
Gradient norm: 2356.878173828125
Rank 2, Epoch 4, Batch 179, Loss: 2748.885254
Gradient norm: 2145.943359375
Rank 2, Epoch 4, Batch 180, Loss: 3274.654541
Gradient norm: 2370.060791015625
Rank 2, Epoch 4, Batch 181, Loss: 3599.896729
Gradient norm: 2384.3984375
Rank 2, Epoch 4, Batch 182, Loss: 3142.268555
Gradient norm: 2279.064697265625
Rank 2, Epoch 4, Batch 183, Loss: 2071.723877
Gradient norm: 2386.36962890625
Rank 2, Epoch 4, Batch 184, Loss: 1553.560791
Gradient norm: 2143.472412109375
Rank 2, Epoch 4, Batch 185, Loss: 1879.927002
Gradient norm: 2321.693359375
Rank 2, Epoch 4, Batch 186, Loss: 2105.972656
Gradient norm: 2211.25
Rank 2, Epoch 4, Batch 187, Loss: 2710.540039
Gradient norm: 2507.562255859375
Rank 2, Epoch 4, Batch 188, Loss: 2779.312012
Gradient norm: 2156.6435546875
Rank 2, Epoch 4, Batch 189, Loss: 2167.444092
Gradient norm: 2392.991943359375
Rank 2, Epoch 4, Batch 190, Loss: 3303.550049
Gradient norm: 2405.048095703125
Rank 2, Epoch 4, Batch 191, Loss: 3328.943115
Gradient norm: 2381.990234375
Rank 2, Epoch 4, Batch 192, Loss: 2903.249023
Gradient norm: 2101.341796875
Rank 2, Epoch 4, Batch 193, Loss: 3226.893555
Gradient norm: 2432.014404296875
Rank 2, Epoch 4, Batch 194, Loss: 2498.001465
Gradient norm: 2143.70654296875
Rank 2, Epoch 4, Batch 195, Loss: 1721.871094
Gradient norm: 2225.63671875
Rank 2, Epoch 4, Batch 196, Loss: 2057.575439
Gradient norm: 2355.25390625
Rank 2, Epoch 4, Batch 197, Loss: 2356.357422
Gradient norm: 2347.818359375
Rank 2, Epoch 4, Batch 198, Loss: 2604.315186
Gradient norm: 2322.529541015625
Rank 2, Epoch 4, Batch 199, Loss: 3178.947266
Gradient norm: 2325.52197265625
Rank 2, Epoch 4, Batch 200, Loss: 2697.765137
Gradient norm: 2285.3271484375
Rank 2, Epoch 4, Batch 201, Loss: 2941.453125
Gradient norm: 2486.509033203125
Rank 2, Epoch 4, Batch 202, Loss: 2995.928223
Gradient norm: 2386.437255859375
Rank 2, Epoch 4, Batch 203, Loss: 2435.553223
Gradient norm: 2404.956298828125
Rank 2, Epoch 4, Batch 204, Loss: 2084.456299
Gradient norm: 1967.888671875
Rank 2, Epoch 4, Batch 205, Loss: 2659.416016
Gradient norm: 2381.856201171875
Rank 2, Epoch 4, Batch 206, Loss: 2950.062744
Gradient norm: 2531.044921875
Rank 2, Epoch 4, Batch 207, Loss: 2505.134766
Gradient norm: 2223.224609375
Rank 2, Epoch 4, Batch 208, Loss: 2815.237061
Gradient norm: 2227.13818359375
Rank 2, Epoch 4, Batch 209, Loss: 1943.021851
Gradient norm: 2157.422607421875
Gradient norm: 3758.8134765625
Rank 1, Epoch 4, Batch 106, Loss: 8249.090820
Gradient norm: 3514.5478515625
Rank 1, Epoch 4, Batch 107, Loss: 4965.291016
Gradient norm: 3426.581298828125
Rank 1, Epoch 4, Batch 108, Loss: 4276.056641
Gradient norm: 3778.759765625
Rank 1, Epoch 4, Batch 109, Loss: 5902.188477
Gradient norm: 3904.03564453125
Rank 1, Epoch 4, Batch 110, Loss: 6309.819336
Gradient norm: 3758.9931640625
Rank 1, Epoch 4, Batch 111, Loss: 7178.230957
Gradient norm: 3780.34375
Rank 1, Epoch 4, Batch 112, Loss: 6383.445312
Gradient norm: 3484.3017578125
Rank 1, Epoch 4, Batch 113, Loss: 6776.260742
Gradient norm: 3644.44287109375
Rank 1, Epoch 4, Batch 114, Loss: 5470.044922
Gradient norm: 3969.358642578125
Rank 1, Epoch 4, Batch 115, Loss: 9027.595703
Gradient norm: 3865.93310546875
Rank 1, Epoch 4, Batch 116, Loss: 8383.944336
Gradient norm: 3703.6220703125
Rank 1, Epoch 4, Batch 117, Loss: 7110.103516
Gradient norm: 3971.85888671875
Rank 1, Epoch 4, Batch 118, Loss: 6954.449219
Gradient norm: 4014.145263671875
Rank 1, Epoch 4, Batch 119, Loss: 8506.219727
Gradient norm: 3914.82763671875
Rank 1, Epoch 4, Batch 120, Loss: 6536.975586
Gradient norm: 3778.35693359375
Rank 1, Epoch 4, Batch 121, Loss: 5616.372070
Gradient norm: 3644.729736328125
Rank 1, Epoch 4, Batch 122, Loss: 6463.199219
Gradient norm: 3506.1328125
Rank 1, Epoch 4, Batch 123, Loss: 6347.554199
Gradient norm: 3687.209716796875
Rank 1, Epoch 4, Batch 124, Loss: 5499.414062
Gradient norm: 3967.5078125
Rank 1, Epoch 4, Batch 125, Loss: 8518.535156
Gradient norm: 3803.833251953125
Rank 1, Epoch 4, Batch 126, Loss: 8233.375000
Gradient norm: 4026.225341796875
Rank 1, Epoch 4, Batch 127, Loss: 9269.511719
Gradient norm: 4005.599609375
Rank 1, Epoch 4, Batch 128, Loss: 5456.442383
Gradient norm: 3865.622314453125
Rank 1, Epoch 4, Batch 129, Loss: 5297.508789
Gradient norm: 4124.333984375
Rank 1, Epoch 4, Batch 130, Loss: 7924.738281
Gradient norm: 4071.91357421875
Rank 1, Epoch 4, Batch 131, Loss: 7500.168457
Gradient norm: 3752.389404296875
Rank 1, Epoch 4, Batch 132, Loss: 8867.811523
Gradient norm: 3773.416259765625
Rank 1, Epoch 4, Batch 133, Loss: 7933.632812
Gradient norm: 4050.0927734375
Rank 1, Epoch 4, Batch 134, Loss: 10552.517578
Gradient norm: 3858.123779296875
Rank 1, Epoch 4, Batch 135, Loss: 6662.027344
Gradient norm: 4186.0400390625
Rank 1, Epoch 4, Batch 136, Loss: 8015.366211
Gradient norm: 3814.376220703125
Rank 1, Epoch 4, Batch 137, Loss: 6300.144531
Gradient norm: 3958.984375
Rank 1, Epoch 4, Batch 138, Loss: 7682.786133
Gradient norm: 3844.66455078125
Rank 1, Epoch 4, Batch 139, Loss: 6158.667969
Gradient norm: 4025.363525390625
Rank 1, Epoch 4, Batch 140, Loss: 7156.604492
Gradient norm: 4032.04638671875
Rank 1, Epoch 4, Batch 141, Loss: 5624.468750
Gradient norm: 3917.3291015625
Rank 1, Epoch 4, Batch 142, Loss: 8182.254883
Gradient norm: 3993.666748046875
Rank 1, Epoch 4, Batch 143, Loss: 7678.559570
Gradient norm: 3876.83642578125
Rank 1, Epoch 4, Batch 144, Loss: 4607.984375
Gradient norm: 4147.00634765625
Rank 1, Epoch 4, Batch 145, Loss: 9220.280273
Gradient norm: 3688.22119140625
Rank 1, Epoch 4, Batch 146, Loss: 8895.398438
Gradient norm: 4096.44287109375
Rank 1, Epoch 4, Batch 147, Loss: 11631.588867
Gradient norm: 3979.120849609375
Rank 1, Epoch 4, Batch 148, Loss: 10654.656250
Gradient norm: 4020.88623046875
Rank 1, Epoch 4, Batch 149, Loss: 5229.249023
Gradient norm: 3524.593994140625
Rank 1, Epoch 4, Batch 150, Loss: 5231.750977
Gradient norm: 4071.83740234375
Rank 1, Epoch 4, Batch 151, Loss: 4316.260742
Gradient norm: 4018.267578125
Rank 1, Epoch 4, Batch 152, Loss: 6010.838867
Gradient norm: 4224.25830078125
Rank 1, Epoch 4, Batch 153, Loss: 7935.812988
Gradient norm: 4368.91259765625
Rank 1, Epoch 4, Batch 154, Loss: 6681.981934
Gradient norm: 4036.90771484375
Rank 1, Epoch 4, Batch 155, Loss: 9696.381836
Gradient norm: 4293.00439453125
Rank 1, Epoch 4, Batch 156, Loss: 10626.230469
Gradient norm: 4053.833740234375
Rank 1, Epoch 4, Batch 157, Loss: 11308.609375
Gradient norm: 3847.51416015625
Rank 1, Epoch 4, Batch 158, Loss: 10530.046875
Gradient norm: 3866.557861328125
Rank 1, Epoch 4, Batch 159, Loss: 8133.219238
Gradient norm: 3676.646240234375
Rank 1, Epoch 4, Batch 160, Loss: 4774.504883
Gradient norm: 4204.32373046875
Rank 1, Epoch 4, Batch 161, Loss: 7186.179199
Gradient norm: 3794.851806640625
Rank 1, Epoch 4, Batch 162, Loss: 7150.740723
Gradient norm: 4085.635009765625
Rank 1, Epoch 4, Batch 163, Loss: 9194.593750
Gradient norm: 4335.537109375
Rank 1, Epoch 4, Batch 164, Loss: 8506.790039
Gradient norm: 3991.689208984375
Rank 1, Epoch 4, Batch 165, Loss: 6349.908203
Gradient norm: 3980.837646484375
Rank 1, Epoch 4, Batch 166, Loss: 3492.598145
Gradient norm: 3831.672119140625
Rank 1, Epoch 4, Batch 167, Loss: 7910.563477
Gradient norm: 4092.0224609375
Rank 1, Epoch 4, Batch 168, Loss: 8404.294922
Gradient norm: 4277.560546875
Rank 1, Epoch 4, Batch 169, Loss: 8579.774414
Gradient norm: 3946.82177734375
Rank 1, Epoch 4, Batch 170, Loss: 8986.774414
Gradient norm: 4244.7587890625
Rank 1, Epoch 4, Batch 171, Loss: 10845.884766
Gradient norm: 4367.9375
Rank 1, Epoch 4, Batch 172, Loss: 12196.916992
Gradient norm: 4127.6279296875
Rank 1, Epoch 4, Batch 173, Loss: 6174.229492
Gradient norm: 3965.376953125
Rank 1, Epoch 4, Batch 174, Loss: 7479.784668
Gradient norm: 4114.599609375
Rank 1, Epoch 4, Batch 175, Loss: 8472.787109
Gradient norm: 4059.572265625
Rank 1, Epoch 4, Batch 176, Loss: 7829.307617
Gradient norm: 3797.16845703125
Rank 1, Epoch 4, Batch 177, Loss: 8279.391602
Gradient norm: 4109.1865234375
Rank 1, Epoch 4, Batch 178, Loss: 10749.011719
Gradient norm: 3964.22314453125
Rank 1, Epoch 4, Batch 179, Loss: 5558.787109
Gradient norm: 4084.48681640625
Rank 1, Epoch 4, Batch 180, Loss: 6973.708008
Gradient norm: 3886.2001953125
Rank 1, Epoch 4, Batch 181, Loss: 7805.662109
Gradient norm: 4378.28955078125
Rank 1, Epoch 4, Batch 182, Loss: 6380.806641
Gradient norm: 3883.313720703125
Rank 1, Epoch 4, Batch 183, Loss: 6938.900391
Gradient norm: 4124.2412109375
Rank 1, Epoch 4, Batch 184, Loss: 8910.411133
Gradient norm: 4091.007080078125
Rank 1, Epoch 4, Batch 185, Loss: 7585.923340
Gradient norm: 4615.82763671875
Rank 1, Epoch 4, Batch 186, Loss: 9988.935547
Gradient norm: 4304.1640625
Rank 1, Epoch 4, Batch 187, Loss: 11207.480469
Gradient norm: 4050.2783203125
Rank 1, Epoch 4, Batch 188, Loss: 10056.929688
Gradient norm: 4323.7548828125
Rank 1, Epoch 4, Batch 189, Loss: 10065.911133
Gradient norm: 4022.659423828125
Rank 1, Epoch 4, Batch 190, Loss: 7578.934570
Gradient norm: 4347.3330078125
Rank 1, Epoch 4, Batch 191, Loss: 11006.849609
Gradient norm: 4165.4794921875
Rank 1, Epoch 4, Batch 192, Loss: 5665.736328
Gradient norm: 4483.74169921875
Rank 1, Epoch 4, Batch 193, Loss: 5168.954102
Gradient norm: 4267.36181640625
Rank 1, Epoch 4, Batch 194, Loss: 5201.797852
Gradient norm: 4395.90283203125
Rank 1, Epoch 4, Batch 195, Loss: 8282.007812
Gradient norm: 4138.552734375
Rank 1, Epoch 4, Batch 196, Loss: 9247.240234
Gradient norm: 3438.032470703125
Rank 1, Epoch 4, Batch 197, Loss: 9593.789062
Gradient norm: 4252.77392578125
Rank 1, Epoch 4, Batch 198, Loss: 10146.674805
Gradient norm: 4250.4814453125
Rank 1, Epoch 4, Batch 199, Loss: 9300.299805
Gradient norm: 4261.29150390625
Rank 1, Epoch 4, Batch 200, Loss: 11900.003906
Gradient norm: 4031.31298828125
Rank 1, Epoch 4, Batch 201, Loss: 10486.633789
Gradient norm: 3923.699951171875
Rank 1, Epoch 4, Batch 202, Loss: 6949.018555
Gradient norm: 4629.91455078125
Rank 1, Epoch 4, Batch 203, Loss: 9075.371094
Gradient norm: 4724.341796875
Rank 1, Epoch 4, Batch 204, Loss: 5400.475586
Gradient norm: 3985.33056640625
Rank 1, Epoch 4, Batch 205, Loss: 9809.927734
Gradient norm: 4529.7373046875
Rank 1, Epoch 4, Batch 206, Loss: 10489.541992
Gradient norm: 4103.5498046875
Rank 1, Epoch 4, Batch 207, Loss: 6393.289062
Gradient norm: 4162.01123046875
Rank 1, Epoch 4, Batch 208, Loss: 7266.823242
Gradient norm: 4227.10546875
Rank 1, Epoch 4, Batch 209, Loss: 7616.674805
Gradient norm: 4461.76220703125
Rank 1, Epoch 4, Batch 210, Loss: 11276.102539
Gradient norm: 4594.353515625
Rank 2, Epoch 4, Batch 210, Loss: 2602.562256
Gradient norm: 2434.416015625
Rank 2, Epoch 4, Batch 211, Loss: 3474.480469
Gradient norm: 2357.227294921875
Rank 2, Epoch 4, Batch 212, Loss: 1931.843750
Gradient norm: 2592.687255859375
Rank 2, Epoch 4, Batch 213, Loss: 3620.018555
Gradient norm: 2361.81396484375
Rank 2, Epoch 4, Batch 214, Loss: 2967.466797
Gradient norm: 2498.36328125
Rank 2, Epoch 4, Batch 215, Loss: 3099.558105
Gradient norm: 2361.244140625
Rank 2, Epoch 4, Batch 216, Loss: 2942.362061
Gradient norm: 2322.582763671875
Rank 2, Epoch 4, Batch 217, Loss: 2289.393066
Gradient norm: 2296.056884765625
Rank 2, Epoch 4, Batch 218, Loss: 2256.987793
Gradient norm: 2467.293212890625
Rank 2, Epoch 4, Batch 219, Loss: 2774.226562
Gradient norm: 2577.25634765625
Rank 2, Epoch 4, Batch 220, Loss: 2783.922852
Gradient norm: 2386.559326171875
Rank 2, Epoch 4, Batch 221, Loss: 2188.526855
Gradient norm: 2594.26806640625
Rank 2, Epoch 4, Batch 222, Loss: 3404.529541
Gradient norm: 2436.277587890625
Rank 2, Epoch 4, Batch 223, Loss: 2946.757812
Gradient norm: 2426.588134765625
Rank 2, Epoch 4, Batch 224, Loss: 2892.239990
Gradient norm: 2633.49951171875
Rank 2, Epoch 4, Batch 225, Loss: 3221.332520
Gradient norm: 2404.45703125
Rank 2, Epoch 4, Batch 226, Loss: 3076.534424
Gradient norm: 2554.86083984375
Rank 2, Epoch 4, Batch 227, Loss: 5090.008789
Gradient norm: 2561.410888671875
Rank 2, Epoch 4, Batch 228, Loss: 1780.264038
Gradient norm: 2404.146728515625
Rank 2, Epoch 4, Batch 229, Loss: 2532.105225
Gradient norm: 2460.15185546875
Rank 2, Epoch 4, Batch 230, Loss: 2583.903076
Gradient norm: 2466.108154296875
Rank 2, Epoch 4, Batch 231, Loss: 2930.849854
Gradient norm: 2191.62451171875
Rank 2, Epoch 4, Batch 232, Loss: 2984.999023
Gradient norm: 2500.088623046875
Rank 2, Epoch 4, Batch 233, Loss: 3653.509033
Gradient norm: 2496.137939453125
Rank 2, Epoch 4, Batch 234, Loss: 2232.784912
Gradient norm: 2451.52099609375
Rank 2, Epoch 4, Batch 235, Loss: 3376.014160
Gradient norm: 2445.557373046875
Rank 2, Epoch 4, Batch 236, Loss: 2877.659180
Gradient norm: 2554.608154296875
Rank 2, Epoch 4, Batch 237, Loss: 3778.140381
Gradient norm: 2533.11474609375
Rank 2, Epoch 4, Batch 238, Loss: 2011.271729
Gradient norm: 2373.82666015625
Rank 2, Epoch 4, Batch 239, Loss: 2500.001953
Gradient norm: 2532.721435546875
Rank 2, Epoch 4, Batch 240, Loss: 3358.038574
Gradient norm: 2574.21435546875
Rank 2, Epoch 4, Batch 241, Loss: 3755.222656
Gradient norm: 2576.90966796875
Rank 2, Epoch 4, Batch 242, Loss: 2357.464111
Gradient norm: 2484.306884765625
Rank 2, Epoch 4, Batch 243, Loss: 2929.051025
Gradient norm: 2236.187744140625
Rank 2, Epoch 4, Batch 244, Loss: 3027.287842
Gradient norm: 2477.048095703125
Rank 2, Epoch 4, Batch 245, Loss: 3033.135742
Gradient norm: 2651.53271484375
Rank 2, Epoch 4, Batch 246, Loss: 2535.154785
Gradient norm: 2200.320068359375
Rank 2, Epoch 4, Batch 247, Loss: 3238.835449
Gradient norm: 2706.513916015625
Rank 2, Epoch 4, Batch 248, Loss: 3275.275879
Gradient norm: 2743.549072265625
Rank 2, Epoch 4, Batch 249, Loss: 3390.407715
Gradient norm: 2412.3388671875
Rank 2, Epoch 4, Batch 250, Loss: 2549.338867
Gradient norm: 2586.21923828125
Rank 2, Epoch 4, Batch 251, Loss: 3382.138916
Gradient norm: 2705.40869140625
Rank 2, Epoch 4, Batch 252, Loss: 3057.491211
Gradient norm: 2529.763916015625
Rank 2, Epoch 4, Batch 253, Loss: 3472.049805
Gradient norm: 2528.811279296875
Rank 2, Epoch 4, Batch 254, Loss: 3799.970703
Gradient norm: 2565.0205078125
Rank 2, Epoch 4, Batch 255, Loss: 3649.559814
Gradient norm: 2687.262451171875
Rank 2, Epoch 4, Batch 256, Loss: 3246.380859
Gradient norm: 2521.9150390625
Rank 2, Epoch 4, Batch 257, Loss: 3674.251953
Gradient norm: 2734.47705078125
Rank 2, Epoch 4, Batch 258, Loss: 3689.986572
Gradient norm: 2508.061767578125
Rank 2, Epoch 4, Batch 259, Loss: 1888.819336
Gradient norm: 2897.37890625
Rank 2, Epoch 4, Batch 260, Loss: 1730.374390
Rank 2, Epoch 4, Val Loss: 626.4062, Val Acc: 0.1000, Time: 198.52s
Rank 0, Epoch 4, Batch 209, Loss: 2660.798828
Gradient norm: 1989.9189453125
Rank 0, Epoch 4, Batch 210, Loss: 1938.930420
Gradient norm: 2359.10107421875
Rank 0, Epoch 4, Batch 211, Loss: 3093.102295
Gradient norm: 2448.37744140625
Rank 0, Epoch 4, Batch 212, Loss: 3092.303223
Gradient norm: 2165.6201171875
Rank 0, Epoch 4, Batch 213, Loss: 2640.068115
Gradient norm: 2519.6728515625
Rank 0, Epoch 4, Batch 214, Loss: 2276.276123
Gradient norm: 2346.9208984375
Rank 0, Epoch 4, Batch 215, Loss: 2921.193848
Gradient norm: 2248.18212890625
Rank 0, Epoch 4, Batch 216, Loss: 3100.585693
Gradient norm: 2355.20166015625
Rank 0, Epoch 4, Batch 217, Loss: 2980.528809
Gradient norm: 2573.86181640625
Rank 0, Epoch 4, Batch 218, Loss: 2369.771729
Gradient norm: 2616.382080078125
Rank 0, Epoch 4, Batch 219, Loss: 2850.994629
Gradient norm: 2422.776611328125
Rank 0, Epoch 4, Batch 220, Loss: 2560.229980
Gradient norm: 2487.2607421875
Rank 0, Epoch 4, Batch 221, Loss: 3274.872070
Gradient norm: 2597.14013671875
Rank 0, Epoch 4, Batch 222, Loss: 3281.955566
Gradient norm: 2525.720458984375
Rank 0, Epoch 4, Batch 223, Loss: 3181.875000
Gradient norm: 2386.5927734375
Rank 0, Epoch 4, Batch 224, Loss: 3302.349609
Gradient norm: 2523.898193359375
Rank 0, Epoch 4, Batch 225, Loss: 3684.624023
Gradient norm: 2453.912109375
Rank 0, Epoch 4, Batch 226, Loss: 3377.979004
Gradient norm: 2540.28662109375
Rank 0, Epoch 4, Batch 227, Loss: 2514.818359
Gradient norm: 2593.77392578125
Rank 0, Epoch 4, Batch 228, Loss: 2666.792480
Gradient norm: 2113.526611328125
Rank 0, Epoch 4, Batch 229, Loss: 2593.670410
Gradient norm: 2547.6083984375
Rank 0, Epoch 4, Batch 230, Loss: 2284.038818
Gradient norm: 2406.603271484375
Rank 0, Epoch 4, Batch 231, Loss: 2246.341797
Gradient norm: 2549.1484375
Rank 0, Epoch 4, Batch 232, Loss: 3816.877441
Gradient norm: 2404.5791015625
Rank 0, Epoch 4, Batch 233, Loss: 2628.880127
Gradient norm: 2522.044189453125
Rank 0, Epoch 4, Batch 234, Loss: 3540.350586
Gradient norm: 2508.60595703125
Rank 0, Epoch 4, Batch 235, Loss: 3499.926514
Gradient norm: 2604.139892578125
Rank 0, Epoch 4, Batch 236, Loss: 4000.586426
Gradient norm: 2470.394775390625
Rank 0, Epoch 4, Batch 237, Loss: 2590.929199
Gradient norm: 2400.39453125
Rank 0, Epoch 4, Batch 238, Loss: 2480.627930
Gradient norm: 2498.26513671875
Rank 0, Epoch 4, Batch 239, Loss: 2274.056641
Gradient norm: 2382.915771484375
Rank 0, Epoch 4, Batch 240, Loss: 3141.476562
Gradient norm: 2701.235595703125
Rank 0, Epoch 4, Batch 241, Loss: 3475.730713
Gradient norm: 2630.7060546875
Rank 0, Epoch 4, Batch 242, Loss: 2973.099365
Gradient norm: 2656.282958984375
Rank 0, Epoch 4, Batch 243, Loss: 3399.912109
Gradient norm: 2595.798095703125
Rank 0, Epoch 4, Batch 244, Loss: 3521.654541
Gradient norm: 2750.34033203125
Rank 0, Epoch 4, Batch 245, Loss: 3170.516602
Gradient norm: 2587.22607421875
Rank 0, Epoch 4, Batch 246, Loss: 2893.412354
Gradient norm: 2685.670654296875
Rank 0, Epoch 4, Batch 247, Loss: 3479.345703
Gradient norm: 2474.154296875
Rank 0, Epoch 4, Batch 248, Loss: 3047.380615
Gradient norm: 2496.936767578125
Rank 0, Epoch 4, Batch 249, Loss: 3368.237061
Gradient norm: 2605.541015625
Rank 0, Epoch 4, Batch 250, Loss: 5357.979492
Gradient norm: 2429.764404296875
Rank 0, Epoch 4, Batch 251, Loss: 3051.570068
Gradient norm: 2602.270263671875
Rank 0, Epoch 4, Batch 252, Loss: 2934.805908
Gradient norm: 2348.22216796875
Rank 0, Epoch 4, Batch 253, Loss: 1240.147461
Gradient norm: 2686.911865234375
Rank 0, Epoch 4, Batch 254, Loss: 2350.522461
Gradient norm: 2653.316162109375
Rank 0, Epoch 4, Batch 255, Loss: 2956.431396
Gradient norm: 2320.822265625
Rank 0, Epoch 4, Batch 256, Loss: 3763.078613
Gradient norm: 2495.9892578125
Rank 0, Epoch 4, Batch 257, Loss: 3052.206543
Gradient norm: 2778.893310546875
Rank 0, Epoch 4, Batch 258, Loss: 3794.015137
Gradient norm: 2625.353515625
Rank 0, Epoch 4, Batch 259, Loss: 3687.670166
Gradient norm: 2698.734130859375
Rank 0, Epoch 4, Batch 260, Loss: 4288.482422
Rank 0, Epoch 4, Val Loss: 1167.5741, Val Acc: 0.1000, Time: 196.97s
Rank 1, Epoch 4, Batch 211, Loss: 10172.435547
Gradient norm: 3970.367919921875
Rank 1, Epoch 4, Batch 212, Loss: 9540.337891
Gradient norm: 3962.251953125
Rank 1, Epoch 4, Batch 213, Loss: 13278.076172
Gradient norm: 4312.93701171875
Rank 1, Epoch 4, Batch 214, Loss: 6369.417480
Gradient norm: 4173.9033203125
Rank 1, Epoch 4, Batch 215, Loss: 7428.302734
Gradient norm: 4378.13232421875
Rank 1, Epoch 4, Batch 216, Loss: 6077.076172
Gradient norm: 4062.32568359375
Rank 1, Epoch 4, Batch 217, Loss: 8977.839844
Gradient norm: 4454.0546875
Rank 1, Epoch 4, Batch 218, Loss: 10517.145508
Gradient norm: 3982.99560546875
Rank 1, Epoch 4, Batch 219, Loss: 7259.995605
Gradient norm: 4272.5908203125
Rank 1, Epoch 4, Batch 220, Loss: 11242.438477
Gradient norm: 4404.1787109375
Rank 1, Epoch 4, Batch 221, Loss: 10332.526367
Gradient norm: 4416.20458984375
Rank 1, Epoch 4, Batch 222, Loss: 8928.597656
Gradient norm: 3973.829345703125
Rank 1, Epoch 4, Batch 223, Loss: 9583.699219
Gradient norm: 4666.0009765625
Rank 1, Epoch 4, Batch 224, Loss: 5471.758789
Gradient norm: 4327.56640625
Rank 1, Epoch 4, Batch 225, Loss: 6446.114258
Gradient norm: 4572.333984375
Rank 1, Epoch 4, Batch 226, Loss: 8196.803711
Gradient norm: 3808.73046875
Rank 1, Epoch 4, Batch 227, Loss: 8653.919922
Gradient norm: 4419.14208984375
Rank 1, Epoch 4, Batch 228, Loss: 11984.018555
Gradient norm: 4319.42626953125
Rank 1, Epoch 4, Batch 229, Loss: 9996.705078
Gradient norm: 4396.94287109375
Rank 1, Epoch 4, Batch 230, Loss: 10845.663086
Gradient norm: 4388.01318359375
Rank 1, Epoch 4, Batch 231, Loss: 9993.417969
Gradient norm: 4016.37109375
Rank 1, Epoch 4, Batch 232, Loss: 10600.819336
Gradient norm: 4306.94677734375
Rank 1, Epoch 4, Batch 233, Loss: 8507.271484
Gradient norm: 4578.994140625
Rank 1, Epoch 4, Batch 234, Loss: 4991.443359
Gradient norm: 4111.16552734375
Rank 1, Epoch 4, Batch 235, Loss: 8737.440430
Gradient norm: 4249.9697265625
Rank 1, Epoch 4, Batch 236, Loss: 8665.036133
Gradient norm: 4590.404296875
Rank 1, Epoch 4, Batch 237, Loss: 8958.302734
Gradient norm: 4302.916015625
Rank 1, Epoch 4, Batch 238, Loss: 8361.977539
Gradient norm: 4387.1044921875
Rank 1, Epoch 4, Batch 239, Loss: 10022.149414
Gradient norm: 4481.853515625
Rank 1, Epoch 4, Batch 240, Loss: 7460.375000
Gradient norm: 4427.56591796875
Rank 1, Epoch 4, Batch 241, Loss: 10893.411133
Gradient norm: 4674.26123046875
Rank 1, Epoch 4, Batch 242, Loss: 14730.432617
Gradient norm: 4546.650390625
Rank 1, Epoch 4, Batch 243, Loss: 13993.310547
Gradient norm: 4407.51904296875
Rank 1, Epoch 4, Batch 244, Loss: 10454.759766
Gradient norm: 4337.3994140625
Rank 1, Epoch 4, Batch 245, Loss: 4018.821289
Gradient norm: 4818.2080078125
Rank 1, Epoch 4, Batch 246, Loss: 5471.680664
Gradient norm: 3838.32421875
Rank 1, Epoch 4, Batch 247, Loss: 7078.908203
Gradient norm: 4392.91748046875
Rank 1, Epoch 4, Batch 248, Loss: 10934.467773
Gradient norm: 4507.3076171875
Rank 1, Epoch 4, Batch 249, Loss: 10404.783203
Gradient norm: 4351.15673828125
Rank 1, Epoch 4, Batch 250, Loss: 8696.773438
Gradient norm: 5070.841796875
Rank 1, Epoch 4, Batch 251, Loss: 12252.715820
Gradient norm: 4415.1572265625
Rank 1, Epoch 4, Batch 252, Loss: 10453.760742
Gradient norm: 4785.5361328125
Rank 1, Epoch 4, Batch 253, Loss: 14984.108398
Gradient norm: 4463.3671875
Rank 1, Epoch 4, Batch 254, Loss: 14219.911133
Gradient norm: 4799.53515625
Rank 1, Epoch 4, Batch 255, Loss: 5436.535645
Gradient norm: 4807.02734375
Rank 1, Epoch 4, Batch 256, Loss: 12320.585938
Gradient norm: 4826.8349609375
Rank 1, Epoch 4, Batch 257, Loss: 10717.015625
Gradient norm: 4852.873046875
Rank 1, Epoch 4, Batch 258, Loss: 10575.207031
Gradient norm: 4228.7509765625
Rank 1, Epoch 4, Batch 259, Loss: 9204.111328
Gradient norm: 5054.10546875
Rank 1, Epoch 4, Batch 260, Loss: 9217.670898
Rank 1, Epoch 4, Val Loss: 96140.3455, Val Acc: 0.1000, Time: 198.19s
Gradient norm: 4614.13037109375
Rank 1, Epoch 5, Batch 0, Loss: 8070.200684
Gradient norm: 4369.6103515625
Rank 1, Epoch 5, Batch 1, Loss: 12839.062500
Gradient norm: 4775.52685546875
Rank 1, Epoch 5, Batch 2, Loss: 11219.148438
Gradient norm: 4334.20654296875
Rank 1, Epoch 5, Batch 3, Loss: 9903.514648
Gradient norm: 5128.09814453125
Rank 1, Epoch 5, Batch 4, Loss: 11649.059570
Gradient norm: 4153.4501953125
Rank 1, Epoch 5, Batch 5, Loss: 9715.371094
Gradient norm: 4485.92041015625
Rank 1, Epoch 5, Batch 6, Loss: 11620.921875
Gradient norm: 4892.1611328125
Rank 1, Epoch 5, Batch 7, Loss: 10428.009766
Gradient norm: 4764.07763671875
Rank 1, Epoch 5, Batch 8, Loss: 9433.537109
Gradient norm: 4441.0537109375
Rank 1, Epoch 5, Batch 9, Loss: 10346.972656
Gradient norm: 4861.7294921875
Rank 1, Epoch 5, Batch 10, Loss: 8217.039062
Gradient norm: 4889.97412109375
Rank 1, Epoch 5, Batch 11, Loss: 11402.242188
Gradient norm: 4742.2666015625
Rank 1, Epoch 5, Batch 12, Loss: 11724.521484
Gradient norm: 4531.37646484375
Rank 1, Epoch 5, Batch 13, Loss: 9016.377930
Gradient norm: 4225.86865234375
Rank 1, Epoch 5, Batch 14, Loss: 9508.780273
Gradient norm: 4436.45068359375
Rank 1, Epoch 5, Batch 15, Loss: 7594.415039
Gradient norm: 4560.0732421875
Rank 1, Epoch 5, Batch 16, Loss: 14632.472656
Gradient norm: 4692.259765625
Rank 1, Epoch 5, Batch 17, Loss: 14687.916016
Gradient norm: 4483.94921875
Rank 1, Epoch 5, Batch 18, Loss: 12436.890625
Gradient norm: 4619.1845703125
Rank 1, Epoch 5, Batch 19, Loss: 7389.672852
Gradient norm: 4798.6865234375
Rank 1, Epoch 5, Batch 20, Loss: 5680.255859
Gradient norm: 4675.603515625
Rank 1, Epoch 5, Batch 21, Loss: 8884.004883
Gradient norm: 4570.60595703125
Rank 1, Epoch 5, Batch 22, Loss: 8224.519531
Gradient norm: 4944.4541015625
Rank 1, Epoch 5, Batch 23, Loss: 9280.601562
Gradient norm: 4922.50390625
Rank 1, Epoch 5, Batch 24, Loss: 9981.281250
Gradient norm: 4772.39013671875
Rank 1, Epoch 5, Batch 25, Loss: 12665.517578
Gradient norm: 5006.39453125
Rank 1, Epoch 5, Batch 26, Loss: 16173.140625
Gradient norm: 4753.732421875
Rank 1, Epoch 5, Batch 27, Loss: 16144.416992
Gradient norm: 4918.01123046875
Rank 1, Epoch 5, Batch 28, Loss: 16287.847656
Gradient norm: 4329.123046875
Rank 1, Epoch 5, Batch 29, Loss: 5858.254395
Gradient norm: 4590.82958984375
Rank 1, Epoch 5, Batch 30, Loss: 8040.683594
Gradient norm: 4583.130859375
Rank 1, Epoch 5, Batch 31, Loss: 9801.464844
Gradient norm: 4929.33642578125
Rank 1, Epoch 5, Batch 32, Loss: 9492.464844
Gradient norm: 4975.88427734375
Rank 1, Epoch 5, Batch 33, Loss: 14021.838867
Gradient norm: 4961.75146484375
Rank 1, Epoch 5, Batch 34, Loss: 9947.957031
Gradient norm: 5126.125
Rank 1, Epoch 5, Batch 35, Loss: 11248.861328
Gradient norm: 4706.57666015625
Rank 1, Epoch 5, Batch 36, Loss: 13627.217773
Gradient norm: 4417.0234375
Rank 1, Epoch 5, Batch 37, Loss: 9122.180664
Gradient norm: 4826.74658203125
Rank 1, Epoch 5, Batch 38, Loss: 10060.661133
Gradient norm: 4503.55078125
Rank 1, Epoch 5, Batch 39, Loss: 13722.242188
Gradient norm: 5219.27392578125
Rank 1, Epoch 5, Batch 40, Loss: 11783.819336
Gradient norm: 4770.927734375
Rank 1, Epoch 5, Batch 41, Loss: 10514.535156
Gradient norm: 5018.0205078125
Rank 1, Epoch 5, Batch 42, Loss: 7032.484375
Gradient norm: 4376.7548828125
Rank 1, Epoch 5, Batch 43, Loss: 10209.840820
Gradient norm: 4510.0849609375
Rank 1, Epoch 5, Batch 44, Loss: 11616.219727
Gradient norm: 4877.35546875
Rank 1, Epoch 5, Batch 45, Loss: 12337.198242
Gradient norm: 4854.35400390625
Rank 1, Epoch 5, Batch 46, Loss: 13778.669922
Gradient norm: 4883.4150390625
Rank 1, Epoch 5, Batch 47, Loss: 8063.755859
Gradient norm: 5032.490234375
Rank 1, Epoch 5, Batch 48, Loss: 10761.523438
Gradient norm: 4850.5810546875
Rank 1, Epoch 5, Batch 49, Loss: 13507.812500
Gradient norm: 4875.5302734375
Rank 1, Epoch 5, Batch 50, Loss: 13521.996094
Gradient norm: 5121.87548828125
Rank 1, Epoch 5, Batch 51, Loss: 18061.097656
Gradient norm: 4903.57177734375
Rank 1, Epoch 5, Batch 52, Loss: 8518.007812
Gradient norm: 4532.67138671875
Rank 1, Epoch 5, Batch 53, Loss: 10009.523438
Gradient norm: 4519.26416015625
Rank 1, Epoch 5, Batch 54, Loss: 7633.849609
Gradient norm: 5040.4248046875
Rank 1, Epoch 5, Batch 55, Loss: 9521.384766
Gradient norm: 5136.501953125
Rank 1, Epoch 5, Batch 56, Loss: 8539.989258
Gradient norm: 5217.40283203125
Rank 1, Epoch 5, Batch 57, Loss: 11238.907227
Gradient norm: 4722.25390625
Rank 1, Epoch 5, Batch 58, Loss: 11409.089844
Gradient norm: 4819.24609375
Rank 1, Epoch 5, Batch 59, Loss: 16231.164062
Gradient norm: 4857.86865234375
Rank 1, Epoch 5, Batch 60, Loss: 9962.982422
Gradient norm: 4572.6787109375
Rank 1, Epoch 5, Batch 61, Loss: 18298.808594
Gradient norm: 5021.25537109375
Rank 1, Epoch 5, Batch 62, Loss: 11722.133789
Gradient norm: 4843.08984375
Rank 1, Epoch 5, Batch 63, Loss: 13677.894531
Gradient norm: 5052.3720703125
Rank 1, Epoch 5, Batch 64, Loss: 8520.320312
Gradient norm: 4698.3740234375
Rank 1, Epoch 5, Batch 65, Loss: 9725.876953
Gradient norm: 5348.15771484375
Rank 1, Epoch 5, Batch 66, Loss: 10290.908203
Gradient norm: 5017.654296875
Rank 1, Epoch 5, Batch 67, Loss: 10818.088867
Gradient norm: 4749.2138671875
Rank 1, Epoch 5, Batch 68, Loss: 14750.774414
Gradient norm: 4671.03173828125
Rank 1, Epoch 5, Batch 69, Loss: 11039.265625
Gradient norm: 5294.8271484375
Rank 1, Epoch 5, Batch 70, Loss: 12130.355469
Gradient norm: 5141.0224609375
Rank 1, Epoch 5, Batch 71, Loss: 12538.703125
Gradient norm: 5334.19384765625
Rank 1, Epoch 5, Batch 72, Loss: 14501.570312
Gradient norm: 5004.92333984375
Rank 1, Epoch 5, Batch 73, Loss: 15038.769531
Gradient norm: 4953.29248046875
Rank 1, Epoch 5, Batch 74, Loss: 11220.270508
Gradient norm: 5030.9892578125
Rank 1, Epoch 5, Batch 75, Loss: 10862.611328
Gradient norm: 4615.498046875
Rank 1, Epoch 5, Batch 76, Loss: 12268.968750
Gradient norm: 5300.39990234375
Rank 1, Epoch 5, Batch 77, Loss: 10425.843750
Gradient norm: 5097.7314453125
Rank 1, Epoch 5, Batch 78, Loss: 12537.093750
Gradient norm: 5473.21533203125
Rank 1, Epoch 5, Batch 79, Loss: 14869.855469
Gradient norm: 4939.2880859375
Rank 1, Epoch 5, Batch 80, Loss: 9511.409180
Gradient norm: 4944.95361328125
Rank 1, Epoch 5, Batch 81, Loss: 14451.624023
Gradient norm: 4510.146484375
Rank 1, Epoch 5, Batch 82, Loss: 15648.640625
Gradient norm: 5397.3115234375
Rank 1, Epoch 5, Batch 83, Loss: 11738.201172
Gradient norm: 4721.96484375
Rank 1, Epoch 5, Batch 84, Loss: 13926.115234
Gradient norm: 5048.1357421875
Rank 1, Epoch 5, Batch 85, Loss: 8925.582031
Gradient norm: 4901.26416015625
Rank 1, Epoch 5, Batch 86, Loss: 8407.072266
Gradient norm: 5571.96533203125
Rank 1, Epoch 5, Batch 87, Loss: 11269.515625
Gradient norm: 4758.22314453125
Rank 1, Epoch 5, Batch 88, Loss: 10600.255859
Gradient norm: 5139.9404296875
Rank 1, Epoch 5, Batch 89, Loss: 11521.525391
Gradient norm: 5431.06201171875
Rank 1, Epoch 5, Batch 90, Loss: 14656.027344
Gradient norm: 4803.24462890625
Rank 1, Epoch 5, Batch 91, Loss: 15634.558594
Gradient norm: 4876.28125
Rank 1, Epoch 5, Batch 92, Loss: 15363.583984
Gradient norm: 5256.8720703125
Rank 1, Epoch 5, Batch 93, Loss: 19009.033203
Gradient norm: 5254.09716796875
Rank 1, Epoch 5, Batch 94, Loss: 7971.966797
Gradient norm: 5325.47509765625
Rank 1, Epoch 5, Batch 95, Loss: 9137.166016
Gradient norm: 5272.7060546875
Rank 1, Epoch 5, Batch 96, Loss: 12312.429688
Gradient norm: 4928.0625
Rank 1, Epoch 5, Batch 97, Loss: 9726.078125
Gradient norm: 5168.8837890625
Rank 1, Epoch 5, Batch 98, Loss: 12955.965820
Gradient norm: 5574.9345703125
Rank 1, Epoch 5, Batch 99, Loss: 18937.832031
Gradient norm: 5015.2041015625
Rank 1, Epoch 5, Batch 100, Loss: 15089.887695
Gradient norm: 5483.95068359375
Rank 1, Epoch 5, Batch 101, Loss: 16716.796875
Gradient norm: 5364.89013671875
Rank 1, Epoch 5, Batch 102, Loss: 11497.052734
Gradient norm: 5560.98046875
Rank 1, Epoch 5, Batch 103, Loss: 15748.832031
Gradient norm: 4475.72412109375
Rank 1, Epoch 5, Batch 104, Loss: 10128.686523
Gradient norm: 5202.56591796875
Rank 1, Epoch 5, Batch 105, Loss: 12299.062500
Gradient norm: 5237.962890625
Gradient norm: 2225.402587890625
Rank 0, Epoch 5, Batch 0, Loss: 2406.765381
Gradient norm: 2687.52197265625
Rank 0, Epoch 5, Batch 1, Loss: 2914.935547
Gradient norm: 2329.79736328125
Rank 0, Epoch 5, Batch 2, Loss: 2030.663330
Gradient norm: 2672.54638671875
Rank 0, Epoch 5, Batch 3, Loss: 3105.678467
Gradient norm: 2701.196044921875
Rank 0, Epoch 5, Batch 4, Loss: 3905.955811
Gradient norm: 2311.971435546875
Rank 0, Epoch 5, Batch 5, Loss: 3130.356201
Gradient norm: 2443.146484375
Rank 0, Epoch 5, Batch 6, Loss: 3802.007812
Gradient norm: 2714.74072265625
Rank 0, Epoch 5, Batch 7, Loss: 2971.040527
Gradient norm: 2654.642822265625
Rank 0, Epoch 5, Batch 8, Loss: 3260.658203
Gradient norm: 2400.773193359375
Rank 0, Epoch 5, Batch 9, Loss: 3221.759033
Gradient norm: 2766.3427734375
Rank 0, Epoch 5, Batch 10, Loss: 3493.198242
Gradient norm: 2647.47314453125
Rank 0, Epoch 5, Batch 11, Loss: 2746.468750
Gradient norm: 2435.28955078125
Rank 0, Epoch 5, Batch 12, Loss: 3028.744629
Gradient norm: 2713.310546875
Rank 0, Epoch 5, Batch 13, Loss: 2781.314453
Gradient norm: 2730.0234375
Rank 0, Epoch 5, Batch 14, Loss: 3666.550781
Gradient norm: 2652.998779296875
Rank 0, Epoch 5, Batch 15, Loss: 4016.183594
Gradient norm: 2527.375732421875
Rank 0, Epoch 5, Batch 16, Loss: 3448.077881
Gradient norm: 2615.446533203125
Rank 0, Epoch 5, Batch 17, Loss: 4771.270996
Gradient norm: 2634.497802734375
Rank 0, Epoch 5, Batch 18, Loss: 3208.122070
Gradient norm: 2673.96484375
Rank 0, Epoch 5, Batch 19, Loss: 2554.301758
Gradient norm: 2521.22900390625
Rank 0, Epoch 5, Batch 20, Loss: 3463.312012
Gradient norm: 2647.141845703125
Rank 0, Epoch 5, Batch 21, Loss: 2567.244141
Gradient norm: 2844.26123046875
Rank 0, Epoch 5, Batch 22, Loss: 2770.301025
Gradient norm: 2547.823974609375
Rank 0, Epoch 5, Batch 23, Loss: 2765.381836
Gradient norm: 2817.0390625
Rank 0, Epoch 5, Batch 24, Loss: 3880.322754
Gradient norm: 2565.36328125
Rank 0, Epoch 5, Batch 25, Loss: 3708.293945
Gradient norm: 2740.375244140625
Rank 0, Epoch 5, Batch 26, Loss: 4347.782715
Gradient norm: 2598.201416015625
Rank 0, Epoch 5, Batch 27, Loss: 4356.901367
Gradient norm: 2808.19873046875
Rank 0, Epoch 5, Batch 28, Loss: 4150.086914
Gradient norm: 2651.782958984375
Rank 0, Epoch 5, Batch 29, Loss: 3504.770508
Gradient norm: 2393.685302734375
Rank 0, Epoch 5, Batch 30, Loss: 3079.975098
Gradient norm: 2642.376953125
Rank 0, Epoch 5, Batch 31, Loss: 2249.209717
Gradient norm: 2778.697265625
Rank 0, Epoch 5, Batch 32, Loss: 3334.736328
Gradient norm: 2413.159423828125
Rank 0, Epoch 5, Batch 33, Loss: 2299.072510
Gradient norm: 2569.01318359375
Rank 0, Epoch 5, Batch 34, Loss: 3569.415771
Gradient norm: 2703.443603515625
Rank 0, Epoch 5, Batch 35, Loss: 2656.054688
Gradient norm: 2865.501708984375
Rank 0, Epoch 5, Batch 36, Loss: 3842.438965
Gradient norm: 2562.0126953125
Rank 0, Epoch 5, Batch 37, Loss: 3807.270020
Gradient norm: 2642.625
Rank 0, Epoch 5, Batch 38, Loss: 4500.614258
Gradient norm: 2803.98583984375
Rank 0, Epoch 5, Batch 39, Loss: 4075.661133
Gradient norm: 2775.26171875
Rank 0, Epoch 5, Batch 40, Loss: 4156.465332
Gradient norm: 2805.73779296875
Rank 0, Epoch 5, Batch 41, Loss: 2757.220703
Gradient norm: 2694.260498046875
Rank 0, Epoch 5, Batch 42, Loss: 3356.173828
Gradient norm: 2681.954345703125
Rank 0, Epoch 5, Batch 43, Loss: 2283.676758
Gradient norm: 2483.938720703125
Rank 0, Epoch 5, Batch 44, Loss: 3381.549072
Gradient norm: 2669.821533203125
Rank 0, Epoch 5, Batch 45, Loss: 3645.325928
Gradient norm: 2783.464599609375
Rank 0, Epoch 5, Batch 46, Loss: 4478.252930
Gradient norm: 2820.29736328125
Rank 0, Epoch 5, Batch 47, Loss: 3133.964844
Gradient norm: 2656.234130859375
Rank 0, Epoch 5, Batch 48, Loss: 4164.512695
Gradient norm: 2682.129638671875
Rank 0, Epoch 5, Batch 49, Loss: 3181.170898
Gradient norm: 2847.886962890625
Rank 0, Epoch 5, Batch 50, Loss: 3364.443359
Gradient norm: 2658.041259765625
Rank 0, Epoch 5, Batch 51, Loss: 3070.032715
Gradient norm: 2878.533203125
Rank 0, Epoch 5, Batch 52, Loss: 3893.152832
Gradient norm: 2514.851318359375
Rank 0, Epoch 5, Batch 53, Loss: 3964.599121
Gradient norm: 2631.349853515625
Rank 0, Epoch 5, Batch 54, Loss: 5548.081055
Gradient norm: 2783.43017578125
Rank 0, Epoch 5, Batch 55, Loss: 3238.901855
Gradient norm: 2516.1318359375
Rank 0, Epoch 5, Batch 56, Loss: 2324.978516
Gradient norm: 2738.340087890625
Rank 0, Epoch 5, Batch 57, Loss: 2565.448242
Gradient norm: 2791.068115234375
Rank 0, Epoch 5, Batch 58, Loss: 3141.085693
Gradient norm: 2783.558837890625
Rank 0, Epoch 5, Batch 59, Loss: 3954.077637
Gradient norm: 3010.7841796875
Rank 0, Epoch 5, Batch 60, Loss: 4094.462402
Gradient norm: 2828.8681640625
Rank 0, Epoch 5, Batch 61, Loss: 4546.216797
Gradient norm: 2808.740478515625
Rank 0, Epoch 5, Batch 62, Loss: 4616.229004
Gradient norm: 2700.669921875
Rank 0, Epoch 5, Batch 63, Loss: 3483.042969
Gradient norm: 2802.09716796875
Rank 0, Epoch 5, Batch 64, Loss: 2717.784668
Gradient norm: 2616.9052734375
Rank 0, Epoch 5, Batch 65, Loss: 3821.660156
Gradient norm: 2863.235107421875
Rank 0, Epoch 5, Batch 66, Loss: 3424.344238
Gradient norm: 2768.72900390625
Rank 0, Epoch 5, Batch 67, Loss: 4408.189941
Gradient norm: 2573.220947265625
Rank 0, Epoch 5, Batch 68, Loss: 4717.222656
Gradient norm: 2788.082275390625
Rank 0, Epoch 5, Batch 69, Loss: 4019.901367
Gradient norm: 2957.84326171875
Rank 0, Epoch 5, Batch 70, Loss: 2768.358154
Gradient norm: 3046.663818359375
Rank 0, Epoch 5, Batch 71, Loss: 2643.960449
Gradient norm: 2669.925537109375
Rank 0, Epoch 5, Batch 72, Loss: 2330.273438
Gradient norm: 2991.82080078125
Rank 0, Epoch 5, Batch 73, Loss: 4317.648926
Gradient norm: 2830.5498046875
Rank 0, Epoch 5, Batch 74, Loss: 4983.486328
Gradient norm: 2595.411376953125
Rank 0, Epoch 5, Batch 75, Loss: 4188.855957
Gradient norm: 2799.681640625
Rank 0, Epoch 5, Batch 76, Loss: 4297.871094
Gradient norm: 3027.493408203125
Rank 0, Epoch 5, Batch 77, Loss: 5348.095215
Gradient norm: 2926.93408203125
Rank 0, Epoch 5, Batch 78, Loss: 5399.594727
Gradient norm: 2917.5380859375
Rank 0, Epoch 5, Batch 79, Loss: 3503.983154
Gradient norm: 2649.800537109375
Rank 0, Epoch 5, Batch 80, Loss: 2094.474609
Gradient norm: 2928.23095703125
Rank 0, Epoch 5, Batch 81, Loss: 2831.995117
Gradient norm: 2987.827392578125
Rank 0, Epoch 5, Batch 82, Loss: 3622.122559
Gradient norm: 2965.688720703125
Rank 0, Epoch 5, Batch 83, Loss: 4674.996094
Gradient norm: 2679.318359375
Rank 0, Epoch 5, Batch 84, Loss: 4266.666992
Gradient norm: 2771.75244140625
Rank 0, Epoch 5, Batch 85, Loss: 5001.500000
Gradient norm: 2858.0810546875
Rank 0, Epoch 5, Batch 86, Loss: 4823.379883
Gradient norm: 2680.93603515625
Rank 0, Epoch 5, Batch 87, Loss: 3306.211182
Gradient norm: 3134.263427734375
Rank 0, Epoch 5, Batch 88, Loss: 5123.899902
Gradient norm: 3067.779052734375
Rank 0, Epoch 5, Batch 89, Loss: 2565.010986
Gradient norm: 2789.634765625
Rank 0, Epoch 5, Batch 90, Loss: 3774.330566
Gradient norm: 2938.48681640625
Rank 0, Epoch 5, Batch 91, Loss: 3067.621338
Gradient norm: 3004.697265625
Rank 0, Epoch 5, Batch 92, Loss: 3383.661133
Gradient norm: 2816.210693359375
Rank 0, Epoch 5, Batch 93, Loss: 3446.984863
Gradient norm: 2475.693359375
Rank 0, Epoch 5, Batch 94, Loss: 3702.660156
Gradient norm: 2926.900390625
Rank 0, Epoch 5, Batch 95, Loss: 5805.866699
Gradient norm: 2886.393310546875
Rank 0, Epoch 5, Batch 96, Loss: 5520.986328
Gradient norm: 3007.232666015625
Rank 0, Epoch 5, Batch 97, Loss: 4850.554688
Gradient norm: 3075.5634765625
Rank 0, Epoch 5, Batch 98, Loss: 2799.485840
Gradient norm: 2989.672119140625
Rank 0, Epoch 5, Batch 99, Loss: 2725.237305
Gradient norm: 3065.47216796875
Rank 0, Epoch 5, Batch 100, Loss: 3004.481445
Gradient norm: 2904.625732421875
Rank 0, Epoch 5, Batch 101, Loss: 4488.675781
Gradient norm: 2945.08642578125
Rank 0, Epoch 5, Batch 102, Loss: 4067.228760
Gradient norm: 2866.435546875
Rank 0, Epoch 5, Batch 103, Loss: 5096.063477
Gradient norm: 3207.0087890625
Rank 0, Epoch 5, Batch 104, Loss: 6898.200195
Gradient norm: 3102.550048828125
Rank 0, Epoch 5, Batch 105, Loss: 5172.942383
Gradient norm: 3077.150390625
Gradient norm: 2755.02099609375
Rank 2, Epoch 5, Batch 0, Loss: 4110.614746
Gradient norm: 2499.1826171875
Rank 2, Epoch 5, Batch 1, Loss: 3355.197266
Gradient norm: 2714.7578125
Rank 2, Epoch 5, Batch 2, Loss: 4616.693848
Gradient norm: 2729.11962890625
Rank 2, Epoch 5, Batch 3, Loss: 3666.399658
Gradient norm: 2693.029296875
Rank 2, Epoch 5, Batch 4, Loss: 3972.720703
Gradient norm: 2595.66455078125
Rank 2, Epoch 5, Batch 5, Loss: 4795.796875
Gradient norm: 2721.723876953125
Rank 2, Epoch 5, Batch 6, Loss: 3431.165527
Gradient norm: 2534.77587890625
Rank 2, Epoch 5, Batch 7, Loss: 3015.919922
Gradient norm: 2531.873046875
Rank 2, Epoch 5, Batch 8, Loss: 2552.617676
Gradient norm: 2552.473876953125
Rank 2, Epoch 5, Batch 9, Loss: 3064.655029
Gradient norm: 2655.491455078125
Rank 2, Epoch 5, Batch 10, Loss: 2445.475342
Gradient norm: 2302.202880859375
Rank 2, Epoch 5, Batch 11, Loss: 2576.454102
Gradient norm: 2607.16650390625
Rank 2, Epoch 5, Batch 12, Loss: 3080.091797
Gradient norm: 2750.70263671875
Rank 2, Epoch 5, Batch 13, Loss: 3799.900879
Gradient norm: 2531.476318359375
Rank 2, Epoch 5, Batch 14, Loss: 2725.602051
Gradient norm: 2525.693603515625
Rank 2, Epoch 5, Batch 15, Loss: 4804.744141
Gradient norm: 2737.273681640625
Rank 2, Epoch 5, Batch 16, Loss: 4312.452637
Gradient norm: 2700.428955078125
Rank 2, Epoch 5, Batch 17, Loss: 2707.331055
Gradient norm: 2752.806396484375
Rank 2, Epoch 5, Batch 18, Loss: 2560.319336
Gradient norm: 2819.8828125
Rank 2, Epoch 5, Batch 19, Loss: 2528.437012
Gradient norm: 2555.551513671875
Rank 2, Epoch 5, Batch 20, Loss: 3008.046875
Gradient norm: 2703.924560546875
Rank 2, Epoch 5, Batch 21, Loss: 3657.027832
Gradient norm: 2503.536865234375
Rank 2, Epoch 5, Batch 22, Loss: 3809.660156
Gradient norm: 2799.23291015625
Rank 2, Epoch 5, Batch 23, Loss: 5093.799316
Gradient norm: 2624.03564453125
Rank 2, Epoch 5, Batch 24, Loss: 4897.917969
Gradient norm: 2700.802001953125
Rank 2, Epoch 5, Batch 25, Loss: 3591.579834
Gradient norm: 2660.969970703125
Rank 2, Epoch 5, Batch 26, Loss: 2984.122803
Gradient norm: 2730.33837890625
Rank 2, Epoch 5, Batch 27, Loss: 2374.826904
Gradient norm: 2858.864501953125
Rank 2, Epoch 5, Batch 28, Loss: 2311.352051
Gradient norm: 2684.391357421875
Rank 2, Epoch 5, Batch 29, Loss: 2962.334961
Gradient norm: 2469.310791015625
Rank 2, Epoch 5, Batch 30, Loss: 2821.968262
Gradient norm: 2538.94873046875
Rank 2, Epoch 5, Batch 31, Loss: 2921.301025
Gradient norm: 2824.66796875
Rank 2, Epoch 5, Batch 32, Loss: 3290.085205
Gradient norm: 2713.475341796875
Rank 2, Epoch 5, Batch 33, Loss: 3800.342529
Gradient norm: 2685.81689453125
Rank 2, Epoch 5, Batch 34, Loss: 4035.535156
Gradient norm: 2249.5712890625
Rank 2, Epoch 5, Batch 35, Loss: 4981.847656
Gradient norm: 2672.35400390625
Rank 2, Epoch 5, Batch 36, Loss: 5162.169922
Gradient norm: 2470.826416015625
Rank 2, Epoch 5, Batch 37, Loss: 1996.610840
Gradient norm: 2568.97802734375
Rank 2, Epoch 5, Batch 38, Loss: 2135.802246
Gradient norm: 2624.55126953125
Rank 2, Epoch 5, Batch 39, Loss: 2481.187744
Gradient norm: 2579.064453125
Rank 2, Epoch 5, Batch 40, Loss: 2721.563477
Gradient norm: 2693.557861328125
Rank 2, Epoch 5, Batch 41, Loss: 4355.971680
Gradient norm: 2668.036865234375
Rank 2, Epoch 5, Batch 42, Loss: 3562.901367
Gradient norm: 2869.632080078125
Rank 2, Epoch 5, Batch 43, Loss: 3887.054199
Gradient norm: 2926.5263671875
Rank 2, Epoch 5, Batch 44, Loss: 3485.866699
Gradient norm: 2792.0419921875
Rank 2, Epoch 5, Batch 45, Loss: 2955.944336
Gradient norm: 2899.503662109375
Rank 2, Epoch 5, Batch 46, Loss: 4247.013184
Gradient norm: 2365.66748046875
Rank 2, Epoch 5, Batch 47, Loss: 3548.981445
Gradient norm: 2791.442138671875
Rank 2, Epoch 5, Batch 48, Loss: 4233.189453
Gradient norm: 2769.82373046875
Rank 2, Epoch 5, Batch 49, Loss: 4255.762207
Gradient norm: 3005.077392578125
Rank 2, Epoch 5, Batch 50, Loss: 3445.327393
Gradient norm: 2984.59814453125
Rank 2, Epoch 5, Batch 51, Loss: 3645.142578
Gradient norm: 2681.19873046875
Rank 2, Epoch 5, Batch 52, Loss: 3430.956543
Gradient norm: 2820.491455078125
Rank 2, Epoch 5, Batch 53, Loss: 4824.853516
Gradient norm: 2892.143310546875
Rank 2, Epoch 5, Batch 54, Loss: 3755.557129
Gradient norm: 2952.68603515625
Rank 2, Epoch 5, Batch 55, Loss: 4342.270508
Gradient norm: 2966.346923828125
Rank 2, Epoch 5, Batch 56, Loss: 4276.352539
Gradient norm: 2787.716552734375
Rank 2, Epoch 5, Batch 57, Loss: 4355.688965
Gradient norm: 2531.44287109375
Rank 2, Epoch 5, Batch 58, Loss: 3592.824707
Gradient norm: 2644.56103515625
Rank 2, Epoch 5, Batch 59, Loss: 3580.315918
Gradient norm: 2470.25048828125
Rank 2, Epoch 5, Batch 60, Loss: 3300.739502
Gradient norm: 2825.072021484375
Rank 2, Epoch 5, Batch 61, Loss: 2540.568359
Gradient norm: 2619.985107421875
Rank 2, Epoch 5, Batch 62, Loss: 3230.555176
Gradient norm: 2772.511962890625
Rank 2, Epoch 5, Batch 63, Loss: 4551.014648
Gradient norm: 2785.297607421875
Rank 2, Epoch 5, Batch 64, Loss: 2187.438477
Gradient norm: 2530.319091796875
Rank 2, Epoch 5, Batch 65, Loss: 2772.380371
Gradient norm: 2798.783203125
Rank 2, Epoch 5, Batch 66, Loss: 4746.031250
Gradient norm: 2681.376953125
Rank 2, Epoch 5, Batch 67, Loss: 5207.705078
Gradient norm: 2764.70361328125
Rank 2, Epoch 5, Batch 68, Loss: 3744.280518
Gradient norm: 2770.781494140625
Rank 2, Epoch 5, Batch 69, Loss: 3369.467773
Gradient norm: 2929.227294921875
Rank 2, Epoch 5, Batch 70, Loss: 2716.106445
Gradient norm: 2878.700439453125
Rank 2, Epoch 5, Batch 71, Loss: 1889.153320
Gradient norm: 2884.146240234375
Rank 2, Epoch 5, Batch 72, Loss: 2700.046387
Gradient norm: 2742.9423828125
Rank 2, Epoch 5, Batch 73, Loss: 4042.009766
Gradient norm: 2568.72021484375
Rank 2, Epoch 5, Batch 74, Loss: 5154.737305
Gradient norm: 3024.386962890625
Rank 2, Epoch 5, Batch 75, Loss: 6807.756348
Gradient norm: 2822.912109375
Rank 2, Epoch 5, Batch 76, Loss: 3973.097168
Gradient norm: 2896.142333984375
Rank 2, Epoch 5, Batch 77, Loss: 4570.139160
Gradient norm: 2782.28857421875
Rank 2, Epoch 5, Batch 78, Loss: 2763.336914
Gradient norm: 2743.294189453125
Rank 2, Epoch 5, Batch 79, Loss: 3265.536865
Gradient norm: 2787.073974609375
Rank 2, Epoch 5, Batch 80, Loss: 4311.865234
Gradient norm: 2981.98681640625
Rank 2, Epoch 5, Batch 81, Loss: 4201.187500
Gradient norm: 2842.897705078125
Rank 2, Epoch 5, Batch 82, Loss: 3728.860840
Gradient norm: 2834.607421875
Rank 2, Epoch 5, Batch 83, Loss: 2890.704590
Gradient norm: 3053.6533203125
Rank 2, Epoch 5, Batch 84, Loss: 3262.043457
Gradient norm: 2715.60888671875
Rank 2, Epoch 5, Batch 85, Loss: 3700.785645
Gradient norm: 2453.002685546875
Rank 2, Epoch 5, Batch 86, Loss: 3744.359375
Gradient norm: 2610.236572265625
Rank 2, Epoch 5, Batch 87, Loss: 3887.375000
Gradient norm: 3057.89794921875
Rank 2, Epoch 5, Batch 88, Loss: 4829.523438
Gradient norm: 2834.230712890625
Rank 2, Epoch 5, Batch 89, Loss: 4330.028320
Gradient norm: 2924.452392578125
Rank 2, Epoch 5, Batch 90, Loss: 4408.814453
Gradient norm: 2674.7646484375
Rank 2, Epoch 5, Batch 91, Loss: 2040.973145
Gradient norm: 2999.700439453125
Rank 2, Epoch 5, Batch 92, Loss: 2996.774658
Gradient norm: 3008.118896484375
Rank 2, Epoch 5, Batch 93, Loss: 4473.951172
Gradient norm: 2849.56103515625
Rank 2, Epoch 5, Batch 94, Loss: 4629.749023
Gradient norm: 2635.986328125
Rank 2, Epoch 5, Batch 95, Loss: 4132.775391
Gradient norm: 3095.302001953125
Rank 2, Epoch 5, Batch 96, Loss: 5676.380859
Gradient norm: 3021.359619140625
Rank 2, Epoch 5, Batch 97, Loss: 4982.545410
Gradient norm: 2884.33935546875
Rank 2, Epoch 5, Batch 98, Loss: 2433.644531
Gradient norm: 2926.7705078125
Rank 2, Epoch 5, Batch 99, Loss: 3421.904053
Gradient norm: 3078.10205078125
Rank 2, Epoch 5, Batch 100, Loss: 4009.278320
Gradient norm: 3047.104736328125
Rank 2, Epoch 5, Batch 101, Loss: 4188.118652
Gradient norm: 3096.6708984375
Rank 2, Epoch 5, Batch 102, Loss: 4629.238281
Gradient norm: 2949.202392578125
Rank 2, Epoch 5, Batch 103, Loss: 4031.515625
Gradient norm: 3011.17041015625
Rank 2, Epoch 5, Batch 104, Loss: 5272.928711
Gradient norm: 2910.88916015625
Rank 2, Epoch 5, Batch 105, Loss: 6114.465820
Gradient norm: 3194.715576171875
Rank 2, Epoch 5, Batch 106, Loss: 5265.707031
Gradient norm: 2804.75244140625
Rank 2, Epoch 5, Batch 107, Loss: 3784.748535
Gradient norm: 3119.283935546875
Rank 2, Epoch 5, Batch 108, Loss: 5471.621582
Gradient norm: 3081.376708984375
Rank 2, Epoch 5, Batch 109, Loss: 3125.010254
Gradient norm: 2999.246337890625
Rank 2, Epoch 5, Batch 110, Loss: 2807.977539
Gradient norm: 3168.57470703125
Rank 2, Epoch 5, Batch 111, Loss: 3065.656250
Gradient norm: 3052.94140625
Rank 2, Epoch 5, Batch 112, Loss: 3819.876709
Gradient norm: 2820.5009765625
Rank 2, Epoch 5, Batch 113, Loss: 4627.555176
Gradient norm: 2915.14208984375
Rank 2, Epoch 5, Batch 114, Loss: 5226.637695
Gradient norm: 2913.9287109375
Rank 2, Epoch 5, Batch 115, Loss: 5548.755371
Gradient norm: 2674.58203125
Rank 2, Epoch 5, Batch 116, Loss: 5730.106445
Gradient norm: 3088.62744140625
Rank 2, Epoch 5, Batch 117, Loss: 3877.607910
Gradient norm: 2805.56298828125
Rank 2, Epoch 5, Batch 118, Loss: 3723.791016
Gradient norm: 3008.9345703125
Rank 2, Epoch 5, Batch 119, Loss: 2700.165283
Gradient norm: 3097.455810546875
Rank 2, Epoch 5, Batch 120, Loss: 4395.172852
Gradient norm: 2969.536376953125
Rank 2, Epoch 5, Batch 121, Loss: 3870.850098
Gradient norm: 3121.31640625
Rank 2, Epoch 5, Batch 122, Loss: 4864.630371
Gradient norm: 2763.724609375
Rank 2, Epoch 5, Batch 123, Loss: 5383.212402
Gradient norm: 3186.319580078125
Rank 2, Epoch 5, Batch 124, Loss: 4155.445312
Gradient norm: 2666.669189453125
Rank 2, Epoch 5, Batch 125, Loss: 3925.768066
Gradient norm: 3162.672119140625
Rank 2, Epoch 5, Batch 126, Loss: 5559.381836
Gradient norm: 3009.224365234375
Rank 2, Epoch 5, Batch 127, Loss: 4187.185059
Gradient norm: 2749.85986328125
Rank 2, Epoch 5, Batch 128, Loss: 3307.177246
Gradient norm: 3126.673095703125
Rank 2, Epoch 5, Batch 129, Loss: 3639.777832
Gradient norm: 3004.224609375
Rank 2, Epoch 5, Batch 130, Loss: 3846.754883
Gradient norm: 3215.60595703125
Rank 2, Epoch 5, Batch 131, Loss: 5127.872070
Gradient norm: 2992.911865234375
Rank 2, Epoch 5, Batch 132, Loss: 5310.763672
Gradient norm: 2988.5859375
Rank 2, Epoch 5, Batch 133, Loss: 4778.729492
Gradient norm: 3037.46826171875
Rank 2, Epoch 5, Batch 134, Loss: 5680.195801
Gradient norm: 3009.2890625
Rank 2, Epoch 5, Batch 135, Loss: 4331.163086
Gradient norm: 3222.9111328125
Rank 2, Epoch 5, Batch 136, Loss: 3476.183594
Gradient norm: 3119.14453125
Rank 2, Epoch 5, Batch 137, Loss: 3127.401367
Gradient norm: 2812.476318359375
Rank 2, Epoch 5, Batch 138, Loss: 4862.336914
Gradient norm: 3151.897216796875
Rank 2, Epoch 5, Batch 139, Loss: 5204.991211
Gradient norm: 3030.85546875
Rank 2, Epoch 5, Batch 140, Loss: 4660.346191
Gradient norm: 3194.070556640625
Rank 2, Epoch 5, Batch 141, Loss: 3649.061768
Gradient norm: 3067.5302734375
Rank 2, Epoch 5, Batch 142, Loss: 4743.537598
Gradient norm: 3287.548828125
Rank 2, Epoch 5, Batch 143, Loss: 6199.923828
Gradient norm: 3248.47900390625
Rank 2, Epoch 5, Batch 144, Loss: 6214.101562
Gradient norm: 3109.8212890625
Rank 2, Epoch 5, Batch 145, Loss: 5030.847656
Gradient norm: 2718.40673828125
Rank 2, Epoch 5, Batch 146, Loss: 4388.101562
Gradient norm: 2980.483642578125
Rank 2, Epoch 5, Batch 147, Loss: 3406.810303
Gradient norm: 2918.884521484375
Rank 2, Epoch 5, Batch 148, Loss: 4015.457275
Gradient norm: 3055.703125
Rank 2, Epoch 5, Batch 149, Loss: 3785.933838
Gradient norm: 2933.277099609375
Rank 2, Epoch 5, Batch 150, Loss: 4578.505859
Gradient norm: 3237.263427734375
Rank 2, Epoch 5, Batch 151, Loss: 3832.945801
Gradient norm: 3127.77197265625
Rank 2, Epoch 5, Batch 152, Loss: 4511.863770
Gradient norm: 3251.7060546875
Rank 2, Epoch 5, Batch 153, Loss: 5451.217773
Gradient norm: 3188.77978515625
Rank 2, Epoch 5, Batch 154, Loss: 5453.908691
Gradient norm: 3266.34521484375
Rank 2, Epoch 5, Batch 155, Loss: 5456.340820
Gradient norm: 3149.99072265625
Rank 2, Epoch 5, Batch 156, Loss: 3945.854980
Gradient norm: 3228.93994140625
Rank 2, Epoch 5, Batch 157, Loss: 5239.136719
Gradient norm: 3303.22607421875
Rank 2, Epoch 5, Batch 158, Loss: 5241.119141
Gradient norm: 2846.809326171875
Rank 2, Epoch 5, Batch 159, Loss: 5213.183105
Gradient norm: 3074.38427734375
Rank 2, Epoch 5, Batch 160, Loss: 5044.051270
Gradient norm: 2973.859375
Rank 2, Epoch 5, Batch 161, Loss: 5484.917480
Gradient norm: 3181.556640625
Rank 2, Epoch 5, Batch 162, Loss: 4112.159180
Gradient norm: 3180.7646484375
Rank 2, Epoch 5, Batch 163, Loss: 3547.819824
Gradient norm: 3352.30419921875
Rank 2, Epoch 5, Batch 164, Loss: 5777.927246
Gradient norm: 3233.431396484375
Rank 2, Epoch 5, Batch 165, Loss: 2553.177734
Gradient norm: 3051.00439453125
Rank 2, Epoch 5, Batch 166, Loss: 4027.675781
Gradient norm: 2957.91064453125
Rank 2, Epoch 5, Batch 167, Loss: 4866.873047
Gradient norm: 3302.79833984375
Rank 2, Epoch 5, Batch 168, Loss: 5076.800781
Gradient norm: 2845.3720703125
Rank 2, Epoch 5, Batch 169, Loss: 6128.147949
Gradient norm: 3347.53515625
Rank 2, Epoch 5, Batch 170, Loss: 6548.235352
Gradient norm: 3109.96142578125
Rank 2, Epoch 5, Batch 171, Loss: 3612.462158
Gradient norm: 3281.228515625
Rank 2, Epoch 5, Batch 172, Loss: 3994.919922
Gradient norm: 3270.24072265625
Rank 2, Epoch 5, Batch 173, Loss: 6508.363281
Gradient norm: 3297.01904296875
Rank 2, Epoch 5, Batch 174, Loss: 6122.963867
Gradient norm: 2918.37744140625
Rank 2, Epoch 5, Batch 175, Loss: 4278.873535
Gradient norm: 3220.32177734375
Rank 2, Epoch 5, Batch 176, Loss: 5068.541016
Gradient norm: 2975.843994140625
Rank 2, Epoch 5, Batch 177, Loss: 4293.291016
Gradient norm: 3075.3349609375
Rank 2, Epoch 5, Batch 178, Loss: 3663.353516
Gradient norm: 3388.31884765625
Rank 2, Epoch 5, Batch 179, Loss: 3373.301758
Gradient norm: 3136.499267578125
Rank 2, Epoch 5, Batch 180, Loss: 6046.854980
Gradient norm: 3063.688720703125
Rank 2, Epoch 5, Batch 181, Loss: 4309.870117
Gradient norm: 2914.0087890625
Rank 2, Epoch 5, Batch 182, Loss: 3894.618164
Gradient norm: 3211.29345703125
Rank 2, Epoch 5, Batch 183, Loss: 4084.079590
Gradient norm: 3428.429931640625
Rank 2, Epoch 5, Batch 184, Loss: 5851.529785
Gradient norm: 3174.712890625
Rank 2, Epoch 5, Batch 185, Loss: 7548.120117
Gradient norm: 3133.12744140625
Rank 2, Epoch 5, Batch 186, Loss: 4720.952637
Gradient norm: 3106.375
Rank 2, Epoch 5, Batch 187, Loss: 4944.037109
Gradient norm: 3230.63916015625
Rank 2, Epoch 5, Batch 188, Loss: 5823.176758
Gradient norm: 3411.48193359375
Rank 2, Epoch 5, Batch 189, Loss: 3701.755859
Gradient norm: 3496.798583984375
Rank 2, Epoch 5, Batch 190, Loss: 3839.201904
Gradient norm: 3202.803466796875
Rank 2, Epoch 5, Batch 191, Loss: 4903.738281
Gradient norm: 3110.4453125
Rank 2, Epoch 5, Batch 192, Loss: 5202.843750
Gradient norm: 3359.177734375
Rank 2, Epoch 5, Batch 193, Loss: 4835.888184
Gradient norm: 3492.24560546875
Rank 2, Epoch 5, Batch 194, Loss: 5357.295898
Gradient norm: 3404.819580078125
Rank 2, Epoch 5, Batch 195, Loss: 6545.650391
Gradient norm: 3222.712890625
Rank 2, Epoch 5, Batch 196, Loss: 5356.400879
Gradient norm: 3136.38427734375
Rank 2, Epoch 5, Batch 197, Loss: 6383.468262
Gradient norm: 3051.40234375
Rank 2, Epoch 5, Batch 198, Loss: 6065.064453
Gradient norm: 3175.798583984375
Rank 2, Epoch 5, Batch 199, Loss: 5579.583008
Gradient norm: 3154.012939453125
Rank 2, Epoch 5, Batch 200, Loss: 3666.027344
Gradient norm: 3411.950439453125
Rank 2, Epoch 5, Batch 201, Loss: 4648.753418
Gradient norm: 2935.0693359375
Rank 2, Epoch 5, Batch 202, Loss: 2972.934570
Gradient norm: 3376.248046875
Rank 2, Epoch 5, Batch 203, Loss: 4197.132812
Gradient norm: 3281.212646484375
Rank 2, Epoch 5, Batch 204, Loss: 4750.647949
Gradient norm: 3337.160400390625
Rank 2, Epoch 5, Batch 205, Loss: 5684.975586
Gradient norm: 3478.178466796875
Rank 2, Epoch 5, Batch 206, Loss: 5845.025879
Gradient norm: 3488.528564453125
Rank 2, Epoch 5, Batch 207, Loss: 4659.446289
Gradient norm: 3060.681884765625
Rank 2, Epoch 5, Batch 208, Loss: 5582.015625
Gradient norm: 3160.147705078125
Rank 2, Epoch 5, Batch 209, Loss: 7227.326172
Gradient norm: 3251.43896484375
Rank 2, Epoch 5, Batch 210, Loss: 7242.390625
Gradient norm: 3239.01171875
Rank 0, Epoch 5, Batch 106, Loss: 5378.278809
Gradient norm: 3078.228271484375
Rank 0, Epoch 5, Batch 107, Loss: 3091.515625
Gradient norm: 2879.938720703125
Rank 0, Epoch 5, Batch 108, Loss: 3866.301270
Gradient norm: 3153.025634765625
Rank 0, Epoch 5, Batch 109, Loss: 3990.695068
Gradient norm: 2687.32470703125
Rank 0, Epoch 5, Batch 110, Loss: 4121.052246
Gradient norm: 2919.789306640625
Rank 0, Epoch 5, Batch 111, Loss: 4023.132324
Gradient norm: 2884.393798828125
Rank 0, Epoch 5, Batch 112, Loss: 3972.060059
Gradient norm: 2959.045654296875
Rank 0, Epoch 5, Batch 113, Loss: 4576.580078
Gradient norm: 2838.83056640625
Rank 0, Epoch 5, Batch 114, Loss: 3480.705078
Gradient norm: 3169.4794921875
Rank 0, Epoch 5, Batch 115, Loss: 4651.638672
Gradient norm: 3060.648681640625
Rank 0, Epoch 5, Batch 116, Loss: 5466.752930
Gradient norm: 2892.990966796875
Rank 0, Epoch 5, Batch 117, Loss: 3844.668945
Gradient norm: 2736.992431640625
Rank 0, Epoch 5, Batch 118, Loss: 5533.148438
Gradient norm: 2912.6708984375
Rank 0, Epoch 5, Batch 119, Loss: 3927.442627
Gradient norm: 3172.333984375
Rank 0, Epoch 5, Batch 120, Loss: 3803.915771
Gradient norm: 2899.06494140625
Rank 0, Epoch 5, Batch 121, Loss: 2771.068115
Gradient norm: 2860.140380859375
Rank 0, Epoch 5, Batch 122, Loss: 3820.020508
Gradient norm: 2883.75830078125
Rank 0, Epoch 5, Batch 123, Loss: 3897.201416
Gradient norm: 2803.417724609375
Rank 0, Epoch 5, Batch 124, Loss: 3666.923828
Gradient norm: 2897.03125
Rank 0, Epoch 5, Batch 125, Loss: 5158.390625
Gradient norm: 2994.99072265625
Rank 0, Epoch 5, Batch 126, Loss: 5155.303711
Gradient norm: 2986.472900390625
Rank 0, Epoch 5, Batch 127, Loss: 4908.174805
Gradient norm: 3172.224365234375
Rank 0, Epoch 5, Batch 128, Loss: 4101.711426
Gradient norm: 3129.4150390625
Rank 0, Epoch 5, Batch 129, Loss: 4963.999512
Gradient norm: 2895.182861328125
Rank 0, Epoch 5, Batch 130, Loss: 3456.753418
Gradient norm: 3003.4453125
Rank 0, Epoch 5, Batch 131, Loss: 5474.495117
Gradient norm: 2900.2197265625
Rank 0, Epoch 5, Batch 132, Loss: 3589.736572
Gradient norm: 3178.85888671875
Rank 0, Epoch 5, Batch 133, Loss: 3680.159180
Gradient norm: 3195.00927734375
Rank 0, Epoch 5, Batch 134, Loss: 4891.176758
Gradient norm: 3248.488037109375
Rank 0, Epoch 5, Batch 135, Loss: 5178.430664
Gradient norm: 3193.054443359375
Rank 0, Epoch 5, Batch 136, Loss: 4291.070312
Gradient norm: 2949.879638671875
Rank 0, Epoch 5, Batch 137, Loss: 5100.436523
Gradient norm: 2579.794189453125
Rank 0, Epoch 5, Batch 138, Loss: 3821.398438
Gradient norm: 3190.954345703125
Rank 0, Epoch 5, Batch 139, Loss: 5302.350098
Gradient norm: 2869.21240234375
Rank 0, Epoch 5, Batch 140, Loss: 4645.497070
Gradient norm: 2983.91845703125
Rank 0, Epoch 5, Batch 141, Loss: 4147.173340
Gradient norm: 2864.5986328125
Rank 0, Epoch 5, Batch 142, Loss: 4047.637939
Gradient norm: 2924.596435546875
Rank 0, Epoch 5, Batch 143, Loss: 2927.388184
Gradient norm: 2874.112548828125
Rank 0, Epoch 5, Batch 144, Loss: 3435.150391
Gradient norm: 3301.8291015625
Rank 0, Epoch 5, Batch 145, Loss: 4875.267578
Gradient norm: 3130.97998046875
Rank 0, Epoch 5, Batch 146, Loss: 4740.333984
Gradient norm: 2828.074951171875
Rank 0, Epoch 5, Batch 147, Loss: 4963.395508
Gradient norm: 3082.301025390625
Rank 0, Epoch 5, Batch 148, Loss: 6290.347656
Gradient norm: 2959.451904296875
Rank 0, Epoch 5, Batch 149, Loss: 3733.206543
Gradient norm: 3028.9736328125
Rank 0, Epoch 5, Batch 150, Loss: 4495.209961
Gradient norm: 3417.961181640625
Rank 0, Epoch 5, Batch 151, Loss: 3409.100830
Gradient norm: 2845.0263671875
Rank 0, Epoch 5, Batch 152, Loss: 3391.219971
Gradient norm: 3267.491455078125
Rank 0, Epoch 5, Batch 153, Loss: 4856.640625
Gradient norm: 3047.806884765625
Rank 0, Epoch 5, Batch 154, Loss: 4845.493164
Gradient norm: 3110.29931640625
Rank 0, Epoch 5, Batch 155, Loss: 4147.364258
Gradient norm: 3165.26025390625
Rank 0, Epoch 5, Batch 156, Loss: 4240.808594
Gradient norm: 3359.515869140625
Rank 0, Epoch 5, Batch 157, Loss: 6414.884766
Gradient norm: 3165.152587890625
Rank 0, Epoch 5, Batch 158, Loss: 7267.127930
Gradient norm: 2928.84521484375
Rank 0, Epoch 5, Batch 159, Loss: 4416.824219
Gradient norm: 3044.307861328125
Rank 0, Epoch 5, Batch 160, Loss: 4443.856445
Gradient norm: 3197.109375
Rank 0, Epoch 5, Batch 161, Loss: 6497.706543
Gradient norm: 2965.510986328125
Rank 0, Epoch 5, Batch 162, Loss: 2718.372559
Gradient norm: 2692.3212890625
Rank 0, Epoch 5, Batch 163, Loss: 1518.535034
Gradient norm: 3068.966064453125
Rank 0, Epoch 5, Batch 164, Loss: 3637.851562
Gradient norm: 3222.32470703125
Rank 0, Epoch 5, Batch 165, Loss: 4461.764160
Gradient norm: 3165.37353515625
Rank 0, Epoch 5, Batch 166, Loss: 4926.155273
Gradient norm: 3341.825927734375
Rank 0, Epoch 5, Batch 167, Loss: 5937.665039
Gradient norm: 3144.151123046875
Rank 0, Epoch 5, Batch 168, Loss: 5624.014648
Gradient norm: 3295.83349609375
Rank 0, Epoch 5, Batch 169, Loss: 6116.532227
Gradient norm: 3047.661376953125
Rank 0, Epoch 5, Batch 170, Loss: 5027.590820
Gradient norm: 3243.269287109375
Rank 0, Epoch 5, Batch 171, Loss: 5660.989258
Gradient norm: 3186.919921875
Rank 0, Epoch 5, Batch 172, Loss: 4805.243164
Gradient norm: 3039.5712890625
Rank 0, Epoch 5, Batch 173, Loss: 5522.699219
Gradient norm: 3420.474853515625
Rank 0, Epoch 5, Batch 174, Loss: 3548.783691
Gradient norm: 3019.029052734375
Rank 0, Epoch 5, Batch 175, Loss: 3547.677246
Gradient norm: 3114.907470703125
Rank 0, Epoch 5, Batch 176, Loss: 4627.649414
Gradient norm: 3030.3134765625
Rank 0, Epoch 5, Batch 177, Loss: 4919.203125
Gradient norm: 3175.923583984375
Rank 0, Epoch 5, Batch 178, Loss: 3655.744629
Gradient norm: 3328.1748046875
Rank 0, Epoch 5, Batch 179, Loss: 5490.798828
Gradient norm: 3135.440673828125
Rank 0, Epoch 5, Batch 180, Loss: 4503.813477
Gradient norm: 3392.010009765625
Rank 0, Epoch 5, Batch 181, Loss: 4882.157227
Gradient norm: 3158.974853515625
Rank 0, Epoch 5, Batch 182, Loss: 6334.664551
Gradient norm: 2944.62841796875
Rank 0, Epoch 5, Batch 183, Loss: 6115.861816
Gradient norm: 3128.66162109375
Rank 0, Epoch 5, Batch 184, Loss: 5978.029297
Gradient norm: 3400.76806640625
Rank 0, Epoch 5, Batch 185, Loss: 4646.746582
Gradient norm: 3228.596435546875
Rank 0, Epoch 5, Batch 186, Loss: 5152.252441
Gradient norm: 3487.27001953125
Rank 0, Epoch 5, Batch 187, Loss: 5403.345703
Gradient norm: 3305.813720703125
Rank 0, Epoch 5, Batch 188, Loss: 3325.806885
Gradient norm: 3289.5810546875
Rank 0, Epoch 5, Batch 189, Loss: 3745.429688
Gradient norm: 3267.842041015625
Rank 0, Epoch 5, Batch 190, Loss: 4524.972656
Gradient norm: 3127.074462890625
Rank 0, Epoch 5, Batch 191, Loss: 5561.791992
Gradient norm: 2748.494384765625
Rank 0, Epoch 5, Batch 192, Loss: 5375.046875
Gradient norm: 3343.6474609375
Rank 0, Epoch 5, Batch 193, Loss: 6984.664551
Gradient norm: 3323.297119140625
Rank 0, Epoch 5, Batch 194, Loss: 5717.865723
Gradient norm: 3490.31591796875
Rank 0, Epoch 5, Batch 195, Loss: 4959.999512
Gradient norm: 3243.4228515625
Rank 0, Epoch 5, Batch 196, Loss: 3439.182861
Gradient norm: 3491.15771484375
Rank 0, Epoch 5, Batch 197, Loss: 4384.663086
Gradient norm: 3283.894775390625
Rank 0, Epoch 5, Batch 198, Loss: 5124.887695
Gradient norm: 3191.93017578125
Rank 0, Epoch 5, Batch 199, Loss: 6956.998047
Gradient norm: 3078.21923828125
Rank 0, Epoch 5, Batch 200, Loss: 5787.835938
Gradient norm: 3323.988037109375
Rank 0, Epoch 5, Batch 201, Loss: 6652.197754
Gradient norm: 3330.740966796875
Rank 0, Epoch 5, Batch 202, Loss: 5247.645508
Gradient norm: 3210.529296875
Rank 0, Epoch 5, Batch 203, Loss: 5078.312500
Gradient norm: 3537.150146484375
Rank 0, Epoch 5, Batch 204, Loss: 4347.887695
Gradient norm: 3552.529052734375
Rank 0, Epoch 5, Batch 205, Loss: 3884.688232
Gradient norm: 3195.11474609375
Rank 0, Epoch 5, Batch 206, Loss: 4391.386719
Gradient norm: 3334.1181640625
Rank 0, Epoch 5, Batch 207, Loss: 5680.729004
Gradient norm: 3429.924560546875
Rank 0, Epoch 5, Batch 208, Loss: 5891.765137
Gradient norm: 3502.756103515625
Rank 0, Epoch 5, Batch 209, Loss: 7041.617188
Gradient norm: 3401.336181640625
Rank 0, Epoch 5, Batch 210, Loss: 8834.488281
Rank 1, Epoch 5, Batch 106, Loss: 12644.750000
Gradient norm: 5366.97314453125
Rank 1, Epoch 5, Batch 107, Loss: 14222.385742
Gradient norm: 5588.7744140625
Rank 1, Epoch 5, Batch 108, Loss: 12150.436523
Gradient norm: 5511.56591796875
Rank 1, Epoch 5, Batch 109, Loss: 13769.167969
Gradient norm: 5408.208984375
Rank 1, Epoch 5, Batch 110, Loss: 12665.511719
Gradient norm: 5095.69677734375
Rank 1, Epoch 5, Batch 111, Loss: 14959.908203
Gradient norm: 4826.306640625
Rank 1, Epoch 5, Batch 112, Loss: 15227.916992
Gradient norm: 5202.46728515625
Rank 1, Epoch 5, Batch 113, Loss: 11511.136719
Gradient norm: 4765.873046875
Rank 1, Epoch 5, Batch 114, Loss: 13507.572266
Gradient norm: 4928.20947265625
Rank 1, Epoch 5, Batch 115, Loss: 13674.546875
Gradient norm: 5445.29443359375
Rank 1, Epoch 5, Batch 116, Loss: 13530.285156
Gradient norm: 4956.72900390625
Rank 1, Epoch 5, Batch 117, Loss: 9051.551758
Gradient norm: 5512.24462890625
Rank 1, Epoch 5, Batch 118, Loss: 14165.122070
Gradient norm: 5181.0380859375
Rank 1, Epoch 5, Batch 119, Loss: 13485.096680
Gradient norm: 5409.697265625
Rank 1, Epoch 5, Batch 120, Loss: 11868.271484
Gradient norm: 5347.16552734375
Rank 1, Epoch 5, Batch 121, Loss: 15502.031250
Gradient norm: 5681.6015625
Rank 1, Epoch 5, Batch 122, Loss: 13542.843750
Gradient norm: 5535.333984375
Rank 1, Epoch 5, Batch 123, Loss: 15649.089844
Gradient norm: 5179.7958984375
Rank 1, Epoch 5, Batch 124, Loss: 15965.690430
Gradient norm: 5448.68896484375
Rank 1, Epoch 5, Batch 125, Loss: 14546.517578
Gradient norm: 5572.99755859375
Rank 1, Epoch 5, Batch 126, Loss: 11175.996094
Gradient norm: 5373.05029296875
Rank 1, Epoch 5, Batch 127, Loss: 12829.183594
Gradient norm: 5759.14990234375
Rank 1, Epoch 5, Batch 128, Loss: 17367.986328
Gradient norm: 4964.595703125
Rank 1, Epoch 5, Batch 129, Loss: 13965.867188
Gradient norm: 5536.35107421875
Rank 1, Epoch 5, Batch 130, Loss: 15808.949219
Gradient norm: 5605.0517578125
Rank 1, Epoch 5, Batch 131, Loss: 10732.637695
Gradient norm: 5142.48046875
Rank 1, Epoch 5, Batch 132, Loss: 17410.980469
Gradient norm: 5682.1279296875
Rank 1, Epoch 5, Batch 133, Loss: 14638.393555
Gradient norm: 5317.64892578125
Rank 1, Epoch 5, Batch 134, Loss: 15903.543945
Gradient norm: 5099.06689453125
Rank 1, Epoch 5, Batch 135, Loss: 15070.559570
Gradient norm: 5151.32861328125
Rank 1, Epoch 5, Batch 136, Loss: 9133.083008
Gradient norm: 5319.21484375
Rank 1, Epoch 5, Batch 137, Loss: 14840.388672
Gradient norm: 5009.47705078125
Rank 1, Epoch 5, Batch 138, Loss: 11968.265625
Gradient norm: 5004.197265625
Rank 1, Epoch 5, Batch 139, Loss: 15628.109375
Gradient norm: 5457.70068359375
Rank 1, Epoch 5, Batch 140, Loss: 10898.837891
Gradient norm: 5684.6767578125
Rank 1, Epoch 5, Batch 141, Loss: 15683.244141
Gradient norm: 5607.10791015625
Rank 1, Epoch 5, Batch 142, Loss: 9871.125000
Gradient norm: 5388.66552734375
Rank 1, Epoch 5, Batch 143, Loss: 12437.688477
Gradient norm: 5303.06396484375
Rank 1, Epoch 5, Batch 144, Loss: 13223.476562
Gradient norm: 5552.36572265625
Rank 1, Epoch 5, Batch 145, Loss: 15226.975586
Gradient norm: 5279.798828125
Rank 1, Epoch 5, Batch 146, Loss: 15170.911133
Gradient norm: 5381.5576171875
Rank 1, Epoch 5, Batch 147, Loss: 16340.229492
Gradient norm: 5891.3408203125
Rank 1, Epoch 5, Batch 148, Loss: 24263.437500
Gradient norm: 5568.10888671875
Rank 1, Epoch 5, Batch 149, Loss: 18015.261719
Gradient norm: 5444.0849609375
Rank 1, Epoch 5, Batch 150, Loss: 10370.967773
Gradient norm: 5651.89111328125
Rank 1, Epoch 5, Batch 151, Loss: 9587.720703
Gradient norm: 5635.43896484375
Rank 1, Epoch 5, Batch 152, Loss: 12566.791016
Gradient norm: 5232.5361328125
Rank 1, Epoch 5, Batch 153, Loss: 11760.166016
Gradient norm: 5272.8115234375
Rank 1, Epoch 5, Batch 154, Loss: 12958.583984
Gradient norm: 5538.7216796875
Rank 1, Epoch 5, Batch 155, Loss: 14299.808594
Gradient norm: 5701.48779296875
Rank 1, Epoch 5, Batch 156, Loss: 19152.443359
Gradient norm: 6087.11279296875
Rank 1, Epoch 5, Batch 157, Loss: 17432.734375
Gradient norm: 5269.35009765625
Rank 1, Epoch 5, Batch 158, Loss: 15612.946289
Gradient norm: 5655.203125
Rank 1, Epoch 5, Batch 159, Loss: 16566.082031
Gradient norm: 5583.271484375
Rank 1, Epoch 5, Batch 160, Loss: 14769.237305
Gradient norm: 5228.9951171875
Rank 1, Epoch 5, Batch 161, Loss: 18216.062500
Gradient norm: 4292.0732421875
Rank 1, Epoch 5, Batch 162, Loss: 11544.044922
Gradient norm: 5500.966796875
Rank 1, Epoch 5, Batch 163, Loss: 13635.207031
Gradient norm: 5688.3046875
Rank 1, Epoch 5, Batch 164, Loss: 10412.780273
Gradient norm: 5868.77197265625
Rank 1, Epoch 5, Batch 165, Loss: 13213.774414
Gradient norm: 5688.26806640625
Rank 1, Epoch 5, Batch 166, Loss: 13636.880859
Gradient norm: 5669.91796875
Rank 1, Epoch 5, Batch 167, Loss: 14133.866211
Gradient norm: 6006.7275390625
Rank 1, Epoch 5, Batch 168, Loss: 19399.117188
Gradient norm: 5700.38330078125
Rank 1, Epoch 5, Batch 169, Loss: 18075.873047
Gradient norm: 5335.5634765625
Rank 1, Epoch 5, Batch 170, Loss: 16535.544922
Gradient norm: 6033.4873046875
Rank 1, Epoch 5, Batch 171, Loss: 19091.912109
Gradient norm: 5650.10205078125
Rank 1, Epoch 5, Batch 172, Loss: 19179.974609
Gradient norm: 5329.74365234375
Rank 1, Epoch 5, Batch 173, Loss: 12736.825195
Gradient norm: 5460.06640625
Rank 1, Epoch 5, Batch 174, Loss: 9849.642578
Gradient norm: 5000.0380859375
Rank 1, Epoch 5, Batch 175, Loss: 12073.583984
Gradient norm: 5871.55908203125
Rank 1, Epoch 5, Batch 176, Loss: 17390.843750
Gradient norm: 5764.619140625
Rank 1, Epoch 5, Batch 177, Loss: 16834.312500
Gradient norm: 5487.10693359375
Rank 1, Epoch 5, Batch 178, Loss: 15332.423828
Gradient norm: 5299.1572265625
Rank 1, Epoch 5, Batch 179, Loss: 14349.850586
Gradient norm: 5809.9853515625
Rank 1, Epoch 5, Batch 180, Loss: 10667.666992
Gradient norm: 5814.93017578125
Rank 1, Epoch 5, Batch 181, Loss: 13545.414062
Gradient norm: 5479.1171875
Rank 1, Epoch 5, Batch 182, Loss: 15363.082031
Gradient norm: 6050.17333984375
Rank 1, Epoch 5, Batch 183, Loss: 25414.734375
Gradient norm: 6170.66162109375
Rank 1, Epoch 5, Batch 184, Loss: 20775.796875
Gradient norm: 5915.28662109375
Rank 1, Epoch 5, Batch 185, Loss: 18852.794922
Gradient norm: 5657.60546875
Rank 1, Epoch 5, Batch 186, Loss: 8036.500488
Gradient norm: 5208.26953125
Rank 1, Epoch 5, Batch 187, Loss: 14332.908203
Gradient norm: 5520.5009765625
Rank 1, Epoch 5, Batch 188, Loss: 10707.848633
Gradient norm: 4791.53564453125
Rank 1, Epoch 5, Batch 189, Loss: 13630.601562
Gradient norm: 4992.24853515625
Rank 1, Epoch 5, Batch 190, Loss: 11898.371094
Gradient norm: 5557.72705078125
Rank 1, Epoch 5, Batch 191, Loss: 17427.587891
Gradient norm: 5635.7138671875
Rank 1, Epoch 5, Batch 192, Loss: 22098.412109
Gradient norm: 6061.36474609375
Rank 1, Epoch 5, Batch 193, Loss: 15321.311523
Gradient norm: 6267.9912109375
Rank 1, Epoch 5, Batch 194, Loss: 14305.257812
Gradient norm: 5188.26708984375
Rank 1, Epoch 5, Batch 195, Loss: 11793.171875
Gradient norm: 5675.38623046875
Rank 1, Epoch 5, Batch 196, Loss: 14738.426758
Gradient norm: 6343.68603515625
Rank 1, Epoch 5, Batch 197, Loss: 16158.298828
Gradient norm: 5743.921875
Rank 1, Epoch 5, Batch 198, Loss: 19086.255859
Gradient norm: 5949.37646484375
Rank 1, Epoch 5, Batch 199, Loss: 14999.329102
Gradient norm: 5559.99853515625
Rank 1, Epoch 5, Batch 200, Loss: 15266.619141
Gradient norm: 5960.005859375
Rank 1, Epoch 5, Batch 201, Loss: 16980.351562
Gradient norm: 6215.9267578125
Rank 1, Epoch 5, Batch 202, Loss: 18430.742188
Gradient norm: 5561.48779296875
Rank 1, Epoch 5, Batch 203, Loss: 17086.666016
Gradient norm: 5991.419921875
Rank 1, Epoch 5, Batch 204, Loss: 21674.349609
Gradient norm: 5846.2841796875
Rank 1, Epoch 5, Batch 205, Loss: 15319.967773
Gradient norm: 5698.54736328125
Rank 1, Epoch 5, Batch 206, Loss: 19005.974609
Gradient norm: 5788.9453125
Rank 1, Epoch 5, Batch 207, Loss: 15711.344727
Gradient norm: 6132.2138671875
Rank 1, Epoch 5, Batch 208, Loss: 16881.199219
Gradient norm: 5807.6845703125
Rank 1, Epoch 5, Batch 209, Loss: 14321.460938
Gradient norm: 5426.09423828125
Rank 1, Epoch 5, Batch 210, Loss: 12342.768555
Gradient norm: 5607.44091796875
Rank 2, Epoch 5, Batch 211, Loss: 5420.550781
Gradient norm: 3259.7744140625
Rank 2, Epoch 5, Batch 212, Loss: 5229.443848
Gradient norm: 3426.597412109375
Rank 2, Epoch 5, Batch 213, Loss: 2976.064209
Gradient norm: 3216.246826171875
Rank 2, Epoch 5, Batch 214, Loss: 3189.390625
Gradient norm: 3285.27392578125
Rank 2, Epoch 5, Batch 215, Loss: 3763.097168
Gradient norm: 3057.88037109375
Rank 2, Epoch 5, Batch 216, Loss: 4897.142578
Gradient norm: 3386.759765625
Rank 2, Epoch 5, Batch 217, Loss: 4940.115234
Gradient norm: 3302.603271484375
Rank 2, Epoch 5, Batch 218, Loss: 7169.092773
Gradient norm: 3512.532470703125
Rank 2, Epoch 5, Batch 219, Loss: 7986.676270
Gradient norm: 3335.541748046875
Rank 2, Epoch 5, Batch 220, Loss: 5442.761719
Gradient norm: 3287.96875
Rank 2, Epoch 5, Batch 221, Loss: 5840.310059
Gradient norm: 3165.673095703125
Rank 2, Epoch 5, Batch 222, Loss: 4923.010742
Gradient norm: 3317.020751953125
Rank 2, Epoch 5, Batch 223, Loss: 5494.503906
Gradient norm: 3065.074462890625
Rank 2, Epoch 5, Batch 224, Loss: 4972.249512
Gradient norm: 3287.7001953125
Rank 2, Epoch 5, Batch 225, Loss: 6324.122559
Gradient norm: 3319.133544921875
Rank 2, Epoch 5, Batch 226, Loss: 3968.829590
Gradient norm: 3181.106201171875
Rank 2, Epoch 5, Batch 227, Loss: 3557.348145
Gradient norm: 3353.803466796875
Rank 2, Epoch 5, Batch 228, Loss: 3556.836426
Gradient norm: 3173.12841796875
Rank 2, Epoch 5, Batch 229, Loss: 4542.693359
Gradient norm: 2988.3203125
Rank 2, Epoch 5, Batch 230, Loss: 5190.629883
Gradient norm: 3586.370849609375
Rank 2, Epoch 5, Batch 231, Loss: 6409.478516
Gradient norm: 3370.2080078125
Rank 2, Epoch 5, Batch 232, Loss: 6496.943359
Gradient norm: 3335.345458984375
Rank 2, Epoch 5, Batch 233, Loss: 6022.295898
Gradient norm: 3404.0751953125
Rank 2, Epoch 5, Batch 234, Loss: 6417.459961
Gradient norm: 3321.595947265625
Rank 2, Epoch 5, Batch 235, Loss: 4350.710449
Gradient norm: 3411.0224609375
Rank 0, Epoch 5, Batch 211, Loss: 5649.218750
Gradient norm: 3460.84423828125
Rank 0, Epoch 5, Batch 212, Loss: 4703.125488
Gradient norm: 3427.1865234375
Rank 0, Epoch 5, Batch 213, Loss: 4394.099609
Gradient norm: 3413.51953125
Rank 0, Epoch 5, Batch 214, Loss: 4882.306152
Gradient norm: 3210.54296875
Rank 0, Epoch 5, Batch 215, Loss: 4380.947266
Gradient norm: 3483.533935546875
Rank 0, Epoch 5, Batch 216, Loss: 5298.798828
Gradient norm: 3066.1201171875
Rank 0, Epoch 5, Batch 217, Loss: 5405.885742
Gradient norm: 3238.280517578125
Rank 0, Epoch 5, Batch 218, Loss: 5469.906250
Gradient norm: 3276.126220703125
Rank 0, Epoch 5, Batch 219, Loss: 7462.655273
Gradient norm: 3268.97509765625
Rank 0, Epoch 5, Batch 220, Loss: 6628.541016
Gradient norm: 3397.7138671875
Rank 0, Epoch 5, Batch 221, Loss: 3313.534668
Gradient norm: 2979.661376953125
Rank 0, Epoch 5, Batch 222, Loss: 4467.838867
Gradient norm: 3261.90087890625
Rank 0, Epoch 5, Batch 223, Loss: 4531.863281
Gradient norm: 3302.31005859375
Rank 0, Epoch 5, Batch 224, Loss: 5564.006836
Gradient norm: 3293.28369140625
Rank 0, Epoch 5, Batch 225, Loss: 6371.945312
Gradient norm: 3558.06591796875
Rank 0, Epoch 5, Batch 226, Loss: 6283.924316
Gradient norm: 3362.1767578125
Rank 0, Epoch 5, Batch 227, Loss: 4138.300293
Gradient norm: 2983.1240234375
Rank 0, Epoch 5, Batch 228, Loss: 4174.325684
Gradient norm: 3182.5048828125
Rank 0, Epoch 5, Batch 229, Loss: 3909.082275
Gradient norm: 3487.970703125
Rank 0, Epoch 5, Batch 230, Loss: 4693.346191
Gradient norm: 3477.781005859375
Rank 0, Epoch 5, Batch 231, Loss: 8382.654297
Gradient norm: 3609.173828125
Rank 0, Epoch 5, Batch 232, Loss: 7620.485352
Gradient norm: 3492.554443359375
Rank 0, Epoch 5, Batch 233, Loss: 6824.051758
Gradient norm: 3545.8037109375
Rank 0, Epoch 5, Batch 234, Loss: 4333.485352
Gradient norm: 3655.21728515625
Rank 0, Epoch 5, Batch 235, Loss: 5276.544922
Rank 1, Epoch 5, Batch 211, Loss: 17008.621094
Gradient norm: 6066.115234375
Rank 1, Epoch 5, Batch 212, Loss: 19039.996094
Gradient norm: 5941.61669921875
Rank 1, Epoch 5, Batch 213, Loss: 15499.498047
Gradient norm: 5769.916015625
Rank 1, Epoch 5, Batch 214, Loss: 13287.955078
Gradient norm: 5986.25341796875
Rank 1, Epoch 5, Batch 215, Loss: 19053.152344
Gradient norm: 6096.11767578125
Rank 1, Epoch 5, Batch 216, Loss: 15870.257812
Gradient norm: 5974.71923828125
Rank 1, Epoch 5, Batch 217, Loss: 14263.753906
Gradient norm: 5246.5869140625
Rank 1, Epoch 5, Batch 218, Loss: 17296.453125
Gradient norm: 5356.07080078125
Rank 1, Epoch 5, Batch 219, Loss: 18180.277344
Gradient norm: 6355.7216796875
Rank 1, Epoch 5, Batch 220, Loss: 27662.703125
Gradient norm: 6031.09423828125
Rank 1, Epoch 5, Batch 221, Loss: 14049.782227
Gradient norm: 5849.625
Rank 1, Epoch 5, Batch 222, Loss: 11174.773438
Gradient norm: 6116.841796875
Rank 1, Epoch 5, Batch 223, Loss: 18118.093750
Gradient norm: 6192.6904296875
Rank 1, Epoch 5, Batch 224, Loss: 13019.714844
Gradient norm: 5794.052734375
Rank 1, Epoch 5, Batch 225, Loss: 16347.024414
Gradient norm: 5915.85400390625
Rank 1, Epoch 5, Batch 226, Loss: 21767.175781
Gradient norm: 6290.7431640625
Rank 1, Epoch 5, Batch 227, Loss: 16790.933594
Gradient norm: 6050.63818359375
Rank 1, Epoch 5, Batch 228, Loss: 17556.082031
Gradient norm: 5832.06298828125
Rank 1, Epoch 5, Batch 229, Loss: 16474.923828
Gradient norm: 6216.93896484375
Rank 1, Epoch 5, Batch 230, Loss: 17864.855469
Gradient norm: 6194.2509765625
Rank 1, Epoch 5, Batch 231, Loss: 25628.591797
Gradient norm: 5750.240234375
Rank 1, Epoch 5, Batch 232, Loss: 20976.416016
Gradient norm: 5927.7158203125
Rank 1, Epoch 5, Batch 233, Loss: 16188.029297
Gradient norm: 6101.318359375
Rank 1, Epoch 5, Batch 234, Loss: 10853.054688
Gradient norm: 5867.5390625
Rank 1, Epoch 5, Batch 235, Loss: 9712.345703
